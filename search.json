[
  {
    "objectID": "dictionary/index.html",
    "href": "dictionary/index.html",
    "title": "The AI Dictionary",
    "section": "",
    "text": "I often find explanations online to be more complicated than they need to be. Here, I hope to fix that. New terms will continue to be added over time.\nClick terms to view expanded definitions.\nDo let me know of any corrections and improvements, and of any terms you would like added!\n\n\n\n    \n      \n      \n    \n\n\n\n\n\nAccuracy\n\n\nA type of metric. It is a value that tells us how often a model produces correct predictions. The higher the accuracy, the better.\n\n\n\n\n\n\n\n\n\n\nActivation Function\n\n\nA function that follows the linear function in a neuron, to introduce nonlinearity.\n\n\n\n\n\n\n\n\n\n\nArchitecture\n\n\nA model that is used as a template or a starting point for another model.\n\n\n\n\n\n\n\n\n\n\nBackpropagation\n\n\nThe name given to the algorithm that computes the gradients for the weights in a model. Note that it does not update the weights, and hence is not an optimizer\n\n\n\n\n\n\n\n\n\n\nBackward Pass\n\n\nThe pass that begins with the outputs, and ends with the gradients. Also see forward pass.\n\n\n\n\n\n\n\n\n\n\nBagging\n\n\nAn ensembling technique. When bagging, each model is trained on random subset of the rows, and a random subset of the columns, with replacement.\n\n\n\n\n\n\n\n\n\n\nBatch\n\n\nA small collection of data from the dataset.\n\n\n\n\n\n\n\n\n\n\nCognitive Map\n\n\nSimply put, a specialized embedding where each row is an object and each column is a location. Thus, each cell tells us whether a given object is at a given location.\n\n\n\n\n\n\n\n\n\n\nCross Entropy Loss\n\n\nA technique for calculating the loss for categorical models with multiple categories.\n\n\n\n\n\n\n\n\n\n\nDataloader\n\n\nAn object that takes data from the dataset, and assembles them into batches. Note that this object does not decide what indices to load from, and hence is not a sampler.\n\n\n\n\n\n\n\n\n\n\nDataset\n\n\nA collection of data.\n\n\n\n\n\n\n\n\n\n\nDecision Tree\n\n\nA type of model that acts like an if-else statement.\n\n\n\n\n\n\n\n\n\n\nDecoder (Transformers)\n\n\nA component of a transformer that is used for generating text. An example is the autocomplete feature on a smartphone’s keyboard.\n\n\n\n\n\n\n\n\n\n\nDocument\n\n\nThe name given to a piece or collection of text. It can range from anything from a single word to a sentence to a paragraph to a page of text to a full book, and so on. Also referred to as sequence.\n\n\n\n\n\n\n\n\n\n\nDot Product\n\n\nThe operation given to the process of taking the product of each corresponding element in a vector, and summing all products. Also known as linear combination.\n\n\n\n\n\n\n\n\n\n\nEmbedding\n\n\nA table, or matrix, where each row represents an item and each column describes the items in some way. The real magic of embeddings happen when you combine two embeddings together in some way to obtain further information.\n\n\n\n\n\n\n\n\n\n\nEncoder (Transformers)\n\n\nA component of a transformer that is used for “understanding” text. Encoders are typically used for classifying sentences by sentiment and figuring out what parts of a sentence refers, for example, to a person or location.\n\n\n\n\n\n\n\n\n\n\nEnsemble\n\n\nA collection of models whos’ predictions are averaged to obtain the final prediction.\n\n\n\n\n\n\n\n\n\n\nError Rate\n\n\nA type of metric. It is a value that tells us how often a model produces incorrect predictions. The lower the error rate, the better.\n\n\n\n\n\n\n\n\n\n\nForward Pass\n\n\nThe pass that begins with the inputs, and ends with the outputs and loss. Also see backward pass.\n\n\n\n\n\n\n\n\n\n\nGradient\n\n\nA numerical value that informs us how to adjust a parameter of the model.\n\n\n\n\n\n\n\n\n\n\nGradient Accumulation\n\n\nA technique for running or fitting large models on a not-so-powerful GPU.\n\n\n\n\n\n\n\n\n\n\nGradient Boosting Machine (GBM)\n\n\nAn ensembling technique where instead of averaging the predictions of all models, each successive model predicts the error of the previous model. The errors are then summed to obtain the final prediction.\n\n\n\n\n\n\n\n\n\n\nInference\n\n\nUsing a trained model for predictions.\n\n\n\n\n\n\n\n\n\n\nK-Fold Cross Validation\n\n\nAn ensembling technique where models are trained on a different set percent of the dataset. For example each model is trained on a different 80% of the dataset.\n\n\n\n\n\n\n\n\n\n\nLatent (Diffusion)\n\n\nA compressed image.\n\n\n\n\n\n\n\n\n\n\nLearning Rate\n\n\nA numerical value which controls how much the gradients update the parameters of a model.\n\n\n\n\n\n\n\n\n\n\nLinear Combination\n\n\nThe operation given to the process of taking the product of each corresponding element in a vector, and summing all products. Also known as dot product.\n\n\n\n\n\n\n\n\n\n\nLoss\n\n\nA measure of performance of a model. It is used by the model to improve itself. Typically, the lower the loss, the better.\n\n\n\n\n\n\n\n\n\n\nMatrix\n\n\nA table of values. See also vector\n\n\n\n\n\n\n\n\n\n\nMean Absolute Error (MAE)\n\n\nA type of metric. It is a value that tells us, on average, how close a set of predicted values is from the actual values. The smaller the MAE, the better.\n\n\n\n\n\n\n\n\n\n\nMean Columnwise Root Mean Squared Error (MCRMSE)\n\n\nA metric used for those tasks where multiple targets need to be predicted. This metric simply takes the average of the RMSE of each target.\n\n\n\n\n\n\n\n\n\n\nMean Squared Error (MSE)\n\n\nA type of metric. It is a value that tells us, on average, how close a set of predicted values is from the actual values. The smaller the MSE, the better.\n\n\n\n\n\n\n\n\n\n\nMetric\n\n\nA measure of performance of a model. It is used by humans to judge the performance of the model.\n\n\n\n\n\n\n\n\n\n\nModel\n\n\nA mathematical equation that mimicks a real life phenomenon. This equation can be used to predict desired quantities.\n\n\n\n\n\n\n\n\n\n\nNamed Entity Recognition (NER)\n\n\nA NLP classification task where a sentence is broken into its components, and the model attempts to assign each component to a specific entity (e.g., person, place, organization).\n\n\n\n\n\n\n\n\n\n\nNeuron\n\n\nA basic processor of information. It consists of the linear combination and an activation function.\n\n\n\n\n\n\n\n\n\n\nNumericalization\n\n\nA process where numbers are assigned to each token. Occurs after tokenization.\n\n\n\n\n\n\n\n\n\n\nOne Hot Encoding\n\n\nA data processing technique where each class in a categorical feature is given its own column that contains true and false values.\n\n\n\n\n\n\n\n\n\n\nOneR Classifier\n\n\nThe simplest type of decision tree. The tree only contains a single split.\n\n\n\n\n\n\n\n\n\n\nOptimizer\n\n\nThe name given to the algorithm that updates the weights in a model. Note that it does not compute the gradients, and hence is not part of backpropagation.\n\n\n\n\n\n\n\n\n\n\nPolicy (Reinforcement Learning)\n\n\nThe strategy the agent uses to choose actions based on its current state. The chosen actions aim to maximize the reward.\n\n\n\n\n\n\n\n\n\n\nRandom Forest\n\n\nThe name given to a bagged ensemble of decision trees.\n\n\n\n\n\n\n\n\n\n\nRectified Linear Unit (ReLU)\n\n\nAn activation function that clips any value less than zero, to zero.\n\n\n\n\n\n\n\n\n\n\nRoot Mean Squared Error (RMSE)\n\n\nA type of metric. It is a value that tells us, on average, how close a set of predicted values is from the actual values. The smaller the RMSE, the better.\n\n\n\n\n\n\n\n\n\n\nRoot Mean Squared Logarithmic Error (RMSLE)\n\n\nA type of metric. It is a value that tells us, on average, how close a set of predicted values is from the actual values. The smaller the RMSLE, the better.\n\n\n\n\n\n\n\n\n\n\nSample\n\n\nA row in a dataset.\n\n\n\n\n\n\n\n\n\n\nSampler\n\n\nAn algorithm that decides what indices of a dataset to load. Note that this algorithm does not load data, and hence is not a dataloader.\n\n\n\n\n\n\n\n\n\n\nSequence\n\n\nThe name given to a piece or collection of text. It can range from anything from a single word to a sentence to a paragraph to a page of text to a full book, and so on. Also referred to as document.\n\n\n\n\n\n\n\n\n\n\nSoftmax\n\n\nA function that calculates the probabilities of a set of predictions.\n\n\n\n\n\n\n\n\n\n\nTabular Data\n\n\nData in the form of a table.\n\n\n\n\n\n\n\n\n\n\nTabular Model\n\n\nA model trained on tabular data. It is used to predict a specified column in the data.\n\n\n\n\n\n\n\n\n\n\nTokenization\n\n\nSplitting a document into its component words.\n\n\n\n\n\n\n\n\n\n\nTransformer\n\n\nThe name given to a Natural Language Processing (NLP) architecture that, in a nutshell, either fills-in-the-blanks or autocompletes text. Transformers consist of either an encoder, decoder, or both.\n\n\n\n\n\n\n\n\n\n\nVector\n\n\nA table of values that has either a single row or a single column. See also matrix.\n\n\n\n\n\n\n\n\n\n\nWeight Decay\n\n\nA technique for making sure weights do not grow too large, and in turn overfit the data.\n\n\n\n\n\n\n\n\n\n\nZero-shot\n\n\nA prefix given to a pretrained model that can be used without finetuning.\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/architecture.html",
    "href": "dictionary/terms/architecture.html",
    "title": "Architecture",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/root_mean_squared_logarithmic_error_rmsle.html",
    "href": "dictionary/terms/root_mean_squared_logarithmic_error_rmsle.html",
    "title": "Root Mean Squared Logarithmic Error (RMSLE)",
    "section": "",
    "text": "It is calculated by:\n\nFirst taking the logarithm of all predicted values.\nTaking the logarithm of all actual values.\nThen taking the difference between each respective predicted and actual value.\nNext squaring all obtained values.\nTaking the averge.\nAnd lastly taking the square root.\n\nLet’s say we have a set of predicted values \\(1, 2, 3, 4\\). The set of actual values is \\(1, 4, 3, 3\\)\n\n\\(\\ln(1), \\ln(2), \\ln(3), \\ln(4) \\approx 0, 0.69, 1.10, 1.39\\)\n\\(\\ln(1), \\ln(4), \\ln(3), \\ln(3) \\approx 0, 1.39, 1.10, 1.10\\)\n\\(0-0, 0.69-1.39, 1.10-1.10, 1.39-1.10 = 0, -0.70, 0, 0.29\\)\n\\((0)^2, (-0.70)^2, (0)^2, (0.29)^2 \\approx 0, 0.49, 0, 0.08\\)\n\\(\\frac{0 + 0.49 + 0 + 0.08}{4} = \\frac{0.57}{4}\\)\n\\(\\sqrt{\\frac{0.57}{4}} \\approx 0.38\\)\n\nThis tells us, that on average, our set of predicted values is \\(0.38\\) units off from the actual values.\nIn a nutshell, you take the root of the mean of the square of the differences between the predicted and actual values.\n\nThe main difference between RMSE and RMSLE is that RMSLE works better for very large values, since the logarithm of the predicted values and actual values is taken. The downside is that negative values will not work, since the logarithm of a negative value is undefined.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe reason the square value is taken is due to the averaging step. Let’s say the first predicted value is off from the first actual value by \\(-3\\) units. And let’s say that the second predicted value is off from the second actual value by \\(3\\) units.\nIf we didn’t take the square, the average would be zero \\(\\left( \\frac{-3 + 3}{2} = \\frac{0}{2} = 0 \\right)\\). This is incorrect as both values are off from the actual value.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/tabular_data.html",
    "href": "dictionary/terms/tabular_data.html",
    "title": "Tabular Data",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/dot_product.html",
    "href": "dictionary/terms/dot_product.html",
    "title": "Dot Product",
    "section": "",
    "text": "\\[\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3 \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n4 \\\\\n5 \\\\\n6 \\\\\n\\end{bmatrix}\n= (1 \\cdot 4) + (2 \\cdot 5) + (3 \\cdot 6) = 32\n\\]\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/transformer.html",
    "href": "dictionary/terms/transformer.html",
    "title": "Transformer",
    "section": "",
    "text": "To learn more about transformers, you can read this to the point rundown.\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/document.html",
    "href": "dictionary/terms/document.html",
    "title": "Document",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/encoder.html",
    "href": "dictionary/terms/encoder.html",
    "title": "Encoder (Transformers)",
    "section": "",
    "text": "To learn more about decoders, you can read this to the point rundown.\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/loss.html",
    "href": "dictionary/terms/loss.html",
    "title": "Loss",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/dataloader.html",
    "href": "dictionary/terms/dataloader.html",
    "title": "Dataloader",
    "section": "",
    "text": "Metaphorically speaking, if we let a dataset be a warehouse, a dataloader be a human, a batch be a crate, and the sampler be the manager, then the manager is responsible for informing the human what items to gather from the warehouse, who then puts them into crates.\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/vector.html",
    "href": "dictionary/terms/vector.html",
    "title": "Vector",
    "section": "",
    "text": "The order of a vector is row by column.\nBelow is a \\(1 \\times 3\\) column vector. \\[\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3 \\\\\n\\end{bmatrix}\n\\]\nBelow is a \\(3 \\times 1\\) row vector. \\[\n\\begin{bmatrix}\n1 & 2 & 3\n\\end{bmatrix}\n\\]\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/mean_columnwise_root_mean_square_error_mcrmse.html",
    "href": "dictionary/terms/mean_columnwise_root_mean_square_error_mcrmse.html",
    "title": "Mean Columnwise Root Mean Squared Error (MCRMSE)",
    "section": "",
    "text": "Let’s say a model had to predict the velocity, acceleration, and drag force for 3 cars.\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/mean_squared_error_mse.html",
    "href": "dictionary/terms/mean_squared_error_mse.html",
    "title": "Mean Squared Error (MSE)",
    "section": "",
    "text": "It is calculated by:\n\nFirst taking the difference between each respective predicted and actual value.\nThen the squaring all obtained values.\nAnd finally taking the average.\n\nLet’s say we have a set of predicted values \\(1, 2, 3, 4\\). The set of actual values is \\(1, 4, 3, 3\\)\n\n\\(1-1, 2-4, 3-3, 4-3, = 0, -2, 0, 1\\)\n\\((0)^2, (-2)^2, (0)^2, (1)^2 = 0, 4, 0, 1\\)\n\\(\\frac{0 + 4 + 0 + 1}{4} = \\frac{5}{4} = 1.25\\)\n\nThis tells us, that on average, our set of predicted values is \\(1.25\\) units off from the actual values.\nIn a nutshell, you take the mean of the square of the differences between the predicted and actual values.\n\nThe main difference between MAE and MSE is that MSE penalizes smaller differences more heavily.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe reason the square value is taken is due to the averaging step. Let’s say the first predicted value is off from the first actual value by \\(-3\\) units. And let’s say that the second predicted value is off from the second actual value by \\(3\\) units.\nIf we didn’t take the square, the average would be zero \\(\\left( \\frac{-3 + 3}{2} = \\frac{0}{2} = 0 \\right)\\). This is incorrect as both values are off from the actual value.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/gradient_accumulation.html",
    "href": "dictionary/terms/gradient_accumulation.html",
    "title": "Gradient Accumulation",
    "section": "",
    "text": "Let’s say you want to use a batch size of 64, but the model doesn’t fit with that size on your GPU.\n\nFirst determine the largest possible batch size that can fit on your GPU. Let’s say it’s 16. It may be better to use batch sizes that are a power of 2.\nCalculate the gradients for \\(X\\) batches without updating the parameters.\n\n\\(X\\) is your desired batch size divided by the batch size you are using.\nDesired batch size is 64; batch size we are using is 16.\n\\(64 ÷ 16 = 4\\)\n\\(X\\) is 4. This is because the size of 4 batches, in this case, sums to 64.\n\nNext, sum all respective gradients — hence the term ‘gradient accumulation’.\nNow update your parameters based on these summed gradients. This will have the same effect as if you used a batch size of 64.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nUsing a smaller batch size to fit a larger model onto your GPU isn’t optimal. A smaller batch size means you would have to tweak your optimal hyperparameters, such as the learning rate. Your loss would also become less accurate since it is being calculated on a smaller group of items.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/decoder.html",
    "href": "dictionary/terms/decoder.html",
    "title": "Decoder (Transformers)",
    "section": "",
    "text": "To learn more about decoders, you can read this to the point rundown.\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/numericalization.html",
    "href": "dictionary/terms/numericalization.html",
    "title": "Numericalization",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/rectified_linear_unit.html",
    "href": "dictionary/terms/rectified_linear_unit.html",
    "title": "Rectified Linear Unit (ReLU)",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/linear_combination.html",
    "href": "dictionary/terms/linear_combination.html",
    "title": "Linear Combination",
    "section": "",
    "text": "\\[\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3 \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n4 \\\\\n5 \\\\\n6 \\\\\n\\end{bmatrix}\n= (1 \\cdot 4) + (2 \\cdot 5) + (3 \\cdot 6) = 32\n\\]\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/backpropagation.html",
    "href": "dictionary/terms/backpropagation.html",
    "title": "Backpropagation",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/embedding.html",
    "href": "dictionary/terms/embedding.html",
    "title": "Embedding",
    "section": "",
    "text": "An example of how two embeddings can be combined together is shown below.\nLet’s say we have an embedding of users, where each column represents a feature about movies. Users like certain features of movies, and a value between -1 and 1 represents this.\n\n\n\nUser\nLong Duration\nSci-Fi\nFantasy\nAnimated\nAction\n\n\n\n\nBilly\n-0.9\n0.3\n0.2\n0.8\n0.25\n\n\nBob\n-0.85\n1\n-0.25\n0\n0.75\n\n\nJoe\n0.9\n0.85\n0.95\n0.35\n1\n\n\n\nNow let’s say we have an embedding of movies, where each column represents a feature about movies.\n\n\n\n\n\n\n\n\n\n\n\nMovie\nLong Duration\nSci-Fi\nFantasy\nAnimated\nAction\n\n\n\n\nThe Lord of the Rings\n1\n-1\n1\n-0.5\n1\n\n\nCars\n-0.9\n-1\n0.8\n1\n0\n\n\nInterstellar\n0.75\n1\n0\n0\n0.3\n\n\n\nWe want to find out which movie would be the best for Billy to watch. To do so, let’s take the dot product between Billy and each of the respective movies.\n\n\n\n\n\n\nBilly & The Lord of the Rings\n\n\n\n\n\n\\[\n(-0.9 \\cdot 1) + (0.3 \\cdot -1) + (0.2 \\cdot 1) + (0.8 \\cdot -0.5) + (0.25 \\cdot 1) = -1.15\n\\]\n\n\n\n\n\n\n\n\n\nBilly & Cars\n\n\n\n\n\n\\[\n(-0.9 \\cdot -0.9) + (0.3 \\cdot -1) + (0.2 \\cdot 0.8) + (0.8 \\cdot 1) + (0.25 \\cdot 0) = 1.47\n\\]\n\n\n\n\n\n\n\n\n\nBilly & Interstellar\n\n\n\n\n\n\\[\n(-0.9 \\cdot 0.75) + (0.3 \\cdot 1) + (0.2 \\cdot 0) + (0.8 \\cdot 0) + (0.25 \\cdot 0.3) = -0.3\n\\]\n\n\n\nWe have obtained the values \\(-1.15\\), \\(1.47\\), and \\(-0.3\\) for each of the movies respectively. From this, we can deduce that Cars (\\(1.47\\)) is probably the best movie for Billy to watch, based on his taste.\nAfter similarly calculating the dot product between Joe and each of the movies, we get the following respective values: \\(1.82\\), \\(-0.55\\), \\(1.82\\). This tells us that both The Lord of the Rings and Interstellar are equally the best movies for Joe to watch.\nAs for Bob, it would be Interstellar.\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/tokenization.html",
    "href": "dictionary/terms/tokenization.html",
    "title": "Tokenization",
    "section": "",
    "text": "Note\n\n\n\n\n\nIf a word is too long or very uncommon, the word itself may be split. Take the word “supercalifragilisticexpialidocious” as an example. It could be split into “super”, “cali”, “fragilistic”, “expi”, “ali”, and “docious”.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/matrix.html",
    "href": "dictionary/terms/matrix.html",
    "title": "Matrix",
    "section": "",
    "text": "The order of a matrix is row by column.\nBelow is \\(3 \\times 2\\) matrix. \\[\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6 \\\\\n\\end{bmatrix}\n\\]\nBelow is \\(2 \\times 3\\) matrix. \\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n\\] \\end{bmatrix}\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/gradient.html",
    "href": "dictionary/terms/gradient.html",
    "title": "Gradient",
    "section": "",
    "text": "The gradients update the parameters by multiplying the two together. How much the gradients update the parameters is controlled by the learning rate.\nA positive gradient tells us that increasing the parameters will increase the loss. On the other hand, decreasing the parameters will decrease the loss.\nA negative gradient tells us that decreasing the parameters will increase the loss On the other hand, increasing the parameters will decrease the loss.\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/zero_shot.html",
    "href": "dictionary/terms/zero_shot.html",
    "title": "Zero-shot",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/model.html",
    "href": "dictionary/terms/model.html",
    "title": "Model",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/error_rate.html",
    "href": "dictionary/terms/error_rate.html",
    "title": "Error Rate",
    "section": "",
    "text": "It can be calculated by dividing the number of incorrect predictions by the number of total predictions. Optionally multiply the result by 100 to obtain a percentage.\n\\[\n\\frac{\\text{number of incorrect predictions}}{\\text{number of total predictions}}\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nError rate is also 1 - accuracy.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/ensemble.html",
    "href": "dictionary/terms/ensemble.html",
    "title": "Ensemble",
    "section": "",
    "text": "The reason why this works is that some models will overestimate while others will underestimate, cancelling out each others’ errors.\nThere are different methods for ensembling.\n\n\n\n\n\n\nEnsembling only works when all models are independent of each other. That is, the models do not depend on one another.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/oner_classifier.html",
    "href": "dictionary/terms/oner_classifier.html",
    "title": "OneR Classifier",
    "section": "",
    "text": "Below is an example determining whether a car is fast or slow.\n\n\n\n\n\nflowchart TB\n  A([Weight &lt; 2000kg])\n  B([Car Is Fast])\n  C([Car Is Slow])\n\n  A -- Yes --&gt; B\n  A -- No --&gt; C\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/sequence.html",
    "href": "dictionary/terms/sequence.html",
    "title": "Sequence",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "web_apps/apps/bear_detector.html",
    "href": "web_apps/apps/bear_detector.html",
    "title": "Bear Classifier",
    "section": "",
    "text": "This webapp was remade on Tuesday, 8 November 2022.\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "web_apps/index.html",
    "href": "web_apps/index.html",
    "title": "App Playground",
    "section": "",
    "text": "Here you can view various apps and gizmos I’ve created. Have a play through some of them let me know what you think!\nMore apps coming soon.™\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMore apps coming soon…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlood Classifier\n\n\n\nImage Classification\n\n\n\nHow well can you classify floods?\n\n\n\nSalman Naqvi\n\n\nTuesday, 20 September 2022 | 2022-09-20\n\n\n\n\n\n\n\n\n\n\n\n\n\nBear Classifier\n\n\n\nImage Classification\n\n\n\nCan you spot a black bear in a black night?\n\n\n\nSalman Naqvi\n\n\nSaturday, 30 April 2022 | 2022-04-30\n\n\n\n\n\n\nNo matching items\n\n\n\nLoading…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ForBo7 // Salman Naqvi",
    "section": "",
    "text": "Onward and upward, with tenacity and sincerity.\nRead more about me here.\nAnd feel free to contact me with one of the fancy buttons below!\n\n \n  \n   \n  \n    \n     fastai Forums\n  \n  \n    \n     Twitter\n  \n  \n    \n     Artstation\n  \n  \n    \n     Email"
  },
  {
    "objectID": "index.html#bits-and-bobs",
    "href": "index.html#bits-and-bobs",
    "title": "ForBo7 // Salman Naqvi",
    "section": "Bits and Bobs",
    "text": "Bits and Bobs\nClick here to explore more bits and bobs.\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDescription\n\n\n\nDate\n\n\n\n\n\n\n\n\nNifty Shell Tools\n\n\nPocket tools to keep in your pocket.\n\n\nJul 15, 2025\n\n\n\n\n\n\nNuances in Shell Scripting\n\n\nShell’s just another programming language.\n\n\nJul 15, 2025\n\n\n\n\n\n\nShell hooks, uv, and Conda.\n\n\nHooking uv and conda together.\n\n\nJul 5, 2025\n\n\n\n\n\n\nRetrieval Augmented Generation in a nutshell.\n\n\nDon’t let jargon fool you.\n\n\nJun 24, 2025\n\n\n\n\n\n\nTerminal know-how.\n\n\nKnow the basics of manipulating your shell.\n\n\nJun 20, 2025\n\n\n\n\n\n\nCognitive Maps\n\n\nCognitive maps in the context of biology and AI.\n\n\nJun 3, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#forblog",
    "href": "index.html#forblog",
    "title": "ForBo7 // Salman Naqvi",
    "section": "ForBlog",
    "text": "ForBlog\nClick here to check out the latest on the ForBlog.\n\n\n\n\n\n\n\n\n\n\nPulling Back the Curtain on VLM Attention\n\n\nUnderstanding LLM Attention and Vision Encoder Attention\n\n\nA LLM narrates our conversation.\n\n\n\n\n\nAug 4, 2025\n\n\nSalman Naqvi\n\n\n\n\n\n\n\n\n\n\n\n\nGetting General Purpose Robots by Decomposing Problems and Working with Data\n\n\nroboOS®, powered by roboBrain™\n\n\nWhy these Robo series of papers are cold drink in a desert of hobbled pipelines\n\n\n\n\n\nJul 24, 2025\n\n\nSalman Naqvi\n\n\n\n\n\n\n\n\n\n\n\n\nCurrent Ideas in Spatial Understanding\n\n\nWhat is good, what is not?\n\n\nA collated set of current ideas.\n\n\n\n\n\nJul 14, 2025\n\n\nSalman Naqvi\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "forblog/posts/20_models_from_scratch.html",
    "href": "forblog/posts/20_models_from_scratch.html",
    "title": "Implementing a Neural Network from Scratch",
    "section": "",
    "text": "This notebook follows the fastai style guide.\nIn this notebook, I will implement a neural network from scratch, and iteratively reimplement with PyTorch. That is, I will implement each element of the training and inference process from scratch, before then using the corresponding element in PyTorch. This notebook assumes a prior understanding of the flow and pieces of a neural network.\nTo recap, the complete training loop of a neural network looks like this.\nThis notebook also serves to show the modular nature of PyTorch.\nLet’s get started with some data."
  },
  {
    "objectID": "forblog/posts/20_models_from_scratch.html#download-data",
    "href": "forblog/posts/20_models_from_scratch.html#download-data",
    "title": "Implementing a Neural Network from Scratch",
    "section": "Download Data",
    "text": "Download Data\nThe goal of our model will be to classify digits from the MNIST dataset.\n\nfrom pathlib import Path\nMNIST_URL = 'https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\nd_path = Path('data')\nd_path.mkdir(exist_ok=True)\nd_path = d_path/'mnist.pkl.gz'\n\n\nfrom urllib.request import urlretrieve\nif not d_path.exists(): urlretrieve(MNIST_URL, d_path)\n\n\n! ls -l data\n\ntotal 33312\n-rw-r--r--  1 salmannaqvi  staff  17051982 May 12 12:37 mnist.pkl.gz\n\n\n\nimport gzip, pickle\nfrom torch import tensor\nwith gzip.open(d_path, 'rb') as f: ((trn_x, trn_y), (vld_x, vld_y), _) = pickle.load(f, encoding='latin-1')\n1trn_x, trn_y, vld_x, vld_y = map(tensor, [trn_x[:1000], trn_y[:1000], vld_x[:1000], vld_y[:1000]])\n\n\n1\n\nTaking 1000 samples each for the sake of speed."
  },
  {
    "objectID": "forblog/posts/20_models_from_scratch.html#a-single-neuron",
    "href": "forblog/posts/20_models_from_scratch.html#a-single-neuron",
    "title": "Implementing a Neural Network from Scratch",
    "section": "A Single Neuron",
    "text": "A Single Neuron\nA neuron comprises of a set of weights, the linear function, and the activation function.\n\n\n\n\n\n\n\n\n\n\ngraph LR\n  A1[Weights]\n  A2[Inputs]\n  A1 & A2 --&gt; B[Linear Combination] --&gt; C[Activation Function]\n\n\n\n\n\n\n\n\n\n\n\nOur dataset contains one thousand 28x28 pixel samples. Therefore, each sample has 28x28=784 inputs. Since we will be classifying digits, there will be 10 outputs–a probablity for each digit.\n\nn, m = trn_x.shape\nc = trn_y.max() + 1\nn, m, c\n\n(1000, 784, tensor(10))\n\n\nLet’s have 50 neurons comprise the hidden layer.\n\nnh = 50\n\nFrom these dimensions, we can create our appropriate weights…\n\nimport torch; torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\nw1, b1 = torch.randn(m, nh), torch.zeros(nh)\nw2, b2 = torch.randn(nh, 1), torch.zeros(1)\nw1.shape, b1.shape, w2.shape, b2.shape\n\n(torch.Size([784, 50]), torch.Size([50]), torch.Size([50, 1]), torch.Size([1]))\n\n\n…and create our linear model!\n\ndef lin(x, w, b): return x @ w + b\n\n\nt = lin(vld_x, w1, b1); t.shape\n\ntorch.Size([1000, 50])\n\n\n\nvld_x.shape, w1.shape\n\n(torch.Size([1000, 784]), torch.Size([784, 50]))\n\n\n\nfrom fastcore.all import *\nimport torch.nn.functional as F\ntest_eq(lin(vld_x, w1, b1), F.linear(vld_x, w1.T, b1))\n\nOur implementation produces the same outputs as PyTorch’s implementation.\nWe now need to implement the activation function, which will be the ReLU (rectified linear unit). Any value less than 0 gets clipped to 0. There are multiple ways we can approach doing this, such as using torch.max.\n\n\n?torch.max\n\n\nDocstring:\n\nmax(input) -&gt; Tensor\n\n\n\nReturns the maximum value of all elements in the ``input`` tensor.\n\n\n\n.. warning::\n\n    This function produces deterministic (sub)gradients unlike ``max(dim=0)``\n\n\n\nArgs:\n\n    input (Tensor): the input tensor.\n\n\n\nExample::\n\n\n\n    &gt;&gt;&gt; a = torch.randn(1, 3)\n\n    &gt;&gt;&gt; a\n\n    tensor([[ 0.6763,  0.7445, -2.2369]])\n\n    &gt;&gt;&gt; torch.max(a)\n\n    tensor(0.7445)\n\n\n\n.. function:: max(input, dim, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor)\n\n   :noindex:\n\n\n\nReturns a namedtuple ``(values, indices)`` where ``values`` is the maximum\n\nvalue of each row of the :attr:`input` tensor in the given dimension\n\n:attr:`dim`. And ``indices`` is the index location of each maximum value found\n\n(argmax).\n\n\n\nIf ``keepdim`` is ``True``, the output tensors are of the same size\n\nas ``input`` except in the dimension ``dim`` where they are of size 1.\n\nOtherwise, ``dim`` is squeezed (see :func:`torch.squeeze`), resulting\n\nin the output tensors having 1 fewer dimension than ``input``.\n\n\n\n.. note:: If there are multiple maximal values in a reduced row then\n\n          the indices of the first maximal value are returned.\n\n\n\nArgs:\n\n    input (Tensor): the input tensor.\n\n    dim (int): the dimension to reduce.\n\n    keepdim (bool): whether the output tensor has :attr:`dim` retained or not. Default: ``False``.\n\n\n\nKeyword args:\n\n    out (tuple, optional): the result tuple of two output tensors (max, max_indices)\n\n\n\nExample::\n\n\n\n    &gt;&gt;&gt; a = torch.randn(4, 4)\n\n    &gt;&gt;&gt; a\n\n    tensor([[-1.2360, -0.2942, -0.1222,  0.8475],\n\n            [ 1.1949, -1.1127, -2.2379, -0.6702],\n\n            [ 1.5717, -0.9207,  0.1297, -1.8768],\n\n            [-0.6172,  1.0036, -0.6060, -0.2432]])\n\n    &gt;&gt;&gt; torch.max(a, 1)\n\n    torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))\n\n\n\n.. function:: max(input, other, *, out=None) -&gt; Tensor\n\n   :noindex:\n\n\n\nSee :func:`torch.maximum`.\n\nType:      builtin_function_or_method\n\n\n\ntorch.max(tensor([-5, 2, 3, -4]), tensor([0]))\n\ntensor([0, 2, 3, 0])\n\n\n\ndef relu(x): return torch.max(x, tensor([0]))\n\nAnother way is to use torch.clamp_min, which is more idiomatic for this case.\n\ndef relu(x): return x.clamp_min(0.)\n\n\nt = lin(vld_x, w1, b1)\ntest_eq(relu(t), F.relu(t))\n\nA single neuron can now be constructed.\n\ndef model(xb):\n  l1 = relu(lin(xb, w1, b1))\n  return lin(l1, w2, b2)\n\n\nres = model(vld_x); res.shape\n\ntorch.Size([1000, 1])"
  },
  {
    "objectID": "forblog/posts/20_models_from_scratch.html#loss-function",
    "href": "forblog/posts/20_models_from_scratch.html#loss-function",
    "title": "Implementing a Neural Network from Scratch",
    "section": "Loss Function",
    "text": "Loss Function\nWith the forward pass being implemented, it is time to determine the loss. Even though we have a multi-class classification problem at hand, I will use mean squared error for simplicity. Later in this post, I will switch to cross entropy loss.\nThe Mean Squared Error (MSE) between two vectors can be represented as:\n\\[\n\\text{MSE} = \\frac{\\sum_{i=1}^{n} (y_i - x_i)^2}{n}\n\\]\nwhere \\(x\\) and \\(y\\) are vectors of length \\(n\\), and \\(x_i\\) and \\(y_i\\) represent the \\(i\\)-th elements of the vectors.\n\n\n\n\n\n\nMSE in its most basic form looks like this.\n\\[\n\\text{MSE} = \\frac{(y - x)^2}{1}\n\\]\nIf we have multiple data points, then it looks like this.\n\\[\n\\text{MSE} = \\frac{(y_1 - x_1)^2+(y_2 - x_2)^2+(y_3 - x_3)^2}{3}\n\\]\n\n\n\nThe tensor holding the predictions and the tensor holding the targets have different shapes. Therefore, there are different ways in which both can be subtracted from each other.\n\nres.shape, vld_y.shape\n\n(torch.Size([1000, 1]), torch.Size([1000]))\n\n\n\n(vld_y - res).shape\n\ntorch.Size([1000, 1000])\n\n\n\n(vld_y[:, None] - res).shape\n\ntorch.Size([1000, 1])\n\n\n\nres[:, 0].shape, res.squeeze().shape\n\n(torch.Size([1000]), torch.Size([1000]))\n\n\n\n(vld_y - res[:, 0]).shape\n\ntorch.Size([1000])\n\n\nHowever, it will be better to add a column to vld_y rather than remove a column from res, so as to keep the shape of all tensors consistent (i.e., all tensors having a row and column, as opposed to some having rows and columns, and others having only a column).\n\n((vld_y[:, None] - res)**2).sum() / res.shape[0]\n\ntensor(717.17)\n\n\n\ndef mse(preds, targs): return (targs[:, None] - preds).pow(2).mean()\n\n\npreds = model(trn_x); mse(preds, trn_y)\n\ntensor(648.87)\n\n\n\ntest_eq(mse(preds, trn_y), F.mse_loss(preds, trn_y[:, None]))"
  },
  {
    "objectID": "forblog/posts/20_models_from_scratch.html#backward-pass",
    "href": "forblog/posts/20_models_from_scratch.html#backward-pass",
    "title": "Implementing a Neural Network from Scratch",
    "section": "Backward Pass",
    "text": "Backward Pass\nNow comes the backward pass; the pass responsible for computing the gradients of our model’s weights.\nFor brevity, I will not explain why I compute the gradients the way I do. It can be taken that the way I compute them is due to the result of calculating the derivatives of the foward pass by hand. If you would like to explore how I did so, you can refer to my other blog post, Backpropagation Explained using English Words*.\nIn short, the derivatives compute to be the following.\n\n\n\n\n\n\n\\[\n\\begin{align}\n  \\frac{\\partial \\text{MSE}}{\\partial \\vec{\\rm{w}}_1} &=\n    \\begin{cases}\n      0 & \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 ≤ 0 \\\\\n      \\frac{2}{N} \\sum^N_{i=1} (\\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2 - \\vec{\\rm{y}}_i) \\cdot \\vec{\\rm{w}}^T_2 \\cdot \\vec{\\rm{x}}_i^T & \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 &gt; 0\n    \\end{cases} \\\\\n  \\frac{\\partial \\text{MSE}}{\\partial b_1} &=\n    \\begin{cases}\n      0 & \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 ≤ 0 \\\\\n      \\frac{2}{N} \\sum^N_{i=1} (\\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2 - \\vec{\\rm{y}}_i) \\cdot \\vec{\\rm{w}}_2^T & \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 &gt; 0\n    \\end{cases} \\\\\n  \\frac{\\partial \\text{MSE}}{\\partial \\vec{\\rm{x}}_i} &=\n    \\begin{cases}\n      0 & \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 ≤ 0 \\\\\n      \\frac{2}{N} \\sum^N_{i=1} (\\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2 - \\vec{\\rm{y}}_i) \\cdot \\vec{\\rm{w}}^T_2 \\cdot \\vec{\\rm{w}}_1^T & \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 &gt; 0\n    \\end{cases} \\\\\n  \\frac{\\partial \\text{MSE}}{\\partial \\vec{\\rm{w}}_2} &= \\frac{2}{N} \\sum^N_{i=1} (\\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2 - \\vec{\\rm{y}}_i) \\cdot \\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\\\\n  \\frac{\\partial \\text{MSE}}{\\partial b_2} &= \\frac{2}{N} \\sum^N_{i=1} \\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2 - \\vec{\\rm{y}}_i\n\\end{align}\n\\]\n\n\n\nWhen implementing backpropagation, it is better to implement the entire equation in pieces, by storing the result of each intermediate gradient. These intermediate gradients can then be reused to calculate the gradients of another variable.\nLet’s prepare the pieces we’ll need and get started.\n\nl1 = relu(lin(trn_x, w1, b1))\nl2 = lin(l1, w2, b2)\nloss = mse(l2, trn_y); loss\n\ntensor(648.87)\n\n\n\nw1 Gradients\nThis is the maths to compute the gradients for w1, as also shown above.\n\\[\n  \\frac{\\partial \\text{MSE}}{\\partial \\vec{\\rm{w}}_1} =\n    \\begin{cases}\n      0 & \\text{if } \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 \\leq 0 \\\\\n      \\frac{2}{N} \\sum^N_{i=1} (\\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2 - \\vec{\\rm{y}}_i) \\cdot \\vec{\\rm{w}}^T_2 \\cdot \\vec{\\rm{x}}_i^T & \\text{if } \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 &gt; 0\n    \\end{cases}\n\\]\n\n\n\n\n\n\nHere, you can see the individual pieces I will compute to implement this equation.\n\n\n\n\n\n\nSubstitutions\n\n\n\n\n\n\\[\n\\begin{align}\n    u_1 &= \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 \\\\\n    u_2 &= \\text{max}(0, u_1) \\\\\n    u_3 &= u_2 \\cdot \\vec{\\rm{w}}_2 + b_2 \\\\\n    u_4 &= \\vec{\\rm{y}}_i - u_3 \\\\\n    u_5 &= u_4^2\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\nDerivatives of the Substitutions\n\n\n\n\n\n\\[\n\\begin{alignat}{4}\n  \\text{gradient of } u_4 &= \\frac{\\partial u_5}{\\partial u_4} &&=\n  \\frac{\\partial}{\\partial u_4} u_4^2 &&&= 2u_4 \\\\\n  \\text{gradient of } u_3 &= \\frac{\\partial u_4}{\\partial u_3} &&= \\frac{\\partial}{\\partial u_3} \\vec{\\rm{y}}_i - u_3 &&&= -1 \\\\\n  \\text{gradient of } u_2 &= \\frac{\\partial u_3}{\\partial u_2} &&= \\frac{\\partial}{\\partial u_2} u_2 \\cdot \\vec{\\rm{w}}_2 + b_2 &&&= \\vec{\\rm{w}}^T_2 \\\\\n  \\text{gradient of } u_1 &= \\frac{\\partial \\vec{\\rm{u}}_2}{\\partial \\vec{\\rm{u}}_1} &&= \\frac{\\partial}{\\partial u_1} \\text{max}(0, u_1) &&&=\n    \\begin{cases}\n      0 & \\vec{\\rm{u}}_1 ≤ 0 \\\\\n      1 & \\vec{\\rm{u}}_1 &gt; 0\n    \\end{cases} \\\\\n  \\text{gradient of } \\vec{\\rm{w}}_1 &= \\frac{\\partial u_1}{\\partial \\vec{\\rm{w}}_1} &&= \\frac{\\partial}{\\partial w_1} \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 &&&= \\vec{\\rm{x}}^T_i\n\\end{alignat}\n\\]\n\n\n\n\n\n\n\ndiff = trn_y[:, None] - l2; diff.shape\n\ntorch.Size([1000, 1])\n\n\n\nloss.g = (2/n) * diff; loss, loss.g.shape\n\n(tensor(648.87), torch.Size([1000, 1]))\n\n\n\ndiff.g = -1 * loss.g; diff[:5], diff.shape\n\n(tensor([[-15.34],\n         [-33.46],\n         [-35.26],\n         [ -6.92],\n         [-21.55]]),\n torch.Size([1000, 1]))\n\n\n\n(w2.shape, diff.g.shape), (w2.T.shape, diff.g[:, None].shape)\n\n((torch.Size([50, 1]), torch.Size([1000, 1])),\n (torch.Size([1, 50]), torch.Size([1000, 1, 1])))\n\n\n\n(diff.g @ w2.T).shape\n\ntorch.Size([1000, 50])\n\n\n\nl2.g = diff.g @ w2.T; l2.g.shape\n\ntorch.Size([1000, 50])\n\n\n\n(l1 &gt; 0).float()\n\ntensor([[0., 1., 1.,  ..., 0., 0., 0.],\n        [0., 0., 1.,  ..., 1., 0., 0.],\n        [1., 1., 1.,  ..., 0., 0., 1.],\n        ...,\n        [0., 0., 0.,  ..., 0., 1., 0.],\n        [1., 1., 0.,  ..., 0., 0., 1.],\n        [0., 0., 1.,  ..., 0., 0., 0.]])\n\n\n\nl1.g = l2.g * (l1 &gt; 0).float(); l1.g.shape\n\ntorch.Size([1000, 50])\n\n\n\n(l1.g.shape, trn_x.shape), (l1.g[:, None, :].shape, trn_x[..., None].shape)\n\n((torch.Size([1000, 50]), torch.Size([1000, 784])),\n (torch.Size([1000, 1, 50]), torch.Size([1000, 784, 1])))\n\n\n\nw1.g = tensor([1, 2])\n\n\nw1.g = (l1.g[:, None, :] * trn_x[..., None]).sum(0); w1.g.shape\n\ntorch.Size([784, 50])\n\n\n\n(w1.shape, w1.g.shape), (w1.g.min(), w1.g.max())\n\n((torch.Size([784, 50]), torch.Size([784, 50])),\n (tensor(-17.50), tensor(25.09)))\n\n\nLet’s verify our derivation is correct by comparing it to the gradients computed by PyTorch.\n\nw1_ = w1.clone().requires_grad_();\n\n\nl1 = relu(lin(trn_x, w1_, b1))\nl2 = lin(l1, w2, b2)\nloss = mse(l2, trn_y)\nloss.backward()\n\n\nw1_.grad\n\ntensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\n\n\n\n(w1.g.min(), w1.g.max()), (w1_.grad.min(), w1_.grad.max())\n\n((tensor(-17.50), tensor(25.09)), (tensor(-17.50), tensor(25.09)))\n\n\n\ntest_close(w1.g, w1_.grad, eps=0.01)\n\nIt is!\n\n\nb1 Gradients\nAs previously mentioned, I can reuse the computed gradients to calculate the gradients for \\(b_1\\). For now though, I will show the entire implemention for easy reference and later, when we will encapsulate the backward pass, I will reuse the already computed gradients.\n\ndiff = trn_y[:, None] - l2\nloss.g = (2/n) * diff\ndiff.g = loss.g * -1\nl2.g = diff.g @ w2.T\nl1.g = l2.g * (l1 &gt; 0).float()\nl1.g.shape, b1.shape\n\n(torch.Size([1000, 50]), torch.Size([50]))\n\n\n\nb1.g = (l1.g * 1).sum(0); b1.g.shape\n\ntorch.Size([50])\n\n\n\nb1.min(), b1.max()\n\n(tensor(0.), tensor(0.))\n\n\n\n\ntrn_x Gradients\n\ndiff = trn_y[:, None] - l2\nloss.g = (2/n) * diff\ndiff.g = loss.g * -1\nl2.g = diff.g @ w2.T\nl1.g = l2.g * (l1 &gt; 0).float()\nl1.g.shape, w1.shape\n\n(torch.Size([1000, 50]), torch.Size([784, 50]))\n\n\n\ntrn_x.g = l1.g @ w1.T\n\n\ntrn_x.g.min(), trn_x.g.max()\n\n(tensor(-2.85, grad_fn=&lt;MinBackward1&gt;), tensor(2.85, grad_fn=&lt;MaxBackward1&gt;))\n\n\n\n\nw1 Gradients\n\ndiff = trn_y[:, None] - l2\nloss.g = (2/n) * diff\ndiff.g = loss.g * -1\ndiff.g.shape, l1.shape\n\n(torch.Size([1000, 1]), torch.Size([1000, 50]))\n\n\n\n(diff.g * l1).sum(0, keepdim=True).T.shape\n\ntorch.Size([50, 1])\n\n\n\n(diff.g[:, None, :] * l1[..., None]).sum(0).shape\n\ntorch.Size([50, 1])\n\n\n\nw2.g = (diff.g[:, None, :] * l1[..., None]).sum(0); w2.g.shape\n\ntorch.Size([50, 1])\n\n\n\nw2.g.min(), w2.g.max()\n\n(tensor(8.37, grad_fn=&lt;MinBackward1&gt;), tensor(388.44, grad_fn=&lt;MaxBackward1&gt;))\n\n\n\n\nb2 Gradients\n\ndiff = trn_y[:, None] - l2\nloss.g = (2/n) * diff\ndiff.g = loss.g * -1\nb2.g = (diff.g * 1).sum(0)\nb2.g.shape, b2.shape\n\n(torch.Size([1]), torch.Size([1]))\n\n\n\n\nVerify\nLet’s verify our remaining gradients.\n\n\nw1_, b1_, w2_, b2_, trn_x_ = [lambda w: w.clone.requires_grad_() for w in [w1, b1, w2, b2, trn_x]]\n\nThe expression above does not work to create copies. Rather than returning a cloned copy that requires gradients, lambda objects will be returned.\n\n\nw1_, b1_, w2_, b2_, trn_x_ = map(lambda w: w.clone().requires_grad_(), [w1, b1, w2, b2, trn_x])\n\n\n\n\ntensor([[-2.81, -1.72, -0.97,  ..., -0.29, -1.62, -0.45],\n        [-1.77, -0.17,  1.32,  ..., -0.92,  0.76,  2.77],\n        [ 0.58,  2.13, -0.98,  ...,  0.41,  1.50,  0.86],\n        ...,\n        [-0.50, -1.90, -0.10,  ..., -1.61,  0.78, -0.09],\n        [ 0.89,  0.50,  1.21,  ...,  0.93, -0.37, -0.85],\n        [ 0.57, -0.50, -1.47,  ...,  0.72,  1.64, -0.85]], requires_grad=True)\n\n\nw1_\n\ntensor([[-2.81, -1.72, -0.97,  ..., -0.29, -1.62, -0.45],\n        [-1.77, -0.17,  1.32,  ..., -0.92,  0.76,  2.77],\n        [ 0.58,  2.13, -0.98,  ...,  0.41,  1.50,  0.86],\n        ...,\n        [-0.50, -1.90, -0.10,  ..., -1.61,  0.78, -0.09],\n        [ 0.89,  0.50,  1.21,  ...,  0.93, -0.37, -0.85],\n        [ 0.57, -0.50, -1.47,  ...,  0.72,  1.64, -0.85]], requires_grad=True)\n\n\n\nl1 = relu(lin(trn_x_, w1_, b1_))\nl2 = lin(l1, w2_, b2_)\nloss = mse(l2, trn_y)\nloss.backward()\n\n\nfor a, b in zip((w1, b1, w2, b2, trn_x), (w1_, b1_, w2_, b2_, trn_x_)): test_close(a.g, b.grad, eps=1e-2)\n\nAll comparisons passed!"
  },
  {
    "objectID": "forblog/posts/20_models_from_scratch.html#encapsulate",
    "href": "forblog/posts/20_models_from_scratch.html#encapsulate",
    "title": "Implementing a Neural Network from Scratch",
    "section": "Encapsulate",
    "text": "Encapsulate\nNow that we have the forward and backward passes sorted, let us cohesively bring them together.\n\ndef forward(inps, targs):\n  l1 = relu(lin(inps, w1, b1))\n  l2 = lin(l1, w2, b2)\n  loss = mse(l2, targs)\n  return l1, l2, loss\n\ndef backward(inps, targs, l1, l2, loss):\n  diff = targs[:, None] - l2\n  loss.g = (2 / n) * diff\n  diff.g = loss.g * -1\n\n  w2.g = (diff.g[:, None, :] * l1[..., None]).sum(0)\n  b2.g = (diff.g * 1).sum(0)\n\n  l2.g = diff.g @ w2.T\n  l1.g = l2.g * (l1 &gt; 0).float()\n\n  w1.g = (l1.g[:, None, :] * trn_x[..., None]).sum(0)\n  b1.g = (l1.g * 1).sum(0)\n\n  inps.g = l1.g @ w1.T\n\n\nl1, l2, loss = forward(trn_x, trn_y)\nbackward(trn_x, trn_y, l1, l2, loss)\n\n\ndef comp_grads(*ws):\n  for a, b in zip(ws, (w1_, b1_, w2_, b2_, trn_x_)): test_close(a.g, b.grad, eps=1e-2)\n\n\ncomp_grads(w1, b1, w2, b2, trn_x)\n\nThe backward function can be further refactored by taking the gradient computations of the linear layers common.\n\ndef backward(inps, targs, l1, l2, loss):\n  diff = targs[:, None] - l2\n  loss.g = (2/n) * diff\n  diff.g = loss.g * -1\n\n  lin_grad(l1, diff, w2, b2)\n  l2.g = diff.g @ w2.T\n  l1.g = l2.g * (l1 &gt; 0).float()\n  lin_grad(inps, l1, w1, b1)\n\ndef lin_grad(inp, out, w, b):\n  inp.g = out.g @ w.T\n  w.g = (out.g[:, None, :] * inp[..., None]).sum(0)\n  b.g = (out.g * 1).sum(0)\n\n\n\nPrevious implementation.\ndef backward(inps, targs, l1, l2, loss):\n  diff = targs[:, None] - l2\n  loss.g = (2 / n) * diff\n  diff.g = loss.g * -1\n\n  w2.g = (diff.g[:, None, :] * l1[..., None]).sum(0)\n  b2.g = (diff.g * 1).sum(0)\n\n  l2.g = diff.g @ w2.T\n  l1.g = l2.g * (l1 &gt; 0).float()\n\n  w1.g = (l1.g[:, None, :] * trn_x[..., None]).sum(0)\n  b1.g = (l1.g * 1).sum(0)\n\n  inps.g = l1.g @ w1.T\n\nbackward(trn_x, trn_y, *forward(trn_x, trn_y))\n\n\ncomp_grads(w1, b1, w2, b2, trn_x)\n\n\nClass\nCurrently, we have functions that each separately handle a part of the network. For instance, mse only computes its respective portion of the forward pass: the mean squared error. backward is a separate function that handles the backward pass for all pieces of the network.\nLet us change how this works, so each piece of the network also handles its respective backward pass. This means, mse will have the ability to compute both its forward pass and backward pass.\n\nclass MSE:\n  def __call__(self, inp, targs):\n    self.inp,self.targs = inp,targs\n    self.out = (inp[:, 0] - targs).pow(2).mean()\n    return self.out\n  \n  def backward(self): self.inp.g = (2 / self.inp.shape[0]) * (self.inp[:, 0] - self.targs)[..., None]\n\n\ntest_eq(MSE()(preds, trn_y), mse(preds, trn_y))\n\n\nclass Lin:\n  def __init__(self, w, b): self.w,self.b = w,b\n\n  def __call__(self, inp):\n    self.inp = inp\n    self.out = self.inp @ self.w + self.b\n    return self.out\n  \n  def backward(self):\n    self.inp.g = self.out.g @ self.w.T\n    self.w.g = (self.out.g[:, None, :] * self.inp[..., None]).sum(0)\n    self.b.g = self.out.g.sum(0)\n\n\ntest_eq(Lin(w1, b1)(trn_x), lin(trn_x, w1, b1))\n\n\nclass ReLU:\n  def __call__(self, inp):\n    self.inp = inp\n    self.out = self.inp.clamp_min(0.)\n    return self.out\n  \n  def backward(self): self.inp.g = self.out.g * (self.inp &gt; 0).float()\n\n\ntest_eq(ReLU()(l1), relu(l1))\n\n\nclass Model:\n  def __init__(self, w1, b1, w2, b2):\n    self.layers = [Lin(w1, b1), ReLU(), Lin(w2, b2)]\n    self.loss = MSE()\n  \n  def __call__(self, inp, targs):\n    for l in self.layers: inp = l(inp)\n    return self.loss(inp, targs)\n  \n  def backward(self):\n    self.loss.backward()\n    for l in self.layers[::-1]: l.backward()\n\n\nmodel = Model(w1, b1, w2, b2)\nl = model(trn_x, trn_y)\nmodel.backward()\n\n\ncomp_grads(w1, b1, w2, b2, trn_x)\n\n\n\nSuper Class\nThe classes we have created have common functionality, meaning their is still room for further refactoring. In particular, all the classes store the forward pass arguments as attributes if needed, have a __call__ dunder method that exectutes the forward pass, and a backward method for the backward pass.\n\nclass Module():\n  def __call__(self, *args):\n    self.args = args\n    self.out = self.forward(*args)\n    return self.out\n  \n  def forward(self): raise Exception('Forward pass not implemented')\n  def backward(self): self.bwd(self.out, *self.args)\n  def bwd(self): raise Exception('Backward pass not implemented.')\n\n\nclass MSE(Module):\n  def forward(self, inp, targs): return (inp[:, 0] - targs).pow(2).mean()\n  def bwd(self, out, inp, targs): inp.g = (2 / inp.shape[0]) * (inp[:, 0] - targs)[..., None]\n\n\ntest_eq(MSE()(preds, trn_y), mse(preds, trn_y))\n\n\nclass Lin(Module):\n  def __init__(self, w, b): self.w,self.b = w,b\n  def forward(self, inp): return inp @ self.w + self.b\n  def bwd(self, out, inp):\n    inp.g = out.g @ self.w.T\n    self.w.g = (out.g[:, None, :] * inp[..., None]).sum(0)\n    self.b.g = out.g.sum(0)\n    \n\n\ntest_eq(Lin(w1, b1)(trn_x), lin(trn_x, w1, b1))\n\n\nclass ReLU(Module):\n  def forward(self, inp): return inp.clamp_min(0.)\n  def bwd(self, out, inp): inp.g = out.g * (inp &gt; 0).float()\n\n\ntest_eq(ReLU()(l1), relu(l1))\n\n\nmodel = Model(w1, b1, w2, b2)\nloss = model(trn_x, trn_y)\nmodel.backward()\n\n\ncomp_grads(w1, b1, w2, b2)\n\nAnd with that, this is the basic underlying paradigm in which PyTorch implements its components.\nSo let us now directly use PyTorch’s nn.Module to handle our components. There is an added benefit that nn.Module automatically keeps track of our gradients, so we do not need to implement the backward pass.\n\n\nPyTorch’s nn.Module\n\nw1.shape, n, m, c, b1.shape\n\n(torch.Size([784, 50]), 1000, 784, tensor(10), torch.Size([50]))\n\n\n\nfrom torch import nn\nclass Linear(nn.Module):\n  def __init__(self, n_inps, n_outs):\n    super().__init__()\n    self.w = torch.randn(n_inps, n_outs).requires_grad_()\n    self.b = torch.randn(n_outs).requires_grad_()\n  \n  def forward(self, inp): return inp @ self.w + self.b\n\n\nF = nn.functional\nclass Model(nn.Module):\n  def __init__(self, n_inp, nh, n_out):\n    super().__init__()\n    self.layers = [Linear(n_inp, nh), nn.ReLU(), Linear(nh, n_out)]\n  \n  def __call__(self, inp, targ):\n    for l in self.layers: inp = l(inp)\n    return F.mse_loss(inp, targ[:, None])\n\n\nmodel = Model(m, nh, 1)\nloss = model(trn_x, trn_y.float())\nloss.backward()\n\n\nmodel.layers\n\n[Linear(), ReLU(), Linear()]\n\n\n\nl0 = model.layers[0]; l0.b.grad\n\ntensor([ 42.11, -25.91,   0.15,  15.73, -16.16,  41.61,  13.73,  81.32,  -8.91,  55.30, -14.12, -82.24,  12.02, -27.58,  -9.48, -90.85,\n        -25.55,  34.89,  -0.68, -14.24,   4.73,  49.70, -27.02,  19.55,  10.14,  38.86,  30.55,  74.17,   2.15,  -2.62, -37.11,  14.04,\n        -12.12,   0.89,  -0.99,  -6.29,  -1.15,  12.26,  -9.73,  -4.13,  -1.53,   1.67,   1.34,  -9.78,  20.50,   7.30,  62.45,   5.94,\n         -3.28, -18.14])"
  },
  {
    "objectID": "forblog/posts/20_models_from_scratch.html#cross-entropy-loss",
    "href": "forblog/posts/20_models_from_scratch.html#cross-entropy-loss",
    "title": "Implementing a Neural Network from Scratch",
    "section": "Cross Entropy Loss",
    "text": "Cross Entropy Loss\nLet’s now implement a much more appropriate loss function for our multi-target problem: cross entropy loss.\n\n\nRedefinition of Model, but without with loss function.\nclass Model(nn.Module):\n  def __init__(self, n_inps, nh, n_outs):\n    super().__init__()\n    self.layers = [nn.Linear(n_inps, nh), nn.ReLU(), nn.Linear(nh, n_outs)]\n\n  def __call__(self, x):\n    for l in self.layers: x = l(x)\n    return x\n\n\n\nmodel = Model(m, nh, c)\npreds = model(trn_x); preds.shape\n\ntorch.Size([1000, 10])\n\n\nAs I have defined here, cross entropy loss simply involves taking the logarithm of the softmax function, and multiplying the results with the one hot encoded targets.\nSoftmax, a multi-class generalization of the sigmoid function, involves taking the exponent of each prediction, and dividing each resulting value with the sum of all predictions to the exponent.\n\\[\n\\text{S}(y_i) = \\frac{e^{y_i}}{\\sum_{j} e^{y_j}}\n\\]\n\n\n\n\n\n\nSigmoid Function Definition\n\n\n\n\\[\n\\sigma(y) = \\frac{1}{1 + e^{-y}}\n\\]\n\n\nLet’s begin by first taking the logarithm of the softmax function.\n\ndef log_softmax(x): return ((x.exp() / x.exp().sum(-1, keepdim=True))).log()\nlog_softmax(preds)\n\ntensor([[-2.40, -2.33, -2.25,  ..., -2.33, -2.40, -2.34],\n        [-2.37, -2.44, -2.21,  ..., -2.30, -2.34, -2.28],\n        [-2.37, -2.45, -2.16,  ..., -2.24, -2.40, -2.40],\n        ...,\n        [-2.36, -2.45, -2.20,  ..., -2.24, -2.39, -2.37],\n        [-2.34, -2.41, -2.28,  ..., -2.20, -2.53, -2.25],\n        [-2.43, -2.37, -2.21,  ..., -2.26, -2.40, -2.37]], grad_fn=&lt;LogBackward0&gt;)\n\n\n\nF.log_softmax(preds, dim=-1)\n\ntensor([[-2.40, -2.33, -2.25,  ..., -2.33, -2.40, -2.34],\n        [-2.37, -2.44, -2.21,  ..., -2.30, -2.34, -2.28],\n        [-2.37, -2.45, -2.16,  ..., -2.24, -2.40, -2.40],\n        ...,\n        [-2.36, -2.45, -2.20,  ..., -2.24, -2.39, -2.37],\n        [-2.34, -2.41, -2.28,  ..., -2.20, -2.53, -2.25],\n        [-2.43, -2.37, -2.21,  ..., -2.26, -2.40, -2.37]], grad_fn=&lt;LogSoftmaxBackward0&gt;)\n\n\n\ntest_close(log_softmax(preds).detach(), F.log_softmax(preds, dim=-1).detach())\n\nOur implementation involves division. According to the rule, \\(\\lg\\left(\\frac{a}{b}\\right) = \\lg(a) - \\lg(b)\\), we can simplify our computation by subtracting the numerators and denominators instead.\n\ndef log_softmax(x): return x.exp().log() - x.exp().sum(-1, keepdim=True).log()\n\n\nlog_softmax(preds)\n\ntensor([[-2.40, -2.33, -2.25,  ..., -2.33, -2.40, -2.34],\n        [-2.37, -2.44, -2.21,  ..., -2.30, -2.34, -2.28],\n        [-2.37, -2.45, -2.16,  ..., -2.24, -2.40, -2.40],\n        ...,\n        [-2.36, -2.45, -2.20,  ..., -2.24, -2.39, -2.37],\n        [-2.34, -2.41, -2.28,  ..., -2.20, -2.53, -2.25],\n        [-2.43, -2.37, -2.21,  ..., -2.26, -2.40, -2.37]], grad_fn=&lt;SubBackward0&gt;)\n\n\nOur implementation has an issue though: it is unstable. Anything involving exponents is inherently unstable. Have a large enough value, and we converge to infinity relatively quickly.\n\nfor x in range(0, 101, 10): print(f'e^{x}={torch.exp(tensor(x))}')\n\ne^0=1.0\ne^10=22026.46484375\ne^20=485165184.0\ne^30=10686474223616.0\ne^40=2.353852703404196e+17\ne^50=5.184705457665547e+21\ne^60=1.1420073962419164e+26\ne^70=2.515438700355918e+30\ne^80=5.540622484676759e+34\ne^90=inf\ne^100=inf\n\n\nFortunately, there is trick to overcoming this known as the LogSumExp simplification.\n\\[\n\\lg\\left(\\sum^n_{j=1} e^{x_j}\\right) = \\lg\\left(e^a \\sum^n_{j=1} \\frac{e^{x_j}}{e^a}\\right) = \\lg\\left(e^a \\sum^n_{j=1} e^{x_j - a}\\right) = a + \\lg\\left(\\sum^n_{j=1} e^{x_j - a}\\right)\n\\]\n\\(a\\) is the largest element in \\(x\\).\nTo begin, we need to get the largest value in each sample.\n\nmax = preds.max(-1)[0]; max.shape, preds.shape\n\n(torch.Size([1000]), torch.Size([1000, 10]))\n\n\n\n\n?torch.max\n\n\nDocstring:\n\nmax(input) -&gt; Tensor\n\n\n\nReturns the maximum value of all elements in the ``input`` tensor.\n\n\n\n.. warning::\n\n    This function produces deterministic (sub)gradients unlike ``max(dim=0)``\n\n\n\nArgs:\n\n    input (Tensor): the input tensor.\n\n\n\nExample::\n\n\n\n    &gt;&gt;&gt; a = torch.randn(1, 3)\n\n    &gt;&gt;&gt; a\n\n    tensor([[ 0.6763,  0.7445, -2.2369]])\n\n    &gt;&gt;&gt; torch.max(a)\n\n    tensor(0.7445)\n\n\n\n.. function:: max(input, dim, keepdim=False, *, out=None) -&gt; (Tensor, LongTensor)\n\n   :noindex:\n\n\n\nReturns a namedtuple ``(values, indices)`` where ``values`` is the maximum\n\nvalue of each row of the :attr:`input` tensor in the given dimension\n\n:attr:`dim`. And ``indices`` is the index location of each maximum value found\n\n(argmax).\n\n\n\nIf ``keepdim`` is ``True``, the output tensors are of the same size\n\nas ``input`` except in the dimension ``dim`` where they are of size 1.\n\nOtherwise, ``dim`` is squeezed (see :func:`torch.squeeze`), resulting\n\nin the output tensors having 1 fewer dimension than ``input``.\n\n\n\n.. note:: If there are multiple maximal values in a reduced row then\n\n          the indices of the first maximal value are returned.\n\n\n\nArgs:\n\n    input (Tensor): the input tensor.\n\n    dim (int): the dimension to reduce.\n\n    keepdim (bool): whether the output tensor has :attr:`dim` retained or not. Default: ``False``.\n\n\n\nKeyword args:\n\n    out (tuple, optional): the result tuple of two output tensors (max, max_indices)\n\n\n\nExample::\n\n\n\n    &gt;&gt;&gt; a = torch.randn(4, 4)\n\n    &gt;&gt;&gt; a\n\n    tensor([[-1.2360, -0.2942, -0.1222,  0.8475],\n\n            [ 1.1949, -1.1127, -2.2379, -0.6702],\n\n            [ 1.5717, -0.9207,  0.1297, -1.8768],\n\n            [-0.6172,  1.0036, -0.6060, -0.2432]])\n\n    &gt;&gt;&gt; torch.max(a, 1)\n\n    torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))\n\n\n\n.. function:: max(input, other, *, out=None) -&gt; Tensor\n\n   :noindex:\n\n\n\nSee :func:`torch.maximum`.\n\nType:      builtin_function_or_method\n\n\nThen we can simply implement the rest of the algorithm.\n\n(preds - max[..., None]).shape\n\ntorch.Size([1000, 10])\n\n\n\n# Output hidden to prevent endless scrolling.\nmax[..., None] + (preds - max[..., None]).exp().sum(-1, keepdim=True).log()\n\n\ntest_close(torch.exp(preds).sum(-1, keepdim=True).log(), max[..., None] + (preds - max[..., None]).exp().sum(-1, keepdim=True).log())\n\n\ndef logsumexp(x):\n  max = x.max(-1)[0]\n  return max[..., None] + (preds - max[..., None]).exp().sum(-1, keepdim=True).log()\n\n\nlogsumexp(preds).shape\n\ntorch.Size([1000, 1])\n\n\n\ntest_close(logsumexp(preds), preds.logsumexp(-1)[..., None])\n\nLet’s compare how quicker our new implemenation is compared to the previous one.\n\n%timeit log_softmax(preds)\n\n337 µs ± 75.8 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\ndef log_softmax(x): return x - logsumexp(x)\n\n\nlog_softmax(preds).shape\n\ntorch.Size([1000, 10])\n\n\n\n%timeit log_softmax(preds)\n\n190 µs ± 56 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\nMuch faster!\nAll that is left now is to multiply our softmax predictions with the one hot encoded targets, and sum the resulting vector. However, due to the nature of our targets, we can employ a nifty trick that removes the need to create a tensor of one hot encoded targets: integer array indexing.\n\nInteger Array Indexing\n\nt = tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]); t\n\ntensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])\n\n\nA fancy name for a simple concept, integer array indexing allows one to access elements in a tensor by simply specifing lists of indices.\n\nt[[0, 1, 2], [0, 1, 2]]\n\ntensor([1, 5, 9])\n\n\nIt is best to think of the tensor as a grid of coordinates, with the first coordinate representing the row, and the second coordinate representing the column. Elements 1, 5, and 9 are at (0, 0), (1, 1), and (2, 2).\n1, 6, and 8 are at (0, 0), (1, 2), and (2, 1)\n\nt[[0, 1, 2], [0, 2, 1]]\n\ntensor([1, 6, 8])\n\n\n3 and 8 are at (0, 2) and (2, 1).\n\nt[[0, 2], [2, 1]]\n\ntensor([3, 8])\n\n\nOur targets consist of the integers from 0 to 9. Each row, or sample, in our predictions tensor represents a set of probabilites for each target.\nThis means we can directly access the prediction for the correct target through integer array indexing.\n\ntrn_y[:3]\n\ntensor([5, 0, 4])\n\n\nThe targets for the first three samples are 5, 0, and, 4. Instead of manually specifying the targets when obtaining the predictions for the first three samples…\n\nsm_preds = log_softmax(preds); sm_preds.shape\n\ntorch.Size([1000, 10])\n\n\n\nsm_preds[0, 5], sm_preds[1, 0], sm_preds[2, 4]\n\n(tensor(-2.27, grad_fn=&lt;SelectBackward0&gt;),\n tensor(-2.37, grad_fn=&lt;SelectBackward0&gt;),\n tensor(-2.26, grad_fn=&lt;SelectBackward0&gt;))\n\n\n…we can use the targets themselves to directly obtain our predictions.\n\nsm_preds[[0, 1, 2], trn_y[:3]]\n\ntensor([-2.27, -2.37, -2.26], grad_fn=&lt;IndexBackward0&gt;)\n\n\nAnd now, our implementation can be completed.\n\ndef nll(preds, targs): return -preds[range(targs.shape[0]), targs].mean()\n\n\nloss = nll(sm_preds, trn_y); loss\n\ntensor(2.30, grad_fn=&lt;NegBackward0&gt;)\n\n\n\n1test_close(F.nll_loss(F.log_softmax(preds, -1), trn_y), loss, 1e-3)\n\n\n1\n\nThe difference between F.cross_entropy and F.nll_loss is that the former expects the input to be the raw model outputs, where as the latter expects the input to already be logarithmic probabilities. It can be said that F.nll_loss computes cross entropy loss by starting at an intemediary step."
  },
  {
    "objectID": "forblog/posts/20_models_from_scratch.html#basic-training-loop",
    "href": "forblog/posts/20_models_from_scratch.html#basic-training-loop",
    "title": "Implementing a Neural Network from Scratch",
    "section": "Basic Training Loop",
    "text": "Basic Training Loop\nOkay, now we have all the components of a machine that is the neural network:\n\nthe linear function,\nthe activation function,\nthe loss function,\nand the backward pass.\n\nIt is time to get the machine up and running as a whole. It’s time to get the training loop looping.\n\n\n\n\n\n\n\n\n\n\ngraph TB\n  A[Load Data] --&gt; B[Make Predictions] --&gt; C[Compute Loss] --&gt; D[Compute Gradients] --&gt; E[Update Weights] --&gt; F[Compute Metric] --&gt; A\n\n\n\n\n\n\n\n\n\n\n\n\nloss_func = F.cross_entropy\n\n\nbs = 50\nxb = trn_x[0:bs]\npreds = model(xb); preds[0], preds.shape\n\n(tensor([-0.08, -0.01,  0.08,  0.11, -0.02,  0.06,  0.13, -0.00, -0.08, -0.01], grad_fn=&lt;SelectBackward0&gt;),\n torch.Size([50, 10]))\n\n\n\nyb = trn_y[:bs]; yb\n\ntensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9,\n        3, 9, 8, 5, 9, 3])\n\n\n\nloss_func(preds, yb)\n\ntensor(2.30, grad_fn=&lt;NllLossBackward0&gt;)\n\n\nWe’ll use accuracy as our metric.\n\npreds.argmax(-1)\n\ntensor([6, 2, 2, 2, 5, 2, 5, 2, 5, 2, 2, 2, 3, 2, 5, 5, 2, 2, 2, 5, 6, 3, 5, 2, 5, 2, 2, 3, 3, 2, 2, 2, 5, 2, 2, 2, 2, 2, 2, 2, 5, 2, 5, 5,\n        2, 2, 2, 2, 5, 5])\n\n\n\n(preds.argmax(-1) == yb).sum()\n\ntensor(5)\n\n\n\ndef accuracy(preds, yb): return ((preds.argmax(-1) == yb).sum()) / yb.shape[0]\n\n\naccuracy(preds, yb)\n\ntensor(0.10)\n\n\n\ntest_close(accuracy(preds, yb), (preds.argmax(-1) == yb).float().mean())\n\n\ndef report(loss, preds, yb): print(f'Loss: {loss:.2f}; Accuracy: {accuracy(preds, yb):.2f}')\n\n\nlr, epochs = .5, 3\nxb, yb = trn_x[:bs], trn_y[:bs]\npreds = model(xb)\nreport(loss_func(preds, yb), preds, yb)\n\nLoss: 2.30; Accuracy: 0.10\n\n\nThe training loop can now be assembled.\n\nfor epoch in range(epochs):\n  for i in range(0, n, bs):\n    s = slice(i, min(n, bs+i))\n    xb, yb = trn_x[s], trn_y[s]\n    preds = model(xb)\n    loss = loss_func(preds, yb)\n    loss.backward()\n    with torch.no_grad():\n      for l in model.layers:\n        if hasattr(l, 'weight'):\n          l.weight -= l.weight.grad * lr\n          l.bias   -= l.bias.grad * lr\n          l.weight.grad.zero_()\n          l.bias  .grad.zero_()\n  report(loss, preds, yb)\n\nLoss: 1.01; Accuracy: 0.66\nLoss: 0.45; Accuracy: 0.88\nLoss: 0.37; Accuracy: 0.82\n\n\nLet’s take a closer look at how we slice: s = slice(i, min(n, bs+i)). We have to use min to prevent the slices from going out of bounds.\n\n\n?slice\n\n\nInit signature: slice(self, /, *args, **kwargs)\n\nDocstring:     \n\nslice(stop)\n\nslice(start, stop[, step])\n\n\n\nCreate a slice object.  This is used for extended slicing (e.g. a[0:10:2]).\n\nType:           type\n\nSubclasses:     \n\n\n\nfor i in range(0, n, bs): print(slice(i, min(n, bs+i)))\n\nslice(0, 50, None)\nslice(50, 100, None)\nslice(100, 150, None)\nslice(150, 200, None)\nslice(200, 250, None)\nslice(250, 300, None)\nslice(300, 350, None)\nslice(350, 400, None)\nslice(400, 450, None)\nslice(450, 500, None)\nslice(500, 550, None)\nslice(550, 600, None)\nslice(600, 650, None)\nslice(650, 700, None)\nslice(700, 750, None)\nslice(750, 800, None)\nslice(800, 850, None)\nslice(850, 900, None)\nslice(900, 950, None)\nslice(950, 1000, None)\n\n\nSimply adding bs to n at the end parameter for range will not work.\n\nfor i in range(0, n+bs, bs): print(slice(i, bs+i))\n\nslice(0, 50, None)\nslice(50, 100, None)\nslice(100, 150, None)\nslice(150, 200, None)\nslice(200, 250, None)\nslice(250, 300, None)\nslice(300, 350, None)\nslice(350, 400, None)\nslice(400, 450, None)\nslice(450, 500, None)\nslice(500, 550, None)\nslice(550, 600, None)\nslice(600, 650, None)\nslice(650, 700, None)\nslice(700, 750, None)\nslice(750, 800, None)\nslice(800, 850, None)\nslice(850, 900, None)\nslice(900, 950, None)\nslice(950, 1000, None)\nslice(1000, 1050, None)"
  },
  {
    "objectID": "forblog/posts/20_models_from_scratch.html#parameters-optimizers",
    "href": "forblog/posts/20_models_from_scratch.html#parameters-optimizers",
    "title": "Implementing a Neural Network from Scratch",
    "section": "Parameters & Optimizers",
    "text": "Parameters & Optimizers\nCurrently, we update our weights by checking whether a layer in our network has a weight attribute.\nfor epoch in range(epochs):\n  for i in range(0, n, bs):\n    s = slice(i, min(n, bs+i))\n    xb, yb = trn_x[s], trn_y[s]\n    preds = model(xb)\n    loss = loss_func(preds, yb)\n    loss.backward()\n    with torch.no_grad(): \n      for l in model.layers: \n        if hasattr(l, 'weight'): \n          l.weight -= l.weight.grad * lr \n          l.bias   -= l.bias.grad * lr \n          l.weight.grad.zero_() \n          l.bias  .grad.zero_() \n  report(loss, preds, yb)\nPyTorch actually keeps track which layers have weights. Let us explore.\nHere, PyTorch knows that our model has a linear layer with 3 inputs and 4 outputs.\n\nm1 = nn.Module()\nm1.foo = nn.Linear(3, 4); m1\n\nModule(\n  (foo): Linear(in_features=3, out_features=4, bias=True)\n)\n\n\n\nlist(m1.named_children())\n\n[('foo', Linear(in_features=3, out_features=4, bias=True))]\n\n\nIn a similar manner, we can access the layer’s parameters.\n\nlist(m1.foo.parameters())\n\n[Parameter containing:\n tensor([[-0.37,  0.20, -0.39],\n         [-0.47,  0.00,  0.18],\n         [ 0.51, -0.35,  0.36],\n         [ 0.12,  0.10, -0.03]], requires_grad=True),\n Parameter containing:\n tensor([ 0.31, -0.42,  0.35,  0.16], requires_grad=True)]\n\n\nHowever, this approach will require us to loop through all layers to access all parameters. PyTorch instead provides a way to directly return the parameters of all layers.\n\nlist(m1.parameters())\n\n[Parameter containing:\n tensor([[-0.37,  0.20, -0.39],\n         [-0.47,  0.00,  0.18],\n         [ 0.51, -0.35,  0.36],\n         [ 0.12,  0.10, -0.03]], requires_grad=True),\n Parameter containing:\n tensor([ 0.31, -0.42,  0.35,  0.16], requires_grad=True)]\n\n\n\nclass MLP(nn.Module):\n  def __init__(self, n_inps, nh, n_outs):\n    super().__init__()\n    self.l1 = nn.Linear(n_inps, nh)\n    self.l2 = nn.Linear(nh, n_outs)\n    self.relu = nn.ReLU()\n  \n  def forward(self, x): return self.l2(self.relu(self.l1(x)))\n\n\nn, m, nh, c\n\n(1000, 784, 50, tensor(10))\n\n\n\nmodel = MLP(m, nh, c); model.l1\n\nLinear(in_features=784, out_features=50, bias=True)\n\n\n\nmodel\n\nMLP(\n  (l1): Linear(in_features=784, out_features=50, bias=True)\n  (l2): Linear(in_features=50, out_features=10, bias=True)\n  (relu): ReLU()\n)\n\n\n\nfor name, l in model.named_children(): print(f'{name}: {l}')\n\nl1: Linear(in_features=784, out_features=50, bias=True)\nl2: Linear(in_features=50, out_features=10, bias=True)\nrelu: ReLU()\n\n\n\nfor p in model.parameters(): print(p.shape)\n\ntorch.Size([50, 784])\ntorch.Size([50])\ntorch.Size([10, 50])\ntorch.Size([10])\n\n\nSince we can directly access the parameters, we do not need to check whether a certain parameter exists.\n\ndef fit():\n  for epoch in range(epochs):\n    for i in range(0, n, bs):\n      s = slice(i, min(n, bs+i))\n      xb, yb = trn_x[s], trn_y[s]\n      preds = model(xb)\n      loss = loss_func(preds, yb)\n      loss.backward()\n      with torch.no_grad():\n        for p in model.parameters(): p -= p.grad * lr\n1        model.zero_grad()\n    report(loss, preds, yb)\n\n\n1\n\ntorch.zero_grad() can also be called directly on the model itself.\n\n\n\n\n\nfit()\n\nLoss: 0.84; Accuracy: 0.74\nLoss: 0.45; Accuracy: 0.88\nLoss: 0.37; Accuracy: 0.84\n\n\nLet us implement this functionality–where the model itself knows what its layers and parameters are–ourselves.\nTo do so, we will need to define the __setattr__ dunder method, where any submodules defined are registered as parameters of the model.\n\nclass MyModule:\n  def __init__(self, n_inps, nh, n_outs):\n    self._modules = {}\n    self.l1 = nn.Linear(n_inps, nh)\n    self.l2 = nn.Linear(nh, n_outs)\n  \n  def __setattr__(self, k, v):\n    if not k.startswith('_'): self._modules[k] = v\n1    super().__setattr__(k, v)\n  \n  def __repr__(self): return f'{self._modules}'\n\n  def parameters(self):\n    for l in self._modules.values(): yield from l.parameters()\n\n\n1\n\nclass MyModule is actually class MyModule(object)\n\n\n\n\n\nmdl = MyModule(m, nh, c); mdl, model\n\n({'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)},\n MLP(\n   (l1): Linear(in_features=784, out_features=50, bias=True)\n   (l2): Linear(in_features=50, out_features=10, bias=True)\n   (relu): ReLU()\n ))\n\n\n\nfor p in mdl.parameters(): print(p.shape)\n\ntorch.Size([50, 784])\ntorch.Size([50])\ntorch.Size([10, 50])\ntorch.Size([10])\n\n\n\nRegistering Modules\nTo use our original approach, where a list of layers are specified, we can use the add_module method provided by PyTorch.\n\n?nn.Module.add_module\n\n\nSignature:\n\nnn.Module.add_module(\n\n    self,\n\n    name: str,\n\n    module: Optional[ForwardRef('Module')],\n\n) -&gt; None\n\nDocstring:\n\nAdds a child module to the current module.\n\n\n\nThe module can be accessed as an attribute using the given name.\n\n\n\nArgs:\n\n    name (str): name of the child module. The child module can be\n\n        accessed from this module using the given name\n\n    module (Module): child module to be added to the module.\n\nFile:      ~/mambaforge/envs/default/lib/python3.10/site-packages/torch/nn/modules/module.py\n\nType:      function\n\n\n\n\nlayers = [nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c)]\n\n\nfrom functools import reduce\nclass Model(nn.Module):\n  def __init__(self, layers):\n    super().__init__()\n    self.layers = layers\n    for i, l in enumerate(self.layers): self.add_module(f'layer_{i}', l)\n\n1  def forward(self, x): return reduce(lambda val, layer: layer(val), self.layers, x)\n\n\n1\n\nIn essence, reduce uses the output of the function as input to the same function in the next iteration.\n\n\n\n\n\n\n?reduce\n\n\nDocstring:\n\nreduce(function, iterable[, initial]) -&gt; value\n\n\n\nApply a function of two arguments cumulatively to the items of a sequence\n\nor iterable, from left to right, so as to reduce the iterable to a single\n\nvalue.  For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates\n\n((((1+2)+3)+4)+5).  If initial is present, it is placed before the items\n\nof the iterable in the calculation, and serves as a default when the\n\niterable is empty.\n\nType:      builtin_function_or_method\n\n\n\nreduce(lambda x,y: x+y, [1, 2, 3, 4, 5])\n\n15\n\n\n\nmodel = Model(layers); model\n\nModel(\n  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n  (layer_1): ReLU()\n  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n)\n\n\n\nmodel(xb).shape\n\ntorch.Size([50, 10])\n\n\nAlternatively, nn.ModuleList can do the registration for us.\n\n\n?nn.ModuleList\n\n\nInit signature:\n\nnn.ModuleList(\n\n    modules: Optional[Iterable[torch.nn.modules.module.Module]] = None,\n\n) -&gt; None\n\nDocstring:     \n\nHolds submodules in a list.\n\n\n\n:class:`~torch.nn.ModuleList` can be indexed like a regular Python list, but\n\nmodules it contains are properly registered, and will be visible by all\n\n:class:`~torch.nn.Module` methods.\n\n\n\nArgs:\n\n    modules (iterable, optional): an iterable of modules to add\n\n\n\nExample::\n\n\n\n    class MyModule(nn.Module):\n\n        def __init__(self):\n\n            super().__init__()\n\n            self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n\n\n\n        def forward(self, x):\n\n            # ModuleList can act as an iterable, or be indexed using ints\n\n            for i, l in enumerate(self.linears):\n\n                x = self.linears[i // 2](x) + l(x)\n\n            return x\n\nInit docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\n\nFile:           ~/mambaforge/envs/default/lib/python3.10/site-packages/torch/nn/modules/container.py\n\nType:           type\n\nSubclasses:     ParametrizationList\n\n\n\nclass SequentialModel(nn.Module):\n  def __init__(self, layers):\n    super().__init__()\n    self.layers = nn.ModuleList(layers)\n  \n  def forward(self, x): return reduce(lambda x, layer: layer(x), self.layers, x)\n\n\nmodel = SequentialModel(layers); model\n\nSequentialModel(\n  (layers): ModuleList(\n    (0): Linear(in_features=784, out_features=50, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=50, out_features=10, bias=True)\n  )\n)\n\n\n\nfit()\n\nLoss: 0.93; Accuracy: 0.78\nLoss: 0.52; Accuracy: 0.86\nLoss: 0.38; Accuracy: 0.86\n\n\n\nmodel = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c)); model\n\nSequential(\n  (0): Linear(in_features=784, out_features=50, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=50, out_features=10, bias=True)\n)\n\n\n\nfit()\n\nLoss: 0.88; Accuracy: 0.74\nLoss: 0.48; Accuracy: 0.86\nLoss: 0.39; Accuracy: 0.88\n\n\n\n\nOptimizer\nOptimizer is simply the name given to the algorithm that updates the weights.\n\nclass Optimizer:\n  def __init__(self, params, lr=0.5): self.params,self.lr = list(params), lr\n\n  def step(self):\n    with torch.no_grad():\n      for p in self.params: p -= p.grad * self.lr\n  \n  def zero_grad(self):\n    for p in self.params: p.grad.data.zero_()\n\n\nmodel = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))\nopt = Optimizer(model.parameters())\n\nThe weight update step can now be cleaned up by using opt.step() and opt.zero_grad() instead.\ndef fit():\n  for epoch in range(epochs):\n    for i in range(0, n, bs):\n      s = slice(i, min(n, i+bs))\n      xb, yb = trn_x[s], trn_y[s]\n      preds = model(xb)\n      loss = loss_func(preds, yb)\n      loss.backward()\n      with torch.no_grad(): \n        for p in model.parameters(): p -= p.grad * lr \n        model.zero_grad() \n    report(loss, preds, yb)\n\ndef fit():\n  for epoch in range(epochs):\n    for i in range(0, n, bs):\n      s = slice(i, min(n, i+bs))\n      xb, yb = trn_x[s], trn_y[s]\n      preds = model(xb)\n      loss = loss_func(preds, yb)\n      loss.backward()\n      opt.step() \n      opt.zero_grad() \n    report(loss, preds, yb)\n\n\nfit()\n\nLoss: 0.89; Accuracy: 0.74\nLoss: 0.51; Accuracy: 0.88\nLoss: 0.41; Accuracy: 0.86\n\n\n\nfrom torch import optim\ndef get_model():\n  model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))\n  return model, optim.SGD(model.parameters(), lr=lr)\n\n\nmodel, opt = get_model()\nloss_func(model(xb), yb)\n\ntensor(2.32, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\nfit()\n\nLoss: 0.82; Accuracy: 0.78\nLoss: 0.42; Accuracy: 0.90\nLoss: 0.35; Accuracy: 0.86"
  },
  {
    "objectID": "forblog/posts/20_models_from_scratch.html#dataset-and-dataloader",
    "href": "forblog/posts/20_models_from_scratch.html#dataset-and-dataloader",
    "title": "Implementing a Neural Network from Scratch",
    "section": "Dataset and Dataloader",
    "text": "Dataset and Dataloader\nI sometimes get confuzzled between the two terms, with regard to what each component actually does. The best way to think about these terms is that a dataset simply stores data in a massive warehouse, while a dataloader takes data from the dataset and tosses them into crates known as batches.\nAs it currently is, we iterate through our dataset by obtaining a slice object, and then slicing out some data to form a batch.\nfor i in range(0, n, bs):\n  s = slice(i, min(n, bs+i))\n  xb, yb = trn_x[s], trn_y[s]\nWe will now simplify how we approach this logic.\n\nDataset\nThe first point of simplification is to create a single dataset that will return both a sample and its associated target, from a single index. This will prevent us from having to index into two separate tensors.\n\nclass Dataset():\n  def __init__(self, x, y): self.x, self.y = x, y\n  def __len__(self): return len(self.x)\n  def __getitem__(self, i): return self.x[i], self.y[i]\n\n\ntrn_ds, vld_ds = Dataset(trn_x, trn_y), Dataset(vld_x, vld_y)\nassert len(trn_ds) == len(trn_x)\nassert len(vld_ds) == len(vld_x)\n\n\nxb, yb = trn_ds[0:5]\nassert xb.shape == (5, 28*28)\nassert yb.shape == (5,)\nxb, yb\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([5, 0, 4, 1, 9]))\n\n\n\nmodel, opt = get_model()\nfor epoch in range(epochs):\n  for i in range(0, n, bs):\n    xb, yb = trn_ds[i:min(n, bs+i)] \n    preds = model(xb)\n    loss = loss_func(preds, yb)\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n  report(loss, preds, yb)\n\nLoss: 1.19; Accuracy: 0.70\nLoss: 0.50; Accuracy: 0.88\nLoss: 0.34; Accuracy: 0.88\n\n\n\n\nDataLoader\nLet us now abstract away how the data from our datasets is loaded, by putting the logic that fetches data from the dataset…\nfor i in range(0, n, bs):\n  xb, yb = trn_ds[i:min(n,i+bs)] \n  ...\n…into a class that we can call a dataloader.\nfor xb, yb in train_dl: \n  ...\n\nclass DataLoader():\n  def __init__(self, ds, bs): self.ds,self.bs = ds,bs\n  def __iter__(self):\n    for i in range(0, len(self.ds), self.bs): yield self.ds[i:min(len(self.ds), i+self.bs)]\n\n\ntrn_dl, vld_dl = DataLoader(trn_ds, bs), DataLoader(vld_ds, bs)\n\n\nnext(iter(vld_dl))\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([3, 8, 6, 9, 6, 4, 5, 3, 8, 4, 5, 2, 3, 8, 4, 8, 1, 5, 0, 5, 9, 7, 4, 1, 0, 3, 0, 6, 2, 9, 9, 4, 1, 3, 6, 8, 0, 7, 7, 6, 8, 9, 0, 3,\n         8, 3, 7, 7, 8, 4]))\n\n\n\nxb, yb = next(iter(vld_dl)); xb.shape\n\ntorch.Size([50, 784])\n\n\n\nimport matplotlib.pyplot as plt\nplt.imshow(xb[0].view(28, 28)); yb[0]\n\ntensor(3)\n\n\n\n\n\n\n\n\n\n\ndef fit():\n  for epoch in range(epochs):\n    for xb, yb in trn_dl: \n      preds = model(xb)\n      loss = loss_func(preds, yb)\n      loss.backward()\n      opt.step()\n      opt.zero_grad()\n    report(loss, preds, yb)\n\n\nmodel, opt = get_model()\nfit()\n\nLoss: 0.79; Accuracy: 0.82\nLoss: 0.49; Accuracy: 0.84\nLoss: 0.30; Accuracy: 0.88\n\n\nAnd just like that, we have abstracted our loading logic from three lines…\nfor i in range(0, n, bs):\n  s = slice(i, min(n, bs+i))\n  xb, yb = trn_x[s], trn_y[s]\n  ...\n…to a much more readable single line.\nfor xb, yb in trn_dl:\n  ...\n\n\nRandom Sampling\nSampling is the method by which the dataloader selects indices from the dataset to load. Sampling from the training set should be random (due to the nature of our data), but not for the validation set.\nTherefore, we will need to create an additional class for the our dataloader; a component that tells the dataloader from which indices to load data from the dataset.\n\nimport random\n?random.shuffle\n\n\nSignature: random.shuffle(x, random=None)\n\nDocstring:\n\nShuffle list x in place, and return None.\n\n\n\nOptional argument random is a 0-argument function returning a\n\nrandom float in [0.0, 1.0); if it is the default None, the\n\nstandard random.random will be used.\n\nFile:      ~/mambaforge/envs/default/lib/python3.10/random.py\n\nType:      method\n\n\n\n\nclass Sampler():\n  def __init__(self, ds, shuffle=False): self.n,self.shuffle = len(ds),shuffle\n  def __iter__(self):\n    res = list(range(self.n))\n    if self.shuffle: random.shuffle(res)\n    return iter(res)\n\n\nss = Sampler(trn_ds); ss\n\n&lt;__main__.Sampler at 0x150dddd80&gt;\n\n\n\n\ntry: print(next(ss))\nexcept: pass\n\nThis does not work because __iter__ is not being called. __iter__ only gets called when we wrap the class with iter().\n\ntry: print(next(iter(ss)))\nexcept: pass\n\n0\n\n\n\n\nit = iter(ss); it\n\n&lt;list_iterator at 0x150996fe0&gt;\n\n\n\nfor o in range(5): print(next(it))\n\n0\n1\n2\n3\n4\n\n\nThe Sampler currently returns a single index in each iteration. We need to change that so a number of indices (equal to our batch size) is returned in each iteration. We can do this through a fancy slicing function known as islice.\n\nfrom itertools import islice\n?islice\n\n\nInit signature: islice(self, /, *args, **kwargs)\n\nDocstring:     \n\nislice(iterable, stop) --&gt; islice object\n\nislice(iterable, start, stop[, step]) --&gt; islice object\n\n\n\nReturn an iterator whose next() method returns selected values from an\n\niterable.  If start is specified, will skip all preceding elements;\n\notherwise, start defaults to zero.  Step defaults to one.  If\n\nspecified as another value, step determines how many values are\n\nskipped between successive calls.  Works like a slice() on a list\n\nbut returns an iterator.\n\nType:           type\n\nSubclasses:     \n\n\n\niter returns a single element from an iterable at a time. islice is a type of iterator that returns \\(x\\) elements from an iterable at a time. It is an, erm, iterative slice.\n\nlist(islice(ss, 5))\n\n[0, 1, 2, 3, 4]\n\n\nLet’s define an additional class that takes a sampler, and assembles its output into batches.\n\nclass BatchSampler:\n  def __init__(self, sampler, bs, drop_last=False): store_attr()\n1  def __iter__(self): yield from chunked(iter(self.sampler), self.bs, drop_last=self.drop_last)\n\n\n1\n\nfastcore’s chunked function has the exact same functionality as islice, but with some extra quality of life features. This includes being able to specify how many chunks, or slices, we want back (rather than the number of elements in a chunk), as well as being able to specify whether we would like to drop, or keep, chunks that are smaller than our specified chunk size. This latter option is what we will use–it will abstract away the min check we use in our DataLoader (self.ds[i:min(len(self.ds), i+self.bs)]).\n\n\n\n\n\n\n??chunked\n\n\nSignature: chunked(it, chunk_sz=None, drop_last=False, n_chunks=None)\n\nSource:   \n\ndef chunked(it, chunk_sz=None, drop_last=False, n_chunks=None):\n\n    \"Return batches from iterator `it` of size `chunk_sz` (or return `n_chunks` total)\"\n\n    assert bool(chunk_sz) ^ bool(n_chunks)\n\n    if n_chunks: chunk_sz = max(math.ceil(len(it)/n_chunks), 1)\n\n    if not isinstance(it, Iterator): it = iter(it)\n\n    while True:\n\n        res = list(itertools.islice(it, chunk_sz))\n\n        if res and (len(res)==chunk_sz or not drop_last): yield res\n\n        if len(res)&lt;chunk_sz: return\n\nFile:      ~/mambaforge/envs/default/lib/python3.10/site-packages/fastcore/basics.py\n\nType:      function\n\n\n\nlist(islice(ss, 5))\n\n[0, 1, 2, 3, 4]\n\n\n\nlist(chunked(ss, 5))[:5]\n\n[[0, 1, 2, 3, 4],\n [5, 6, 7, 8, 9],\n [10, 11, 12, 13, 14],\n [15, 16, 17, 18, 19],\n [20, 21, 22, 23, 24]]\n\n\n\nbatches = BatchSampler(ss, 4)\nlist(islice(batches, 5))\n\n[[0, 1, 2, 3],\n [4, 5, 6, 7],\n [8, 9, 10, 11],\n [12, 13, 14, 15],\n [16, 17, 18, 19]]\n\n\n\n\nCollation\nThere is one last piece of the puzzle left. Each sample in our Dataset also stores its associated target. We need to split these apart when dataloading. In other words, we need to split the data and target in each sample into their own batches; into an x batch and a y batch.\n\ndef collate(b):\n  xs, ys = zip(*b)\n  return torch.stack(xs), torch.stack(ys)\n\n\nclass DataLoader():\n  def __init__(self, ds, batches, collate_fn=collate): store_attr()\n  def __iter__(self): yield from (self.collate_fn(self.ds[i] for i in b) for b in self.batches) \n\nLet’s breakdown the latter line and explore what it does, piece by piece.\n\ntrn_samp = BatchSampler(Sampler(trn_ds, shuffle=True), bs)\nvld_samp = BatchSampler(Sampler(vld_ds, shuffle=False), bs)\n\nfor b in self.batches, we loop through each batch.\n\nb = next(iter(vld_samp)); b[:10]\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nself.ds[i] for i in b; using the indices in each batch, we access the respective samples in the dataset.\n\np = [vld_ds[i] for i in b]; len(p)\n\n50\n\n\nAs can be seen below, p also stores the target.\n\np[0]\n\n(tensor([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.18, 0.62, 0.76, 0.80, 0.28, 0.34, 0.05, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.05, 0.93, 0.99, 0.99, 0.99,\n         0.99, 0.99, 0.89, 0.33, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.05, 0.77, 0.69, 0.50, 0.69, 0.81, 0.92, 0.96, 0.87, 0.09, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.08, 0.54, 0.99, 0.37, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.30, 0.99,\n         0.56, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.07, 0.78, 0.99, 0.66, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.18, 0.85, 0.99, 0.84, 0.11, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.37, 0.88, 0.99, 0.96, 0.25, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.05, 0.50, 0.98, 0.99, 0.92,\n         0.16, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.67, 0.99, 0.99, 0.66, 0.23, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.81, 0.99, 0.99, 0.25, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.54, 0.99, 0.99, 0.98, 0.57, 0.10, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.04, 0.68, 0.88,\n         0.99, 0.99, 0.90, 0.28, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.03, 0.05, 0.99, 0.99, 0.99, 0.96, 0.41, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.18, 0.74, 0.99, 0.99, 0.88, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.04, 0.00, 0.00, 0.00, 0.00, 0.00, 0.07, 0.68, 0.99,\n         0.99, 0.10, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.14, 0.90, 0.61, 0.44,\n         0.34, 0.73, 0.75, 0.85, 0.99, 0.99, 0.86, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.47, 1.00, 0.99, 0.99, 0.99, 0.99, 1.00, 0.99, 0.99, 0.95, 0.26, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.54, 1.00, 0.99, 0.99, 0.99, 0.99, 1.00, 0.67, 0.18, 0.09, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.02, 0.28, 0.64, 0.74, 0.68, 0.68, 0.26, 0.02,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n tensor(3))\n\n\nThen we simply run the collate function.\n\nxs, ys = zip(*p); ys\n\n(tensor(3),\n tensor(8),\n tensor(6),\n tensor(9),\n tensor(6),\n tensor(4),\n tensor(5),\n tensor(3),\n tensor(8),\n tensor(4),\n tensor(5),\n tensor(2),\n tensor(3),\n tensor(8),\n tensor(4),\n tensor(8),\n tensor(1),\n tensor(5),\n tensor(0),\n tensor(5),\n tensor(9),\n tensor(7),\n tensor(4),\n tensor(1),\n tensor(0),\n tensor(3),\n tensor(0),\n tensor(6),\n tensor(2),\n tensor(9),\n tensor(9),\n tensor(4),\n tensor(1),\n tensor(3),\n tensor(6),\n tensor(8),\n tensor(0),\n tensor(7),\n tensor(7),\n tensor(6),\n tensor(8),\n tensor(9),\n tensor(0),\n tensor(3),\n tensor(8),\n tensor(3),\n tensor(7),\n tensor(7),\n tensor(8),\n tensor(4))\n\n\nAnd there we have our collated x and y batches!\n\ntorch.stack(ys)\n\ntensor([3, 8, 6, 9, 6, 4, 5, 3, 8, 4, 5, 2, 3, 8, 4, 8, 1, 5, 0, 5, 9, 7, 4, 1, 0, 3, 0, 6, 2, 9, 9, 4, 1, 3, 6, 8, 0, 7, 7, 6, 8, 9, 0, 3,\n        8, 3, 7, 7, 8, 4])\n\n\n\ntrn_samp = BatchSampler(Sampler(trn_ds, shuffle=True), bs)\nvld_samp = BatchSampler(Sampler(vld_ds, shuffle=False), bs)\n\n\ntrn_dl = DataLoader(trn_ds, batches=trn_samp)\nvld_dl = DataLoader(vld_ds, batches=vld_samp)\n\n\nxb, yb = next(iter(vld_dl))\nplt.imshow(xb[0].view(28, 28))\nyb[0]\n\ntensor(3)\n\n\n\n\n\n\n\n\n\n\nxb.shape, yb.shape\n\n(torch.Size([50, 784]), torch.Size([50]))\n\n\n\nmodel, opt = get_model()\nfit()\n\nLoss: 1.03; Accuracy: 0.74\nLoss: 0.46; Accuracy: 0.82\nLoss: 0.30; Accuracy: 0.90\n\n\nWe do not need to update the fit() function, as its logic remains the same despite our changes to the dataloader.\n\n??fit\n\n\nSignature: fit()\n\nDocstring: &lt;no docstring&gt;\n\nSource:   \n\ndef fit():\n\n  for epoch in range(epochs):\n\n    for xb, yb in trn_dl: # &lt;&lt;\n\n      preds = model(xb)\n\n      loss = loss_func(preds, yb)\n\n      loss.backward()\n\n      opt.step()\n\n      opt.zero_grad()\n\n    report(loss, preds, yb)\n\nFile:      /var/folders/fy/vg316qk1001227svr6d4d8l40000gn/T/ipykernel_52843/769712355.py\n\nType:      function\n\n\n\n\n\nMultiprocessing DataLoader\nWe can speed up how quickly data is loaded by using multiple CPU cores.\n\n%%timeit\nit = iter(trn_dl)\n\n227 ns ± 1.45 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\n\nimport torch.multiprocessing as mp\nclass DataLoader():\n  def __init__(self, ds, batches, n_workers=1, collate_fun=collate): store_attr()\n  def __iter__(self):\n    with mp.Pool(self.n_workers) as ex: yield from ex.map(self.ds.__getitem__, iter(self.batches))\n\n\ntrn_dl = DataLoader(trn_ds, batches=trn_samp, n_workers=4)\n\n\n%%timeit\nit = iter(trn_dl)\n\n197 ns ± 0.557 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nLet’s break down how exactly our __iter__ method works.\nWe slice batches by specifying a list of indices.\n\ntrn_ds[[3, 6, 8, 1]]\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([1, 1, 1, 0]))\n\n\nBehind the scenes, the square bracket notation calls the __getitem__ dunder method.\n\n??trn_ds.__getitem__\n\n\nSignature: trn_ds.__getitem__(i)\n\nDocstring: &lt;no docstring&gt;\n\nSource:      def __getitem__(self, i): return self.x[i], self.y[i]\n\nFile:      /var/folders/fy/vg316qk1001227svr6d4d8l40000gn/T/ipykernel_52843/694427655.py\n\nType:      method\n\n\n\nIn fact, we can index directly using __getitem__.\n\ntrn_ds.__getitem__([3, 6, 8, 1])\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([1, 1, 1, 0]))\n\n\nTherefore, by dividing our batches into smaller sets, we can take advantage of the __getitem__ dunder method to allow each CPU core to handle a separate set of items.\nSo we can divide our batches into smaller sets that each CPU core can manage.\n\nlen(list(map(trn_ds.__getitem__, ([3, 6], [8, 1]))))\n\n2\n\n\n\nfor o in map(trn_ds.__getitem__, ([3, 6], [8, 1])): print(o)\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 1]))\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 0]))\n\n\n\n\nSampling in PyTorch\n\nfrom torch.utils.data import DataLoader, SequentialSampler, RandomSampler, BatchSampler\n?BatchSampler\n\n\nInit signature:\n\nBatchSampler(\n\n    sampler: Union[torch.utils.data.sampler.Sampler[int], Iterable[int]],\n\n    batch_size: int,\n\n    drop_last: bool,\n\n) -&gt; None\n\nDocstring:     \n\nWraps another sampler to yield a mini-batch of indices.\n\n\n\nArgs:\n\n    sampler (Sampler or Iterable): Base sampler. Can be any iterable object\n\n    batch_size (int): Size of mini-batch.\n\n    drop_last (bool): If ``True``, the sampler will drop the last batch if\n\n        its size would be less than ``batch_size``\n\n\n\nExample:\n\n    &gt;&gt;&gt; list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n\n    [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n\n    &gt;&gt;&gt; list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n\n    [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n\nFile:           ~/mambaforge/envs/default/lib/python3.10/site-packages/torch/utils/data/sampler.py\n\nType:           type\n\nSubclasses:     \n\n\n\nPyTorch provides a wrapper which assembles the indices, sampled by our desired sampler, into batches.\n\n?RandomSampler\n\n\nInit signature:\n\nRandomSampler(\n\n    data_source: Sized,\n\n    replacement: bool = False,\n\n    num_samples: Optional[int] = None,\n\n    generator=None,\n\n) -&gt; None\n\nDocstring:     \n\nSamples elements randomly. If without replacement, then sample from a shuffled dataset.\n\nIf with replacement, then user can specify :attr:`num_samples` to draw.\n\n\n\nArgs:\n\n    data_source (Dataset): dataset to sample from\n\n    replacement (bool): samples are drawn on-demand with replacement if ``True``, default=``False``\n\n    num_samples (int): number of samples to draw, default=`len(dataset)`.\n\n    generator (Generator): Generator used in sampling.\n\nFile:           ~/mambaforge/envs/default/lib/python3.10/site-packages/torch/utils/data/sampler.py\n\nType:           type\n\nSubclasses:     \n\n\n\n\ntrn_samp = BatchSampler(    RandomSampler(trn_ds), bs, drop_last=False)\nvld_samp = BatchSampler(SequentialSampler(vld_ds), bs, drop_last=False)\n\nTo construct a dataloader with PyTorch, we have to provide the dataset and a sampler.\n\n?DataLoader\n\n\nInit signature:\n\nDataLoader(\n\n    dataset: torch.utils.data.dataset.Dataset[+T_co],\n\n    batch_size: Optional[int] = 1,\n\n    shuffle: Optional[bool] = None,\n\n    sampler: Union[torch.utils.data.sampler.Sampler, Iterable, NoneType] = None,\n\n    batch_sampler: Union[torch.utils.data.sampler.Sampler[Sequence], Iterable[Sequence], NoneType] = None,\n\n    num_workers: int = 0,\n\n    collate_fn: Optional[Callable[[List[~T]], Any]] = None,\n\n    pin_memory: bool = False,\n\n    drop_last: bool = False,\n\n    timeout: float = 0,\n\n    worker_init_fn: Optional[Callable[[int], NoneType]] = None,\n\n    multiprocessing_context=None,\n\n    generator=None,\n\n    *,\n\n    prefetch_factor: Optional[int] = None,\n\n    persistent_workers: bool = False,\n\n    pin_memory_device: str = '',\n\n)\n\nDocstring:     \n\nData loader. Combines a dataset and a sampler, and provides an iterable over\n\nthe given dataset.\n\n\n\nThe :class:`~torch.utils.data.DataLoader` supports both map-style and\n\niterable-style datasets with single- or multi-process loading, customizing\n\nloading order and optional automatic batching (collation) and memory pinning.\n\n\n\nSee :py:mod:`torch.utils.data` documentation page for more details.\n\n\n\nArgs:\n\n    dataset (Dataset): dataset from which to load the data.\n\n    batch_size (int, optional): how many samples per batch to load\n\n        (default: ``1``).\n\n    shuffle (bool, optional): set to ``True`` to have the data reshuffled\n\n        at every epoch (default: ``False``).\n\n    sampler (Sampler or Iterable, optional): defines the strategy to draw\n\n        samples from the dataset. Can be any ``Iterable`` with ``__len__``\n\n        implemented. If specified, :attr:`shuffle` must not be specified.\n\n    batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\n\n        returns a batch of indices at a time. Mutually exclusive with\n\n        :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\n\n        and :attr:`drop_last`.\n\n    num_workers (int, optional): how many subprocesses to use for data\n\n        loading. ``0`` means that the data will be loaded in the main process.\n\n        (default: ``0``)\n\n    collate_fn (Callable, optional): merges a list of samples to form a\n\n        mini-batch of Tensor(s).  Used when using batched loading from a\n\n        map-style dataset.\n\n    pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n\n        into device/CUDA pinned memory before returning them.  If your data elements\n\n        are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n\n        see the example below.\n\n    drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n\n        if the dataset size is not divisible by the batch size. If ``False`` and\n\n        the size of dataset is not divisible by the batch size, then the last batch\n\n        will be smaller. (default: ``False``)\n\n    timeout (numeric, optional): if positive, the timeout value for collecting a batch\n\n        from workers. Should always be non-negative. (default: ``0``)\n\n    worker_init_fn (Callable, optional): If not ``None``, this will be called on each\n\n        worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n\n        input, after seeding and before data loading. (default: ``None``)\n\n    generator (torch.Generator, optional): If not ``None``, this RNG will be used\n\n        by RandomSampler to generate random indexes and multiprocessing to generate\n\n        `base_seed` for workers. (default: ``None``)\n\n    prefetch_factor (int, optional, keyword-only arg): Number of batches loaded\n\n        in advance by each worker. ``2`` means there will be a total of\n\n        2 * num_workers batches prefetched across all workers. (default value depends\n\n        on the set value for num_workers. If value of num_workers=0 default is ``None``.\n\n        Otherwise if value of num_workers&gt;0 default is ``2``).\n\n    persistent_workers (bool, optional): If ``True``, the data loader will not shutdown\n\n        the worker processes after a dataset has been consumed once. This allows to\n\n        maintain the workers `Dataset` instances alive. (default: ``False``)\n\n    pin_memory_device (str, optional): the data loader will copy Tensors\n\n        into device pinned memory before returning them if pin_memory is set to true.\n\n\n\n\n\n.. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n\n             cannot be an unpicklable object, e.g., a lambda function. See\n\n             :ref:`multiprocessing-best-practices` on more details related\n\n             to multiprocessing in PyTorch.\n\n\n\n.. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n\n             When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n\n             it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\n\n             rounding depending on :attr:`drop_last`, regardless of multi-process loading\n\n             configurations. This represents the best guess PyTorch can make because PyTorch\n\n             trusts user :attr:`dataset` code in correctly handling multi-process\n\n             loading to avoid duplicate data.\n\n\n\n             However, if sharding results in multiple workers having incomplete last batches,\n\n             this estimate can still be inaccurate, because (1) an otherwise complete batch can\n\n             be broken into multiple ones and (2) more than one batch worth of samples can be\n\n             dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\n\n             cases in general.\n\n\n\n             See `Dataset Types`_ for more details on these two types of datasets and how\n\n             :class:`~torch.utils.data.IterableDataset` interacts with\n\n             `Multi-process data loading`_.\n\n\n\n.. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and\n\n             :ref:`data-loading-randomness` notes for random seed related questions.\n\nFile:           ~/mambaforge/envs/default/lib/python3.10/site-packages/torch/utils/data/dataloader.py\n\nType:           type\n\nSubclasses:     \n\n\n\n\ntrn_dl = DataLoader(trn_ds, batch_sampler=trn_samp, collate_fn=collate)\nvld_dl = DataLoader(vld_dl, batch_sampler=vld_samp, collate_fn=collate)\n\n\nmodel, opt = get_model()\nfit()\nloss_func(model(xb), yb), accuracy(model(xb), yb)\n\nLoss: 1.05; Accuracy: 0.64\nLoss: 0.69; Accuracy: 0.72\nLoss: 0.55; Accuracy: 0.84\n\n\n(tensor(1.02, grad_fn=&lt;NllLossBackward0&gt;), tensor(0.66))\n\n\nInstead of separately wrapping the RandomSampler and SequentialSampler classes, we can let the DataLoader class do this for us.\n\ntrn_dl = DataLoader(trn_ds, bs, sampler=    RandomSampler(trn_ds), collate_fn=collate)\nvld_dl = DataLoader(vld_ds, bs, sampler=SequentialSampler(trn_ds), collate_fn=collate)\n\nIn fact, we don’t even need to specify the sampler. All we have to do is toggle and set some parameters.\n\ntrn_dl = DataLoader(trn_ds, bs, shuffle=True, drop_last=True, num_workers=2)\nvld_dl = DataLoader(vld_ds, bs, shuffle=False, num_workers=2)\n\n\nmodel, opt = get_model(); fit()\n\nLoss: 0.80; Accuracy: 0.80\nLoss: 0.27; Accuracy: 0.94\nLoss: 0.40; Accuracy: 0.88\n\n\n\nloss_func(model(xb), yb), accuracy(model(xb), yb)\n\n(tensor(0.84, grad_fn=&lt;NllLossBackward0&gt;), tensor(0.68))\n\n\nAs our dataset already knows how to sample a batch of indices all at once, we can actually skip the batch_sampler and collate_fn entirely. 🙃\nclass Dataset():\n  def __init__(self, x, y): self.x, self.y = x, y\n  def __len__(self): return len(self.x)\n  def __getitem__(self, i): return self.x[i], self.y[i]\n\ntrn_ds[[4, 6, 7]]\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([9, 1, 3]))\n\n\n\ntrn_dl = DataLoader(trn_ds, sampler=trn_samp)\nvld_dl = DataLoader(vld_ds, sampler=vld_samp)\n\n\nxb, yb = next(iter(trn_dl)); xb.shape, yb.shape\n\n(torch.Size([1, 50, 784]), torch.Size([1, 50]))"
  },
  {
    "objectID": "forblog/posts/20_models_from_scratch.html#validation",
    "href": "forblog/posts/20_models_from_scratch.html#validation",
    "title": "Implementing a Neural Network from Scratch",
    "section": "Validation",
    "text": "Validation\nWhen training and evaluating a model, model.train() and model.eval() need to be called respectively. These methods are used by layers such as nn.BatchNorm2d and nn.Dropout to ensure appropriate behaviour during different phases of the process.\n\n?model.train\n\n\nSignature: model.train(mode: bool = True) -&gt; ~T\n\nDocstring:\n\nSets the module in training mode.\n\n\n\nThis has any effect only on certain modules. See documentations of\n\nparticular modules for details of their behaviors in training/evaluation\n\nmode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n\netc.\n\n\n\nArgs:\n\n    mode (bool): whether to set training mode (``True``) or evaluation\n\n                 mode (``False``). Default: ``True``.\n\n\n\nReturns:\n\n    Module: self\n\nFile:      ~/mambaforge/envs/default/lib/python3.10/site-packages/torch/nn/modules/module.py\n\nType:      method\n\n\n\n\n?model.eval\n\n\nSignature: model.eval() -&gt; ~T\n\nDocstring:\n\nSets the module in evaluation mode.\n\n\n\nThis has any effect only on certain modules. See documentations of\n\nparticular modules for details of their behaviors in training/evaluation\n\nmode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n\netc.\n\n\n\nThis is equivalent with :meth:`self.train(False) &lt;torch.nn.Module.train&gt;`.\n\n\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n\n`.eval()` and several similar mechanisms that may be confused with it.\n\n\n\nReturns:\n\n    Module: self\n\nFile:      ~/mambaforge/envs/default/lib/python3.10/site-packages/torch/nn/modules/module.py\n\nType:      method\n\n\n\n\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n  for epoch in range(epochs):\n    model.train()\n    for xb, yb in train_dl:\n      preds = model(xb)\n      loss = loss_func(preds, yb)\n      loss.backward()\n      opt.step()\n      opt.zero_grad()\n    \n    model.eval()\n    with torch.no_grad():\n      tot_loss, tot_acc, count = (0.,) * 3\n      for xb, yb in valid_dl:\n        preds = model(xb)\n        n = len(xb)\n        count += n\n        tot_loss += loss_func(preds, yb).item() * n\n        tot_acc  += accuracy (preds, yb).item() * n\n    print(epoch, tot_loss/count, tot_acc/count)\n  return tot_loss/count, tot_acc/count\n\n\ndef get_dls(trainn_ds, valid_ds, bs, **kwargs):\n  return (DataLoader(trn_ds, batch_size=bs,   shuffle=True, **kwargs),\n          DataLoader(vld_ds, batch_size=bs*2,               **kwargs))\n\n\ntrn_dl, vld_dl = get_dls(trn_ds, vld_ds, bs)\nmodel, opt = get_model()\n\n\n%time loss, acc = fit(5, model, loss_func, opt, trn_dl, vld_dl)\n\n0 1.3015430688858032 0.6180000007152557\n1 0.7089294970035553 0.7680000007152558\n2 0.6260120451450348 0.7990000009536743\n3 0.501511612534523 0.8490000128746032\n4 0.5909725487232208 0.8119999945163727\nCPU times: user 1.55 s, sys: 41.8 ms, total: 1.59 s\nWall time: 358 ms"
  },
  {
    "objectID": "forblog/posts/20_models_from_scratch.html#conclusion",
    "href": "forblog/posts/20_models_from_scratch.html#conclusion",
    "title": "Implementing a Neural Network from Scratch",
    "section": "Conclusion",
    "text": "Conclusion\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/25_current_ideas_in_spatial_understanding.html",
    "href": "forblog/posts/25_current_ideas_in_spatial_understanding.html",
    "title": "Current Ideas in Spatial Understanding",
    "section": "",
    "text": "This post was updated with a new paper on Friday, 18 July 2025. This post was updated with a new paper on Monday, 21 July 2025.\nI went through a bunch of papers I found relating to enhancing spatial understanding in VLMs to get an idea of what’s going on. Here, I’ve simply extracted the main idea that was explored. I haven’t evaluated the worthiness/effectiveness of the mentioned ideas.\nFrom what I’ve seen, I’ve noticed the following issues with VLMs:"
  },
  {
    "objectID": "forblog/posts/25_current_ideas_in_spatial_understanding.html#conclusion",
    "href": "forblog/posts/25_current_ideas_in_spatial_understanding.html#conclusion",
    "title": "Current Ideas in Spatial Understanding",
    "section": "Conclusion",
    "text": "Conclusion\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/17_meanshift_clustering.html",
    "href": "forblog/posts/17_meanshift_clustering.html",
    "title": "Implementing and Optimizing Meanshift Clustering",
    "section": "",
    "text": "This notebook follows the fastai style guide.\nMeanshift clustering is a technique for unsupervised learning. Give this algorithm a bunch of data and it will figure out what groups the data can be sorted into. It does this by iteratively moving all data points until they converge to a single point.\nThe steps of the algorithm can be summarized as follows:\nThis is the data we will work with to illustrate meanshift clustering. The data points are put into clearly seperate clusters for the sake of clarity.\nIn the end, all clusters will converge at their respective center (marked by X)."
  },
  {
    "objectID": "forblog/posts/17_meanshift_clustering.html#implementation",
    "href": "forblog/posts/17_meanshift_clustering.html#implementation",
    "title": "Implementing and Optimizing Meanshift Clustering",
    "section": "Implementation",
    "text": "Implementation\nLet’s start off simple and apply the algorithm to a single point.\n\nCalculate Distances\n\nFor each data point \\(x\\) in the dataset, calculate the distance between \\(x\\) and every other data point in the dataset.\n\n\ndata\n\ntensor([[  0.611, -20.199],\n        [  4.455, -24.188],\n        [  2.071, -20.446],\n        ...,\n        [ 25.927,   6.597],\n        [ 18.549,   3.411],\n        [ 24.617,   8.485]])\n\n\n\nX = data.clone(); X.shape\n\ntorch.Size([1500, 2])\n\n\nEach point has an \\(x\\) coordinate and a \\(y\\) coordinate.\n\nx = X[0, :]; x - X\n\ntensor([[  0.000,   0.000],\n        [ -3.844,   3.989],\n        [ -1.460,   0.247],\n        ...,\n        [-25.316, -26.796],\n        [-17.938, -23.610],\n        [-24.006, -28.684]])\n\n\nThe distance metric we’ll use is Euclidean distance — also better known as Pythagoras’ theorem.\n\\[\n\\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\n\\]\n\ndists = (x - X).square().sum(dim=1).sqrt(); dists\n\ntensor([ 0.000,  5.540,  1.481,  ..., 36.864, 29.651, 37.404])\n\n\n\n\nCalculate Weights\n\nCalculate weights for each point in the dataset by passing the calculated distances through the normal distribution.\n\nThe normal distribution is also known as the Gaussian distribution. A distribution is simply a way to describe how data is spread out — this isn’t applicable in our case. What is applicable is the shape of this distribution which we will use to calculate the weights.\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\n\\]\n\ndef gauss_kernel(x, mean, std):\n  return torch.exp(-(x - mean) ** 2 / (2 * std ** 2)) / (std * torch.sqrt(2 * tensor(torch.pi)))\n\nThis is how it looks like.\n\n\n                                                \n\n\nFrom the shape of this graph, we can see that larger values of \\(x\\) give smaller values of \\(y\\), which is what we want — longer distances should have smaller weights meaning they have a smaller effect on the new position of the point.\nWe can control the rate at which the weights go to zero by varying what’s known as the bandwidth, or the standard deviation. The graph above is generated with a bandwith of 2.5.\nThe graph below is generated with a bandwidth of 1.\n\n\n                                                \n\n\nLet’s get our weights now.\n\ngauss_kernel(dists, mean=0, std=2.5)\n\ntensor([    0.160,     0.014,     0.134,  ...,     0.000,     0.000,     0.000])\n\n\n\nbw = 2.5\nws = gauss_kernel(x=dists, mean=0, std=bw)\n\n\n\nMove the Point\n\nCalculate the weighted average for all points in the dataset. This weighted average is the new location for \\(x\\)\n\n\nws.shape, X.shape\n\n(torch.Size([1500]), torch.Size([1500, 2]))\n\n\n\nws[:, None].shape, X.shape\n\n(torch.Size([1500, 1]), torch.Size([1500, 2]))\n\n\nBelow is the formula for weighted average.\n\\[\n\\frac{\\sum wx}{\\sum w}\n\\]\nIn words, multiply each data point in the set with its corresponding weight and sum all products. Divide that with the sum of all weights.\n\nws[:, None] * X, ws[0] * X[0, :]\n\n(tensor([[     0.097,     -3.223],\n         [     0.061,     -0.331],\n         [     0.277,     -2.738],\n         ...,\n         [     0.000,      0.000],\n         [     0.000,      0.000],\n         [     0.000,      0.000]]),\n tensor([ 0.097, -3.223]))\n\n\nLet’s calculate the weighted average and assign it as the new location for our point \\(x\\).\n\nx = (ws[:, None] * X).sum(dim=0) / ws.sum(); x\n\ntensor([  1.695, -20.786])\n\n\nAnd there you have it! We just moved a single data point.\nLet’s do this for all data points and for a single iteration.\n\nfor i, x in enumerate(X):\n    dist = (x - X).square().sum(dim=1).sqrt()\n    ws = gauss_kernel(x=dist, mean=0, std=bw)\n    X[i] = (ws[:, None] * X).sum(dim=0) / ws.sum()\n\n\nplot_data(centroids+2, X, n_samples)\n\n                                                \n\n\nLet’s encapsulate the algorithm so we can run it for multiple iterations.\n\ndef update(X):\n    for i, x in enumerate(X):\n      dist = (x - X).square().sum(dim=1).sqrt()\n      ws = gauss_kernel(x=dist, mean=0, std=bw)\n      X[i] = (ws[:, None] * X).sum(dim=0) / ws.sum()\n\ndef meanshift(data):\n   X = data.clone()\n   for _ in range(5): update(X)\n   return X\n\n\nplot_data(centroids+2, meanshift(data), n_samples)\n\n                                                \n\n\nAll points have converged.\n\n%timeit -n 10 meanshift(data)\n\n1.7 s ± 282 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nThe algorithm took roughly 1.5 seconds to run 5 iterations. We’ll optimize the algorithm further in Optimized Implementation.\nAs we can see below, simply moving the algorithm to the GPU won’t help — in fact, it becamse a bit slower.\n\ndef update(X):\n    for i, x in enumerate(X):\n      dist = (x - X).square().sum(dim=1).sqrt()\n      ws = gauss_kernel(x=dist, mean=0, std=bw)\n      X[i] = (ws[:, None] * X).sum(dim=0) / ws.sum()\n\ndef meanshift(data):\n   X = data.clone().to('cuda')\n   for _ in range(5): update(X)\n   return X.detach().cpu()\n\n%timeit -n 10 meanshift(data)\n\n1.67 s ± 49.7 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\nAnimation\nLet’s see meanshift clustering happen in real time.\n\nX = data.clone()\nfig = plot_data(centroids+2, X, n_samples, display=False)\nfig.update_layout(xaxis_range=[-40, 40], yaxis_range=[-40, 40],  updatemenus=[dict(type='buttons', buttons=[\n    dict(label='Play', method='animate', args=[None]),\n    dict(label='Pause', method='animate', args=[[None], dict(frame_duration=0, frame_redraw='False', mode='immediate', transition_duration=0)])\n])])\n\nframes = [go.Frame(data=fig.data)]\nfor _ in range(5):\n    update(X)\n    frames.append(go.Frame(data=plot_data(centroids+2, X, n_samples, display=False).data))\nfig.frames = frames\nfig.show()"
  },
  {
    "objectID": "forblog/posts/17_meanshift_clustering.html#optimized-implementation",
    "href": "forblog/posts/17_meanshift_clustering.html#optimized-implementation",
    "title": "Implementing and Optimizing Meanshift Clustering",
    "section": "Optimized Implementation",
    "text": "Optimized Implementation\nThe implementation above is roughly 1.5s which is slow. Let’s perform the algorithm on multiple data points simulataneously. We’ll then move the operations onto the GPU.\n\nCalculate Distances\n\nFor each data point \\(x\\) in the dataset, calculate the distance between \\(x\\) and every other data point in the dataset.\n\n\nX = data.clone(); X.shape\n\ntorch.Size([1500, 2])\n\n\nWe’ll begin with a batch size of 8.\n\nbs = 8\nx = X[:bs, :]; x\n\ntensor([[  0.611, -20.199],\n        [  4.455, -24.188],\n        [  2.071, -20.446],\n        [  1.011, -23.082],\n        [  4.516, -22.281],\n        [ -0.149, -22.113],\n        [  4.029, -18.819],\n        [  2.960, -18.646]])\n\n\n\nx.shape, X.shape\n\n(torch.Size([8, 2]), torch.Size([1500, 2]))\n\n\n\nx[:, None, :].shape, X[None, ...].shape\n\n(torch.Size([8, 1, 2]), torch.Size([1, 1500, 2]))\n\n\n\nx[:, None, :] - X[None, ...]\n\ntensor([[[  0.000,   0.000],\n         [ -3.844,   3.989],\n         [ -1.460,   0.247],\n         ...,\n         [-25.316, -26.796],\n         [-17.938, -23.610],\n         [-24.006, -28.684]],\n\n        [[  3.844,  -3.989],\n         [  0.000,   0.000],\n         [  2.383,  -3.742],\n         ...,\n         [-21.472, -30.786],\n         [-14.094, -27.599],\n         [-20.162, -32.673]],\n\n        [[  1.460,  -0.247],\n         [ -2.383,   3.742],\n         [  0.000,   0.000],\n         ...,\n         [-23.856, -27.043],\n         [-16.477, -23.857],\n         [-22.546, -28.931]],\n\n        ...,\n\n        [[ -0.759,  -1.914],\n         [ -4.603,   2.076],\n         [ -2.220,  -1.667],\n         ...,\n         [-26.076, -28.710],\n         [-18.697, -25.523],\n         [-24.766, -30.598]],\n\n        [[  3.418,   1.380],\n         [ -0.426,   5.369],\n         [  1.958,   1.627],\n         ...,\n         [-21.898, -25.417],\n         [-14.520, -22.230],\n         [-20.588, -27.304]],\n\n        [[  2.349,   1.553],\n         [ -1.495,   5.542],\n         [  0.889,   1.800],\n         ...,\n         [-22.967, -25.243],\n         [-15.589, -22.057],\n         [-21.657, -27.131]]])\n\n\n\n(x[:, None, :] - X[None, ...]).shape\n\ntorch.Size([8, 1500, 2])\n\n\n\ndists = (x[:, None, :] - X[None, ...]).square().sum(dim=-1).sqrt(); dists, dists.shape\n\n(tensor([[ 0.000,  5.540,  1.481,  ..., 36.864, 29.651, 37.404],\n         [ 5.540,  0.000,  4.437,  ..., 37.534, 30.989, 38.394],\n         [ 1.481,  4.437,  0.000,  ..., 36.062, 28.994, 36.679],\n         ...,\n         [ 2.059,  5.050,  2.776,  ..., 38.784, 31.639, 39.364],\n         [ 3.686,  5.386,  2.546,  ..., 33.549, 26.552, 34.196],\n         [ 2.816,  5.740,  2.007,  ..., 34.128, 27.009, 34.715]]),\n torch.Size([8, 1500]))\n\n\n\n\nCalculate Weights\n\nCalculate weights for each point in the dataset by passing the calculated distances through the normal distribution.\n\nWe can simplify the guassian kernel to a triangular kernel and still achieve the same results, with less computation.\n\nplot_func(partial(gauss_kernel, mean=0, std=2.5))\n\n                                                \n\n\n\ndef tri_kernel(x, bw): return (-x+bw).clamp_min(0)/bw\nplot_func(partial(tri_kernel, bw=8))\n\n                                                \n\n\n\n%timeit gauss_kernel(dists, mean=0, std=2.5)\n\n311 µs ± 8.06 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n\n\n%timeit tri_kernel(dists, bw=8)\n\n25 µs ± 594 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\n\n\ngauss_kernel(dists, mean=0, std=2.5), tri_kernel(dists, bw=8)\n\n(tensor([[    0.160,     0.014,     0.134,  ...,     0.000,     0.000,     0.000],\n         [    0.014,     0.160,     0.033,  ...,     0.000,     0.000,     0.000],\n         [    0.134,     0.033,     0.160,  ...,     0.000,     0.000,     0.000],\n         ...,\n         [    0.114,     0.021,     0.086,  ...,     0.000,     0.000,     0.000],\n         [    0.054,     0.016,     0.095,  ...,     0.000,     0.000,     0.000],\n         [    0.085,     0.011,     0.116,  ...,     0.000,     0.000,     0.000]]),\n tensor([[1.000, 0.308, 0.815,  ..., 0.000, 0.000, 0.000],\n         [0.308, 1.000, 0.445,  ..., 0.000, 0.000, 0.000],\n         [0.815, 0.445, 1.000,  ..., 0.000, 0.000, 0.000],\n         ...,\n         [0.743, 0.369, 0.653,  ..., 0.000, 0.000, 0.000],\n         [0.539, 0.327, 0.682,  ..., 0.000, 0.000, 0.000],\n         [0.648, 0.282, 0.749,  ..., 0.000, 0.000, 0.000]]))\n\n\n\nws = tri_kernel(dists, bw=8); ws.shape\n\ntorch.Size([8, 1500])\n\n\n\n\nMove the Points\n\nCalculate the weighted average for all points in the dataset. This weighted average is the new location for \\(x\\)\n\n\nws.shape, X.shape\n\n(torch.Size([8, 1500]), torch.Size([1500, 2]))\n\n\n\nws[..., None].shape, X[None, ...].shape\n\n(torch.Size([8, 1500, 1]), torch.Size([1, 1500, 2]))\n\n\n\n(ws[..., None] * X[None, ...]).shape\n\ntorch.Size([8, 1500, 2])\n\n\n\n(ws[..., None] * X[None, ...]).sum(1).shape\n\ntorch.Size([8, 2])\n\n\n\n%timeit (ws[..., None] * X[None, ...]).sum(1)\n\n144 µs ± 31.2 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\n\nLet’s have another look at formula for weighted average.\n\\[\n\\frac{\\sum wx}{\\sum w}\n\\]\nThe numerator is actually the definition for matrix multiplication! Therefore we can speed up the operation above by using the @ operator!\n\n%timeit ws @ X\n\n7.64 µs ± 184 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n\n\nA roughly 40% speed up!\n\nx = (ws @ X) / ws.sum(dim=1, keepdim=True); x\n\ntensor([[  2.049, -20.954],\n        [  3.108, -21.923],\n        [  2.441, -21.021],\n        [  2.176, -21.616],\n        [  3.082, -21.466],\n        [  1.842, -21.393],\n        [  2.946, -20.632],\n        [  2.669, -20.594]])\n\n\nAnd there you have it! We performed this algorithm on 8 data points simultaneously!\nLet’s encapsulate the code so we can perform it over all data points and time it.\n\n?slice\n\n\nbs\n\n8\n\n\n\nmin(1508, 1500)\n\n1500\n\n\n\nX = data.clone()\nn = len(data)\nbs = 8\nfor i in range(0, n, bs):\n    s = slice(i, min(i+bs, n))\n    dists = (X[s][:, None, :] - X[None, ...]).square().sum(dim=-1).sqrt()\n    ws = egauss_kernel(dists, mean=0, std=2.5)\n    X[s] = (ws @ X) / ws.sum(dim=1, keepdim=True)\n\n\nplot_data(centroids+2, X, n_samples)\n\n                                                \n\n\n\ndef update(X):\n  for i in range(0, n, bs):\n    s = slice(i, min(i+bs, n))\n    dists = (X[s][:, None, :] - X[None, ...]).square().sum(dim=-1).sqrt()\n    ws = egauss_kernel(dists, mean=0, std=2.5)\n    X[s] = (ws @ X) / ws.sum(dim=1, keepdim=True)\n\ndef meanshift(data):\n   X = data.clone()\n   for _ in range(5): update(X)\n   return X\n\n\nplot_data(centroids+2, meanshift(data), n_samples)\n\n                                                \n\n\n\n%timeit -n 10 meanshift(data)\n\n700 ms ± 43.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nFrom 1.5 seconds to 0.5 seconds! A 3x speed increase — very nice!\nLet’s move onto the GPU and now see what improvements we get.\n\ndef meanshift(data):\n   X = data.clone().to('cuda')\n   for _ in range(5): update(X)\n   return X.detach().cpu()\n\n%timeit -n 10 meanshift(data)\n\n263 ms ± 27.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n0.5s to 0.25s — a 2x speed increase!"
  },
  {
    "objectID": "forblog/posts/17_meanshift_clustering.html#conclusion",
    "href": "forblog/posts/17_meanshift_clustering.html#conclusion",
    "title": "Implementing and Optimizing Meanshift Clustering",
    "section": "Conclusion",
    "text": "Conclusion\nMeanshift clustering simply involves moving points, by taking into account surrounding points, iteratively until they converge.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/15_un_successfully_implementing_diffedit.html",
    "href": "forblog/posts/15_un_successfully_implementing_diffedit.html",
    "title": "(Un)successfully Implementing DiffEdit",
    "section": "",
    "text": "This notebook follows the fastai style guide.\nWell, my implementation was a partial success: I managed to generate a mask, but failed to apply it. If you don’t understand, hold on as I’ll explain DiffEdit.\nIn this notebook, I try to implement the DiffEdit paper: a diffusion algorithm that allows us to replace the subject of an image with another subject, simply through a text prompt.\nIn a nutshell, this is done by generating a mask from the text prompt. This mask cuts out the subject from the image, which allows a new subject to be added to the image.\nWhile I was successful in generating a mask, I wasn’t successful in applying it to an image. So at the end of this notebook, I’ll use the Hugging Face Stable Diffusion Inpaint Pipeline to see the mask in action.\nIf you would like a refresher on how Stable Diffusion can be implemented from its various components, you can read my post on this here."
  },
  {
    "objectID": "forblog/posts/15_un_successfully_implementing_diffedit.html#basic-workings",
    "href": "forblog/posts/15_un_successfully_implementing_diffedit.html#basic-workings",
    "title": "(Un)successfully Implementing DiffEdit",
    "section": "Basic Workings",
    "text": "Basic Workings\nLet’s say we have an image of a horse in front of a forest. We want to replace the horse with a zebra. At a high level, DiffEdit achieves this in the following manner.\n\nUsing our image, we generate a further image with the prompt ‘horse’.\nWe similarly generate another further image with the prompt ‘zebra’.\nThe difference between both generated images is then taken.\nThe difference is normalized1 and binarized2 to obtain the mask.\nWe again generate an image with the prompt ‘zebra’.\n\nHowever this time, after each denoising step, apply the mask to the latent to obtain a cutout of the zebra.\nThen add the noised background pixels of the original image to the cutout.\n\n\n1 In this case, normalizing means scaling the values to be between 0 and 1.2 Binarizing means making values to be any of 2 possible values. In this case, either 0 or 1."
  },
  {
    "objectID": "forblog/posts/15_un_successfully_implementing_diffedit.html#setup",
    "href": "forblog/posts/15_un_successfully_implementing_diffedit.html#setup",
    "title": "(Un)successfully Implementing DiffEdit",
    "section": "Setup",
    "text": "Setup\n\n! pip install -Uqq fastcore transformers diffusers\n\n\n1import logging; logging.disable(logging.WARNING)\nfrom fastcore.all import *\nfrom fastai.imports import *\nfrom fastai.vision.all import *\n\n\n1\n\nHugging Face can be verbose."
  },
  {
    "objectID": "forblog/posts/15_un_successfully_implementing_diffedit.html#get-components",
    "href": "forblog/posts/15_un_successfully_implementing_diffedit.html#get-components",
    "title": "(Un)successfully Implementing DiffEdit",
    "section": "Get Components",
    "text": "Get Components\n\nfrom transformers import CLIPTokenizer, CLIPTextModel\n\ntokz = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14', torch_dtype=torch.float16)\ntxt_enc = CLIPTextModel.from_pretrained('openai/clip-vit-large-patch14', torch_dtype=torch.float16).to('cuda')\n\n\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\n\nvae = AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema', torch_dtype=torch.float16).to('cuda')\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n\n\nfrom diffusers import LMSDiscreteScheduler\n\nsched = LMSDiscreteScheduler(\n    beta_start = 0.00085,\n    beta_end = 0.012,\n    beta_schedule = 'scaled_linear',\n    num_train_timesteps = 1000\n)"
  },
  {
    "objectID": "forblog/posts/15_un_successfully_implementing_diffedit.html#simple-loop",
    "href": "forblog/posts/15_un_successfully_implementing_diffedit.html#simple-loop",
    "title": "(Un)successfully Implementing DiffEdit",
    "section": "Simple Loop",
    "text": "Simple Loop\nIn this simple loop, I’m making sure I can correctly generate an image based on another image as the starting point.\n\nHyperparameters\n\nprompt = ['earth']\nneg_prompt = ['']\nw, h = 512, 512\nn_inf_steps = 50\ng_scale = 8\nbs = 1\nseed = 77\n\n\n\nEncode Prompt\n\ntxt_inp = tokz(\n    prompt,\n    padding = 'max_length',\n    max_length = tokz.model_max_length,\n    truncation = True,\n    return_tensors = 'pt',\n)\n\n\ntxt_emb = txt_enc(txt_inp['input_ids'].to('cuda'))[0].half()\n\n\nneg_inp = tokz(\n    [''] * bs,\n    padding = 'max_length',\n    max_length = txt_inp['input_ids'].shape[-1],\n    return_tensors = 'pt'\n)\n\n\nneg_emb = txt_enc(neg_inp['input_ids'].to('cuda'))[0].half()\n\n\nembs = torch.cat([neg_emb, txt_emb])\n\n\n\nCompress Image\n\n!curl --output planet.png 'https://images.unsplash.com/photo-1630839437035-dac17da580d0?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=2515&q=80'\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  188k  100  188k    0     0  4829k      0 --:--:-- --:--:-- --:--:-- 4829k\n\n\n\nimg = Image.open('/content/planet.png').resize((512, 512)); img\n\n\n\n\n\n\n\n\n\nimport torchvision.transforms as T\nwith torch.no_grad():\n  img = T.ToTensor()(img).unsqueeze(0).half().to('cuda') * 2 - 1\n  lat = vae.encode(img)\n  lat = 0.18215 * lat.latent_dist.sample(); lat.shape\n\nBelow we can see the all 4 channels of the compressed image.\n\nfig, axs = plt.subplots(1, 4, figsize=(16, 4))\nfor c in range(4):\n  axs[c].imshow(lat[0][c].cpu(), cmap='Greys')\n\n\n\n\n\n\n\n\n\n\nNoise Image\n\nsched = LMSDiscreteScheduler(\n    beta_start=0.00085,\n    beta_end=0.012,\n    beta_schedule='scaled_linear',\n    num_train_timesteps=1000\n); sched\n\nLMSDiscreteScheduler {\n  \"_class_name\": \"LMSDiscreteScheduler\",\n  \"_diffusers_version\": \"0.16.1\",\n  \"beta_end\": 0.012,\n  \"beta_schedule\": \"scaled_linear\",\n  \"beta_start\": 0.00085,\n  \"num_train_timesteps\": 1000,\n  \"prediction_type\": \"epsilon\",\n  \"trained_betas\": null\n}\n\n\n\nsched.set_timesteps(n_inf_steps)\n\n\ntorch.manual_seed(seed)\nnoise = torch.randn_like(lat)\nsched.timesteps = sched.timesteps.to(torch.float32)\nstart_step = 10\nts = tensor([sched.timesteps[start_step]])\nlat = sched.add_noise(lat, noise, timesteps=ts)\n\n\n\nDenoise\n\nfrom tqdm.auto import tqdm\n\nfor i, ts in enumerate(tqdm(sched.timesteps)):\n  if i &gt;= start_step:\n    inp = torch.cat([lat] * 2)\n    inp = sched.scale_model_input(inp, ts)\n\n    with torch.no_grad(): preds = unet(inp, ts, encoder_hidden_states=embs)['sample']\n\n    pred_neg, pred_txt = preds.chunk(2)\n    pred = pred_neg + g_scale * (pred_txt - pred_neg)\n\n    lat = sched.step(pred, ts, lat).prev_sample\n\n\n\n\n\n\nUncompress\n\nlat.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\n\nlat *= (1/0.18215)\nwith torch.no_grad(): img = vae.decode(lat).sample\nimg = (img / 2 + 0.5).clamp(0, 1)\nimg = img[0].detach().cpu().permute(1, 2, 0).numpy()\nimg = (img * 255).round().astype('uint8')\nImage.fromarray(img)\n\n\n\n\n\n\n\n\n\n\nEncapsulate\nI’ll encapsulate the code above so we can focus on DiffEdit.\n\ndef get_embs(prompt, neg_prompt):\n  txt_inp = tok_seq(prompt)\n  txt_emb = calc_emb(txt_inp['input_ids'])\n\n  neg_inp = tok_seq(neg_prompt)\n  neg_emb = calc_emb(neg_inp['input_ids'])\n\n  return torch.cat([neg_emb, txt_emb])\n\ndef tok_seq(prompt):\n  return tokz(\n      prompt,\n      padding = 'max_length',\n      max_length = tokz.model_max_length,\n      truncation = True,\n      return_tensors = 'pt',\n  )\n\ndef calc_emb(inp_ids):\n  return txt_enc(inp_ids.to('cuda'))[0].half()\n\n\ndef get_lat(img, start_step=30):\n  return noise_lat(compress_img(img), start_step)\n\ndef compress_img(img):\n  with torch.no_grad():\n    img = T.ToTensor()(img).unsqueeze(0).half().to('cuda') * 2 - 1\n    lat = vae.encode(img)\n    return 0.18215 * lat.latent_dist.sample()\n\ndef noise_lat(lat, start_step):\n  torch.manual_seed(seed)\n  noise = torch.randn_like(lat)\n\n  sched.set_timesteps(n_inf_steps)\n  sched.timesteps = sched.timesteps.to(torch.float32)\n  ts = tensor([sched.timesteps[start_step]])\n\n  return sched.add_noise(lat, noise, timesteps=ts)\n\n\ndef denoise(lat, ts):\n  inp = torch.cat([lat] * 2)\n  inp = sched.scale_model_input(inp, ts)\n\n  with torch.no_grad(): preds = unet(inp, ts, encoder_hidden_states=embs)['sample']\n\n  pred_neg, pred_txt = preds.chunk(2)\n  pred = pred_neg + g_scale * (pred_txt - pred_neg)\n\n  return sched.step(pred, ts, lat).prev_sample\n\n\ndef decompress(lat):\n  with torch.no_grad(): img = vae.decode(lat*(1/0.18215)).sample\n  img = (img / 2 + 0.5).clamp(0, 1)\n  img = img[0].detach().cpu().permute(1, 2, 0).numpy()\n  return (img * 255).round().astype('uint8')\n\n\nprompt = ['basketball']\nneg_prompt = ['']\nw, h = 512, 512\nn_inf_steps = 70\nstart_step = 30\ng_scale = 7.5\nbs = 1\nseed = 77\n\n! curl --output img.png 'https://images.unsplash.com/photo-1630839437035-dac17da580d0?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=2515&q=80'\nimg = Image.open('/content/img.png').resize((512, 512))\n\nembs = get_embs(prompt, neg_prompt)\nlat = get_lat(img)\nfor i, ts in enumerate(tqdm(sched.timesteps)):\n  if i &gt;= start_step: lat = denoise(lat, ts)\nimg = decompress(lat)\nImage.fromarray(img)\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  188k  100  188k    0     0  5232k      0 --:--:-- --:--:-- --:--:-- 5381k"
  },
  {
    "objectID": "forblog/posts/15_un_successfully_implementing_diffedit.html#diffedit",
    "href": "forblog/posts/15_un_successfully_implementing_diffedit.html#diffedit",
    "title": "(Un)successfully Implementing DiffEdit",
    "section": "DiffEdit",
    "text": "DiffEdit\nLet’s review the steps of DiffEdit once more.\n\nUsing our image, we generate a further image with the prompt ‘horse’.\nWe similarly generate another further image with the prompt ‘zebra’.\nThe difference between both generated images is then taken.\nThe difference is normalized3 and binarized4 to obtain the mask.\nWe then again generate an image with the prompt ‘zebra’.\n\nHowever this time, after each denoising step, apply the mask to the latent to obtain a cutout of the zebra.\nThen add the noised background pixels of the original image to the cutout.\n\n\n3 In this case, normalizing means scaling the values to be between 0 and 1.4 Binarizing means making values to be any of 2 possible values. In this case, either 0 or 1.\nObtain two latents\nFirst, we need to obtain an image of a horse and an image of a zebra.\nWe’ll use this as our original image.\n\n! curl --output img.png 'https://images.unsplash.com/photo-1553284965-fa61e9ad4795?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1742&q=80'\nImage.open('/content/img.png').resize((512, 512))\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  515k  100  515k    0     0  10.7M      0 --:--:-- --:--:-- --:--:-- 10.7M\n\n\n\n\n\n\n\n\n\nThis is the generated image of the horse.\n\nprompt = ['horse']\nimg = Image.open('/content/img.png').resize((512, 512))\nembs = get_embs(prompt, neg_prompt)\nlat1 = get_lat(img)\nfor i, ts in enumerate(tqdm(sched.timesteps)):\n  if i &gt;= start_step: lat1 = denoise(lat1, ts)\n\n\n\n\n\nImage.fromarray(decompress(lat1))\n\n\n\n\n\n\n\n\nAnd this is the generated image of the zebra.\n\nprompt = ['zebra']\nimg = Image.open('/content/img.png').resize((512, 512))\nembs = get_embs(prompt, neg_prompt)\nlat2 = get_lat(img)\nfor i, ts in enumerate(tqdm(sched.timesteps)):\n  if i &gt;= start_step: lat2 = denoise(lat2, ts)\n\n\n\n\n\nImage.fromarray(decompress(lat2))\n\n\n\n\n\n\n\n\n\nlat1[:].shape\n\ntorch.Size([1, 4, 64, 64])\n\n\n\n\nCreate Mask\nWe’ll first convert the generated images to grayscale and then take their difference.\n\nimport torchvision.transforms.functional as F\n\nimg1 = F.to_tensor(F.to_grayscale(Image.fromarray(decompress(lat1[:]))))\nimg2 = F.to_tensor(F.to_grayscale(Image.fromarray(decompress(lat2[:]))))\ndiff = torch.abs(img1 - img2)\n\nThen we’ll normalize the difference to have values between 0 and 1.\n\nnorm = diff / torch.max(diff)\nImage.fromarray((norm*255).squeeze().numpy().round().astype(np.uint8))\n\n\n\n\n\n\n\n\nAnd then finally binarize the values so they are either 0 or 1.\n\nthresh = 0.5\nbin = (norm &gt; thresh).float()\nImage.fromarray((bin.squeeze().numpy()*255).astype(np.uint8))\n\n\n\n\n\n\n\n\n\nImage.fromarray((bin.squeeze().numpy()*255).astype(np.uint8)).save('mask.png')\n\nNow we need to apply transformations to the binarized mask so it encapsulates the shape of the horbra/zeborse (horse + zebra 🫤).\n\nimport cv2 as cv\nfrom google.colab.patches import cv2_imshow\n\nmask = cv.imread('mask.png', cv.IMREAD_GRAYSCALE)\nkernel = cv.getStructuringElement(cv.MORPH_ELLIPSE, (10, 10))\n\nThe kernel is essentially a shape. Multiple shapes are be applied to the image in order to perform transformations.\nI’ve chosen to use an ellipse of size 10 by 10 units.\nApplying an erosion transformation makes our binarized mask look like this. Such transformations remove can remove small, noisy objects.\n\ncv2_imshow(cv.erode(mask, kernel))\n\n\n\n\n\n\n\n\nApplying a dilation transformation makes our binarized mask look like this. Such transformations can fill in gaps and smooth edges.\n\ncv2_imshow(cv.dilate(mask, kernel))\n\n\n\n\n\n\n\n\nTo produce the final mask, I’ll apply the closing transform5 7 times consecutively…\n5 The closing transform is a dilation transform followed immediately by an erosion transform. This allows holes or small black points to be closed.\nmask_closed = mask\nfor _ in range(7):\n  mask_closed = cv.morphologyEx(mask_closed, cv.MORPH_CLOSE, kernel)\n  cv2_imshow(mask_closed)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n…and then apply the dilation transform 3 times consecutively.\n\nmask_dilated = mask_closed\nfor _ in range(3):\n  mask_dilated = cv.dilate(mask_dilated, kernel)\n  cv2_imshow(mask_dilated)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA more concise way of doing the above.\n\nmask_closed = cv.morphologyEx(mask, cv.MORPH_CLOSE, kernel, iterations=7)\nmask_dilated = cv.dilate(mask_closed, kernel, iterations=3)\n\ncv2_imshow(mask_dilated)\n\n\n\n\n\n\n\n\nThen I’ll stack the mask together so I have a 3 channel image.\n\nmask = np.stack((mask_dilated, mask_dilated, mask_dilated), axis=-1)/255; mask.shape\n\n(512, 512, 3)\n\n\nTo read more about such transformations applied above, you can read them at the OpenCV docs here.\n\n\nApply Mask\nNow for the part I couldn’t figure out how to do.\nBy applying the mask to the original iamge. This is how the cutout of the horse looks like.\n\nfore = torch.mul(F.to_tensor(img).permute(1, 2, 0), torch.from_numpy(mask))\nImage.fromarray((fore*255).numpy().round().astype('uint8'))\n\n\n\n\n\n\n\n\nYou can see that it does not exactly cut out the outline: this is good because different subjects will have different levels of protrusion.\nAnd this is how the background pixels look like.\n\ninv_mask = 1 - mask\nback = torch.mul(F.to_tensor(img).permute(1, 2, 0), torch.from_numpy(inv_mask))\nImage.fromarray((back*255).numpy().round().astype('uint8'))\n\n\n\n\n\n\n\n\nAdding both the foreground and the background together…\n\nImage.fromarray(((fore+back)*255).numpy().round().astype(np.uint8))\n\n\n\n\n\n\n\n\n\n\nDetour\nNote the subtle, yet very important difference in the two cells below, along with their output.\n\nx = tensor([1, 2, 3])\ndef foo(y):\n  y += 1\n  return y\nfoo(x)\nx\n\ntensor([2, 3, 4])\n\n\n\nx = tensor([1, 2, 3])\ndef foo(y):\n  z = y + 1\n  return z\nfoo(x)\nx\n\ntensor([1, 2, 3])\n\n\nThis was the reason for the bug that had me pulling my hair out for hours — when you pass a list or any list-like object (or even just objects I think), a copy is not passed, but rather the same object.\n\nHowever, I can’t quite correctly apply the mask to the latent when denoising.\n\nprompt = ['zebra']\nimg = Image.open('/content/img.png').resize((512, 512))\nembs = get_embs(prompt, neg_prompt)\nlat = get_lat(img)\ninv_mask = 1 - mask\n\n\nfor i, ts in enumerate(tqdm(sched.timesteps)):\n  if i &gt;= start_step: \n    back = torch.mul(torch.from_numpy(decompress(get_lat(img, start_step=i)))/255, torch.from_numpy(inv_mask))\n    fore = torch.mul(torch.from_numpy(decompress(lat))/255, torch.from_numpy(mask))\n    bafo = (back + fore)*255\n    lat = compress_img(Image.fromarray(bafo.numpy().round().astype(np.uint8)))\n\n\n\n\n\nImage.fromarray(decompress(lat))\n\n\n\n\n\n\n\n\nAfter asking on the fastai forum, and hours of fiddling about, the reason why this is happening is most likely due to the fact that I keep uncompressing and recompressing the latent. The compression that the VAE performs is lossy, so detail is lost during each compression and decompression.\nMy mask is not calculated in the same latent space as my latent. In other words, my mask was calculated as a 512x512 pixel and 3 channel image, whereas my latent is a 64x64 pixel and 4 channel image. I’m uncompressing the latent so that I can apply the mask to cutout the zebra and add the background pixels, and then recompressing.\nTo fix this, I would need to generate the mask as a 64x64 pixel and 3 channel image.\nTo at least see the mask in action, let’s use the Hugging Face Stable Diffusion Pipeline."
  },
  {
    "objectID": "forblog/posts/15_un_successfully_implementing_diffedit.html#pipeline",
    "href": "forblog/posts/15_un_successfully_implementing_diffedit.html#pipeline",
    "title": "(Un)successfully Implementing DiffEdit",
    "section": "Pipeline",
    "text": "Pipeline\nThe Hugging Face Stable Diffusion pipeline works by simply providing the starting image and a mask. The pipeline will handle the rest.\n\nfrom diffusers import StableDiffusionInpaintPipeline\npipe = StableDiffusionInpaintPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-inpainting\",\n    revision=\"fp16\",\n    torch_dtype=torch.float16,\n).to(\"cuda\")\n\n/usr/local/lib/python3.10/dist-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n  warnings.warn(\n\n\n\nimg\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(77)\n# 35 or 25 steps are good\nout = pipe(\n    prompt=[\"zebra\"], \n    image=img, \n    mask_image=Image.fromarray((mask*255).round().astype(np.uint8)), \n    num_inference_steps = 25\n).images\nout[0]"
  },
  {
    "objectID": "forblog/posts/15_un_successfully_implementing_diffedit.html#takeaways",
    "href": "forblog/posts/15_un_successfully_implementing_diffedit.html#takeaways",
    "title": "(Un)successfully Implementing DiffEdit",
    "section": "Takeaways",
    "text": "Takeaways\nLooking back, the actual problem for me was that I let the paper feel intimidating; all those symbols, variables, jargon, and notation. I ended up glazing over the paper and missing the smaller details.\nTo help prevent this the next time, I should\n\nlist out the variables and what they represent\nwrite out the steps in simpler terms\nand take a deep breath before reading, so I take things slowly.\n\nAnd that’s that.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/26_getting_general_purpose_robots_by_decomposing_problems_and_working_with_data.html",
    "href": "forblog/posts/26_getting_general_purpose_robots_by_decomposing_problems_and_working_with_data.html",
    "title": "Getting General Purpose Robots by Decomposing Problems and Working with Data",
    "section": "",
    "text": "I’ve been going through the RoboOS, RoboBrain, and Robobrain 2.0 papers from the Beijing Academy of Artificial Intelligence (BAAI).\nMost robotic solutions I’ve encountered so far leave me feeling like I’m watching yet another Jenga tower of hacks: blocks ducktaped together for the purpose of solving the previous block’s issues. All without fixing the actual fundamental issue that is the VLM.\nThese series of papers do something that feels like cold drink in a desert of hobbling together complicated pipelines and towers: the papers identify issues with VLMs, and then solve those issues by working with the data.\nRoboOS is an overarching system for managing embodied robotic systems. RoboBrain is the VLM/MLLM that executes the plans."
  },
  {
    "objectID": "forblog/posts/26_getting_general_purpose_robots_by_decomposing_problems_and_working_with_data.html#roboos-a-3-piece-kit-instead-of-a-300-piece-puzzle",
    "href": "forblog/posts/26_getting_general_purpose_robots_by_decomposing_problems_and_working_with_data.html#roboos-a-3-piece-kit-instead-of-a-300-piece-puzzle",
    "title": "Getting General Purpose Robots by Decomposing Problems and Working with Data",
    "section": "RoboOS: A 3-piece kit instead of a 300-piece puzzle",
    "text": "RoboOS: A 3-piece kit instead of a 300-piece puzzle\nRoboOS decomposes the generalizability problem of robotics into three clean layers:\n\nThe Embodied Brain: A cloud-deployed MLLM that thinks globally–planning, replanning, and correcting errors in real time.\nThe Cerebellum Skill Library: A plug-and-play toolkit that enables various actions, allowing the robot to perform different functions.\nReal-Time Shared Memory: Helps maintain spatial memory, temporal memory, and embodiment memory.\n\n\n\n\n\n\n\n\n\nComponent\nAnalogy\nJob\n\n\n\n\nEmbodied Brain (cloud LLM)\nControl Center\nhigh-level planning, error recovery\n\n\nCerebellum Skill Library (edge containers)\nToolbox\nlow-latency skills like grasping, SLAM\n\n\nReal-Time Shared Memory\nWhiteboard\nspatial, temporal, and robot-state cache\n\n\n\nThere are three manageable chunks that allows for the creation of systems that are general enough for specialized tasks, but not overly general that its bad at everything. It might not allow for novel actions, but it can allow for known actions. And it allows for each of these pieces to be iterated on through their own merits.\nIn addition, RoboOS allows for multiple different robots to coordinate with each other. What occured in my mind is that that doesn’t have to be individual separate robots; it could be different pieces of the same robot (e.g., the legs and the hands of a robot)."
  },
  {
    "objectID": "forblog/posts/26_getting_general_purpose_robots_by_decomposing_problems_and_working_with_data.html#robobrain-solve-the-foundations-and-work-with-the-data",
    "href": "forblog/posts/26_getting_general_purpose_robots_by_decomposing_problems_and_working_with_data.html#robobrain-solve-the-foundations-and-work-with-the-data",
    "title": "Getting General Purpose Robots by Decomposing Problems and Working with Data",
    "section": "RoboBrain: solve the foundations, and work with the data",
    "text": "RoboBrain: solve the foundations, and work with the data\nMany general purpose robotic solutions try to remedy issues with VLMs with all sorts of pipelines, gadgets, and gizmos. Rather than actually solving the foundational issue that is the VLM itself. Many solutions use VLMs for purposes that they are exactly bad for. VLMs are langauge biased, spatially blind, and temporally naive.\nRoboBrain 1.0 tackled:\n\ntask planning by using QA pairs that decompose long horizon tasks (“make coffee”) into atomic steps (“reach mug → grasp handle → lift → …”)\naffordance identification by using 6k images with bounding boxes of “where you’d actually touch the mug” and LoRA-tuning the LLM\ntrajectory planning by feeding the LLM start / goal 2-D points and asking it to regress waypoints\n\nRoboBrain 2.0 doubles down on this philosophy with improvements in three aspects:\n\nSpatial understanding with multiview static images, visual grounding, object pointing, and spatial referring examples\nTemporal modeling by linking video frames with positional encoding that tracks both the space and time of each frame, multi-robot planning examples, close-loop interaction examples, and ego-view planning\nReasoning chains\n\nWhat was done for both models was construct a dataset that forced the model to learn these skills.\nIn addition, the RoboBrain papers did a bunch of data curating so as to ensure desired outputs were obtained: truncating examples, increasing throughput of other examples, limiting certain annotations, ensuring diversity of certain examples, mixing data from various stages, and more. In addition, RoboBrain 2.0 is able to output in a wide variety of modalities, making it suitable for a wide range of embodied robotic tasks."
  },
  {
    "objectID": "forblog/posts/26_getting_general_purpose_robots_by_decomposing_problems_and_working_with_data.html#conclusion",
    "href": "forblog/posts/26_getting_general_purpose_robots_by_decomposing_problems_and_working_with_data.html#conclusion",
    "title": "Getting General Purpose Robots by Decomposing Problems and Working with Data",
    "section": "Conclusion",
    "text": "Conclusion\nThe most prominent parts of these papers that stuck out to me is the focus on decomposing problems into much smaller manageable ones, and fixing problems by working with the data.\n\nRoboOS: a minimal, composable OS\nRoboBrain: a VLM trained for robotics. Not a VLM adapted to robotics.\n\nMaybe this is the viable road to allow enough generalizability in robots.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/26_getting_general_purpose_robots_by_decomposing_problems_and_working_with_data.html#links",
    "href": "forblog/posts/26_getting_general_purpose_robots_by_decomposing_problems_and_working_with_data.html#links",
    "title": "Getting General Purpose Robots by Decomposing Problems and Working with Data",
    "section": "Links",
    "text": "Links\n\nBAAI (English)\nBAAI (中文)\nRoboOS\nRobobrain\nRoboBrain 2.0\n\nIn addition, RoboBrain 2 contains information about their training and inference infrastructure."
  },
  {
    "objectID": "forblog/posts/1_how_to_approach_creating_ai_models.html",
    "href": "forblog/posts/1_how_to_approach_creating_ai_models.html",
    "title": "How to Approach Creating AI Models",
    "section": "",
    "text": "This article was rewritten on Monday, 31 October 2022."
  },
  {
    "objectID": "forblog/posts/1_how_to_approach_creating_ai_models.html#introduction",
    "href": "forblog/posts/1_how_to_approach_creating_ai_models.html#introduction",
    "title": "How to Approach Creating AI Models",
    "section": "Introduction",
    "text": "Introduction\nHow you approach making models is crucial. The way AI methods are used in today’s landscape is very different. AI methods are created to solve small, atomic problems. And we’ve got most of the methods to handle these small tasks hammered down. Therefore, applied AI is not about creating models; it’s only a small part of it. It’s 80% problem solving and 20% implementing (I would not be surprised if it actually followed the 80-20 rule1).\n1 The 80/20 Rule, also known as the Pareto PrincipleThink of AI methods as a tool; think of it as a pencil. You can use pencils to draw, take notes, poke holes, and much more. There are also dozens of pencils out there. But what point is there in using any of those pencils if you don’t even know how to properly use a pencil in the first place? The art of creating pencils has already been perfected too.\nOne highly successful approach is the Drivetrain Approach, created by Jeremy Howard — who’s widely known for his fastai course and library —, Margit Zwemer, and Mike Loukides.\nThe goal of the Drivetrain Approach is to not just use data to generate more data — data that is in the form of predictions. But rather to use data to also generate actionable outcomes.\nThe official blogpost goes into much more depth here.\nIn this post, I’ll be providing a short overview of my understanding of this approach by applying it to the Elements of AI course’s final project (this online course was created by the University of Helsinki and Reaktor)."
  },
  {
    "objectID": "forblog/posts/1_how_to_approach_creating_ai_models.html#overview-of-the-drivetrain-approach",
    "href": "forblog/posts/1_how_to_approach_creating_ai_models.html#overview-of-the-drivetrain-approach",
    "title": "How to Approach Creating AI Models",
    "section": "Overview of the Drivetrain Approach",
    "text": "Overview of the Drivetrain Approach\nThere are four main steps to this approach:\n\nDefine the objective\nConsider your possible actions\nConsider your data\nCreate the models\n\n\n\n\nImage Source\n\n\n\nDefine the objective\nWrite out what you are really trying to achieve. What is your goal? Writing it out puts it in a tangible manner.\n\n\nConsider your actions\nThink about what actions you can take to achieve your objective.\nAlso think about what would happen if you did those actions.\nWhat would happen if I did x? Would y really be a good idea? What if z worked out too well? Will x lead to y? What would happen if x turned out poorly?\n\n\nConsider your data\nThink about the data you already have and how it could be used.\nThink about any further data that is needed and how it could be collected.\n\n\nCreate the models\nCreate models. But create models that produce actions. Actions that produce the best results for your objective."
  },
  {
    "objectID": "forblog/posts/1_how_to_approach_creating_ai_models.html#endangered-language-chatbot",
    "href": "forblog/posts/1_how_to_approach_creating_ai_models.html#endangered-language-chatbot",
    "title": "How to Approach Creating AI Models",
    "section": "Endangered Language Chatbot",
    "text": "Endangered Language Chatbot\nThe final project of the Elements of AI course asked me to come up with my own AI method that would solve a problem, and how it would do so.\nThe problem I tackled was the endangerment of languages. The solution I came up with was to create a chatbot that could converse in these endangered languages. I created an overview of how this could be done.\nThe overview can be read here.\nLet’s tackle this problem through the Drivetrain Approach.\n\nDefine the objective\nThe objective is to preserve languages that are in danger of going extinct. Through preserving languages, histories and cultures can be preserved.\n\n\nConsider your actions\nOne way this could be done is to create a chatbot that could converse in endangered languages. However, this would be a monumental task considering the amount of data needed to achieve this.\nAnother action that could be taken is to create an information retrieval (IR) system of sorts. A corpus of written text of the language could be provided, from which insights about the language’s history, culture, and way of conversing could be gained. In turn the language is preserved.\nThe latter action may be easier to achieve.\n\n\nConsider your data\nThe obvious source of data would be a corpora of text.\nHowever, a major problem arises for those languages which are only spoken. Audio recordings of conversations would have to be made which would take a lot of time and effort. This would be especially difficult for those languages where very few speakers remain.\nEven if a language does have written text, gathering enough text for the language can also be a problem: the language may not have much written text. This may especially be the case for endangered languages. Again, one solution is to manually create texts — using an NLP method to create these texts is not viable.\nIn short, for some languages, there may be no choice other than to manually create the data that would be fed into the system — this manual creation also has the chance to skew the performance of the model.\n\n\n\nKuş dili, a whistled language spoken in Turkey. How would such a language be preserved? Image Source\n\n\n\n\nCreate the model\nEither a chatbot needs to be created that speaks as accurately as a native speaker, or an IR system needs to be created that gives meaningful, correct insights into a language and its associated culture.\nThis step may either be easy or hard, depending on the language. Most NLP or IR systems have been built on a few, select languages. Perhaps this step may be easy for those languages that are similar to languages on which NLP or IR systems have already been built on. It will most likely be harder for those languages which are not."
  },
  {
    "objectID": "forblog/posts/1_how_to_approach_creating_ai_models.html#conclusion",
    "href": "forblog/posts/1_how_to_approach_creating_ai_models.html#conclusion",
    "title": "How to Approach Creating AI Models",
    "section": "Conclusion",
    "text": "Conclusion\nThis concludes my understanding of the Drivetrain Approach, through an example.\nApproaches are crucial: you can have state-of-the-art tools, but they are useless if not correctly applied. The approach you take can either make it or break it. Putting it into a concrete, organized, tangible manner goes a long way.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "",
    "text": "This notebook follows the fastai style guide.\nIn this notebook, we’ll implement stable diffusion from its various components through the Hugging Face Diffusers library.\nAt the end, we’ll have our own custom stable diffusion class, from which we can generate images as simply as diffuser.diffuse().\nIf you would like a refresher, I’ve summarized at a high level how a diffuser is trained in this post. Though this notebook focuses on inference and not the training aspect, the linked summary may be helpful.\nLet’s begin."
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#overview",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#overview",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Overview",
    "text": "Overview\nBefore we get hands on with the code, let’s refresh how inference works for a diffuser.\n\nWe input a prompt to the diffuser.\nThis prompt is given a mathematical representation (an embedding) through the text encoder.\nA latent comprised of noise is produced.\nThe U-Net predicts the noise in the latent in conjunction with the prompt.\nThe predicted noise is subtracted from the latent in conjunction with the scheduler.\nAfter many iterations, the denoised latent is decompressed to produce our final generated image.\n\nThe main components in use are:\n\na text encoder,\na U-Net,\nand a VAE decoder."
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#setup",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#setup",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Setup",
    "text": "Setup\n\n! pip install -Uqq fastcore transformers diffusers\n\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 40.9 MB/s eta 0:00:00\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 934.9/934.9 kB 57.7 MB/s eta 0:00:00\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 23.9 MB/s eta 0:00:00\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 29.1 MB/s eta 0:00:00\n\n\n\n\n\n\n1import logging; logging.disable(logging.WARNING)\nfrom fastcore.all import *\nfrom fastai.imports import *\nfrom fastai.vision.all import *\n\n\n1\n\nHugging Face can be very verbose."
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#get-components",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#get-components",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Get Components",
    "text": "Get Components\n\nCLIP Components\nTo process the prompt, we need to download a tokenizer and a text encoder. The tokenizer will split the prompt into tokens while the text encoder will convert the tokens into a numerical representation (an embedding).\n\nfrom transformers import CLIPTokenizer, CLIPTextModel\n\ntokz = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14', torch_dtype=torch.float16)\ntxt_enc = CLIPTextModel.from_pretrained('openai/clip-vit-large-patch14', torch_dtype=torch.float16).to('cuda')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfloat16 is used for faster performance.\n\n\nU-Net and VAE\nThe U-Net will predict the noise in the image, while the VAE will decompress the generated image.\n\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\n\nvae = AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema', torch_dtype=torch.float16).to('cuda')\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScheduler\nThe scheduler will control how much noise is intially added to the image, and will also control how much of the noise predicted from the U-Net will be subtracted from the image.\n\nfrom diffusers import LMSDiscreteScheduler\n\nsched = LMSDiscreteScheduler(\n    beta_start = 0.00085,\n    beta_end = 0.012,\n    beta_schedule = 'scaled_linear',\n    num_train_timesteps = 1000\n); sched\n\nLMSDiscreteScheduler {\n  \"_class_name\": \"LMSDiscreteScheduler\",\n  \"_diffusers_version\": \"0.16.1\",\n  \"beta_end\": 0.012,\n  \"beta_schedule\": \"scaled_linear\",\n  \"beta_start\": 0.00085,\n  \"num_train_timesteps\": 1000,\n  \"prediction_type\": \"epsilon\",\n  \"trained_betas\": null\n}"
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#define-generation-parameters",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#define-generation-parameters",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Define Generation Parameters",
    "text": "Define Generation Parameters\nThe six main parameters needed for generation are:\n\nThe prompt\nThe width and height of the image\nA number describing how noisy the output image is to be (the number of inference steps)\nA number describing how much the diffuser should stick to the prompt (the guidance scale)\nThe batch size\nThe seed\n\n\nprompt = ['a photograph of an astronaut riding a horse']\nw, h = 512, 512\nn_inf_steps = 70\ng_scale = 7.5\nbs = 1\nseed = 77"
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#encode-prompt",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#encode-prompt",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Encode Prompt",
    "text": "Encode Prompt\nNow we need to parse the prompt. To do so, we’ll first tokenize it, and then encode the tokens to produce an embedding.\nFirst, let’s tokenize.\n\ntxt_inp = tokz(\n    prompt,\n    padding = 'max_length',\n    max_length = tokz.model_max_length,\n    truncation = True,\n    return_tensors = 'pt'\n); txt_inp\n\n{'input_ids': tensor([[49406,   320,  8853,   539,   550, 18376,  6765,   320,  4558, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0]])}\n\n\nThe token 49407 is a padding token and represents '&lt;|endoftext|&gt;'. These tokens have been given an attention mask of 0.\n\ntokz.decode(49407)\n\n'&lt;|endoftext|&gt;'\n\n\nNow using the text encoder, we’ll create an embedding out of these tokens.\n\ntxt_emb = txt_enc(txt_inp['input_ids'].to('cuda'))[0].half(); txt_emb\n\ntensor([[[-0.3884,  0.0229, -0.0523,  ..., -0.4902, -0.3066,  0.0674],\n         [ 0.0292, -1.3242,  0.3076,  ..., -0.5254,  0.9766,  0.6655],\n         [ 0.4609,  0.5610,  1.6689,  ..., -1.9502, -1.2266,  0.0093],\n         ...,\n         [-3.0410, -0.0674, -0.1777,  ...,  0.3950, -0.0174,  0.7671],\n         [-3.0566, -0.1058, -0.1936,  ...,  0.4258, -0.0184,  0.7588],\n         [-2.9844, -0.0850, -0.1726,  ...,  0.4373,  0.0092,  0.7490]]],\n       device='cuda:0', dtype=torch.float16, grad_fn=&lt;NativeLayerNormBackward0&gt;)\n\n\n\ntxt_emb.shape\n\ntorch.Size([1, 77, 768])"
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#embeddings-for-cfg",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#embeddings-for-cfg",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Embeddings for CFG",
    "text": "Embeddings for CFG\nWe also need to create an embedding for an empty prompt, also known as the uncondtional prompt. This embedding is what is used to control the guidance.\n\ntxt_inp['input_ids'].shape\n\ntorch.Size([1, 77])\n\n\n\n1max_len = txt_inp['input_ids'].shape[-1]\nuncond_inp = tokz(\n2    [''] * bs,\n    padding = 'max_length',\n    max_length = max_len,\n    return_tensors = 'pt',\n); uncond_inp\n\n\n1\n\nWe use the maximum length of the prompt so the unconditional prompt embedding matches the size of the text prmpt embedding.\n\n2\n\nWe also multiply the list containing the empty prompt with the batch size so we have an empty prompt for each text prompt.\n\n\n\n\n{'input_ids': tensor([[49406, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0]])}\n\n\n\nuncond_inp['input_ids'].shape\n\ntorch.Size([1, 77])\n\n\n\nuncond_emb = txt_enc(uncond_inp['input_ids'].to('cuda'))[0].half()\nuncond_emb.shape\n\ntorch.Size([1, 77, 768])\n\n\nWe can then concatenate both the unconditonal embedding and the text embedding together. This allows images to be generated from each prompt without having to go through the U-Net twice.\n\nembs = torch.cat([uncond_emb, txt_emb])"
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#create-noisy-image",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#create-noisy-image",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Create Noisy Image",
    "text": "Create Noisy Image\nIt’s now time to create our noisy image, which will be the starting point for generation.\nWe’ll create a single latent that is 64 by 64 pixels, and that also has 4 channels. After the latent is denoised, we’ll decompress it to a 512 by 512 pixel image with 3 channels.\n\nbs, unet.config.in_channels, h//8, w//8\n\n(1, 4, 64, 64)\n\n\n\nprint(torch.randn((2, 3, 4)))\nprint(torch.randn((2, 3, 4)).shape)\n\ntensor([[[ 0.0800, -1.3597, -0.2033, -0.5647],\n         [-1.6066,  0.8178,  1.0832,  0.0638],\n         [ 0.3133,  1.8516,  0.4320, -0.9295]],\n\n        [[-1.0798,  3.2928,  0.7443,  1.2190],\n         [-0.4984,  0.3551, -0.6012, -0.5856],\n         [-0.3988, -1.2950, -1.6061, -0.0207]]])\ntorch.Size([2, 3, 4])\n\n\n\ntorch.manual_seed(seed)\nlats = torch.randn((bs, unet.config.in_channels, h//8, w//8)); lats.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\nThe latent is a rank 4 tensor. 1 refers to the batch size, which is the number of images being generated. 4 is the number of channels, and 64 is the number of pixel with regard to both height and width.\n\nlats = lats.to('cuda').half(); lats\n\ntensor([[[[-0.5044, -0.4163, -0.1365,  ..., -1.6104,  0.1381,  1.7676],\n          [ 0.7017,  1.5947, -1.4434,  ..., -1.5859, -0.4089, -2.8164],\n          [ 1.0664, -0.0923,  0.3462,  ..., -0.2390, -1.0947,  0.7554],\n          ...,\n          [-1.0283,  0.2433,  0.3337,  ...,  0.6641,  0.4219,  0.7065],\n          [ 0.4280, -1.5439,  0.1409,  ...,  0.8989, -1.0049,  0.0482],\n          [-1.8682,  0.4988,  0.4668,  ..., -0.5874, -0.4019, -0.2856]],\n\n         [[ 0.5688, -1.2715, -1.4980,  ...,  0.2230,  1.4785, -0.6821],\n          [ 1.8418, -0.5117,  1.1934,  ..., -0.7222, -0.7417,  1.0479],\n          [-0.6558,  0.1201,  1.4971,  ...,  0.1454,  0.4714,  0.2441],\n          ...,\n          [ 0.9492,  0.1953, -2.4141,  ..., -0.5176,  1.1191,  0.5879],\n          [ 0.2129,  1.8643, -1.8506,  ...,  0.8096, -1.5264,  0.3191],\n          [-0.3640, -0.9189,  0.8931,  ..., -0.4944,  0.3916, -0.1406]],\n\n         [[-0.5259,  1.5059, -0.3413,  ...,  1.2539,  0.3669, -0.1593],\n          [-0.2957, -0.1169, -2.0078,  ...,  1.9268,  0.3833, -0.0992],\n          [ 0.5020,  1.0068, -0.9907,  ..., -0.3008,  0.7324, -1.1963],\n          ...,\n          [-0.7437, -1.1250,  0.1349,  ..., -0.6714, -0.6753, -0.7920],\n          [ 0.5415, -0.5269, -1.0166,  ...,  1.1270, -1.7637, -1.5156],\n          [-0.2319,  0.9165,  1.6318,  ...,  0.6602, -1.2871,  1.7568]],\n\n         [[ 0.7100,  0.4133,  0.5513,  ...,  0.0326,  0.9175,  1.4922],\n          [ 0.8862,  1.3760,  0.8599,  ..., -2.1172, -1.6533,  0.8955],\n          [-0.7783, -0.0246,  1.4717,  ...,  0.0328,  0.4316, -0.6416],\n          ...,\n          [ 0.0855, -0.1279, -0.0319,  ..., -0.2817,  1.2744, -0.5854],\n          [ 0.2402,  1.3945, -2.4062,  ...,  0.3435, -0.5254,  1.2441],\n          [ 1.6377,  1.2539,  0.6099,  ...,  1.5391, -0.6304,  0.9092]]]],\n       device='cuda:0', dtype=torch.float16)\n\n\nOur latent has random values which represent noise. This noise needs to be scaled so it can work with the scheduler.\n\nsched.set_timesteps(n_inf_steps); sched\n\nLMSDiscreteScheduler {\n  \"_class_name\": \"LMSDiscreteScheduler\",\n  \"_diffusers_version\": \"0.16.1\",\n  \"beta_end\": 0.012,\n  \"beta_schedule\": \"scaled_linear\",\n  \"beta_start\": 0.00085,\n  \"num_train_timesteps\": 1000,\n  \"prediction_type\": \"epsilon\",\n  \"trained_betas\": null\n}\n\n\n\nlats *= sched.init_noise_sigma; sched.init_noise_sigma\n\ntensor(14.6146)\n\n\n\nsched.sigmas\n\ntensor([14.6146, 13.3974, 12.3033, 11.3184, 10.4301,  9.6279,  8.9020,  8.2443,\n         7.6472,  7.1044,  6.6102,  6.1594,  5.7477,  5.3709,  5.0258,  4.7090,\n         4.4178,  4.1497,  3.9026,  3.6744,  3.4634,  3.2680,  3.0867,  2.9183,\n         2.7616,  2.6157,  2.4794,  2.3521,  2.2330,  2.1213,  2.0165,  1.9180,\n         1.8252,  1.7378,  1.6552,  1.5771,  1.5031,  1.4330,  1.3664,  1.3030,\n         1.2427,  1.1852,  1.1302,  1.0776,  1.0272,  0.9788,  0.9324,  0.8876,\n         0.8445,  0.8029,  0.7626,  0.7236,  0.6858,  0.6490,  0.6131,  0.5781,\n         0.5438,  0.5102,  0.4770,  0.4443,  0.4118,  0.3795,  0.3470,  0.3141,\n         0.2805,  0.2455,  0.2084,  0.1672,  0.1174,  0.0292,  0.0000])\n\n\n\nsched.timesteps\n\ntensor([999.0000, 984.5217, 970.0435, 955.5652, 941.0870, 926.6087, 912.1304,\n        897.6522, 883.1739, 868.6957, 854.2174, 839.7391, 825.2609, 810.7826,\n        796.3043, 781.8261, 767.3478, 752.8696, 738.3913, 723.9130, 709.4348,\n        694.9565, 680.4783, 666.0000, 651.5217, 637.0435, 622.5652, 608.0870,\n        593.6087, 579.1304, 564.6522, 550.1739, 535.6957, 521.2174, 506.7391,\n        492.2609, 477.7826, 463.3043, 448.8261, 434.3478, 419.8696, 405.3913,\n        390.9130, 376.4348, 361.9565, 347.4783, 333.0000, 318.5217, 304.0435,\n        289.5652, 275.0870, 260.6087, 246.1304, 231.6522, 217.1739, 202.6957,\n        188.2174, 173.7391, 159.2609, 144.7826, 130.3043, 115.8261, 101.3478,\n         86.8696,  72.3913,  57.9130,  43.4348,  28.9565,  14.4783,   0.0000],\n       dtype=torch.float64)\n\n\n\nplt.plot(sched.timesteps, sched.sigmas[:-1])"
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#denoise",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#denoise",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Denoise",
    "text": "Denoise\nThe denoising process can now begin!\n\nfrom tqdm.auto import tqdm\n\nfor i, ts in enumerate(tqdm(sched.timesteps)):\n1  inp = torch.cat([lats] * 2)\n2  inp = sched.scale_model_input(inp, ts)\n\n3  with torch.no_grad(): preds = unet(inp, ts, encoder_hidden_states=embs).sample\n\n4  pred_uncond, pred_txt = preds.chunk(2)\n  pred = pred_uncond + g_scale * (pred_txt - pred_uncond)\n\n5  lats = sched.step(pred, ts, lats).prev_sample\n\n\n1\n\nWe first create two latents: one for the text prompt, and one for the unconditional prompt.\n\n2\n\nWe then further scale the noise on the latents.\n\n3\n\nWe then predict noise.\n\n4\n\nWe then perform guidance.\n\n5\n\nWe then subtract the predicted, guided noise from the image."
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#decompress",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#decompress",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Decompress",
    "text": "Decompress\nWe can now decompress the latent and display it.\n\nwith torch.no_grad(): img = vae.decode(1/0.18215*lats).sample\n\n\nimg = (img / 2 + 0.5).clamp(0, 1)\nimg = img[0].detach().cpu().permute(1, 2, 0).numpy()\nimg = (img * 255).round().astype('uint8')\nImage.fromarray(img)\n\n\n\n\n\n\n\n\nAnd there you have it! We implemented stable diffusion using a text encoder, VAE, and U-Net!\nLet’s encapsulate everything so it looks simpler."
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#encapsulation",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#encapsulation",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Encapsulation",
    "text": "Encapsulation\nFirst we’ll encapsulate everything into functions, then we’ll encapsulate into a class.\n\nFunctions\nThe main steps that are happening are:\n\nWe create embeddings.\nWe create latents.\nWe denoise the latents.\nWe decompress the latents.\n\n\nCreate Embeddings\n\ndef set_embs():\n  txt_inp = tok_seq(prompt)\n  uncond_inp = tok_seq(['']*len(prompt), max_len=txt_inp['input_ids'].shape[-1])\n\n  txt_emb = make_emb(txt_inp['input_ids'])\n  uncond_emb = make_emb(uncond_inp['input_ids'])\n  return torch.cat([uncond_emb, txt_emb])\n\ndef tok_seq(prompt, max_len=None):\n  if max_len is None: max_len = tokz.model_max_length\n  return tokz(\n      prompt,\n      padding = 'max_length',\n      max_length = max_len,\n      truncation = True,\n      return_tensors = 'pt'\n  )\n\ndef make_emb(input_ids):\n  return txt_enc(input_ids.to('cuda'))[0].half()\n\n\n\nCreate Latents\n\ndef set_lat():\n  torch.manual_seed(seed)\n  lat = torch.randn((bs, unet.config.in_channels, h//8, w//8))\n  sched.set_timesteps(n_inf_steps)\n  return lat.to('cuda').half() * sched.init_noise_sigma\n\n\n\nDenoise Latents\n\ndef denoise(latent, embeddings, timestep):\n  inp = sched.scale_model_input(torch.cat([latent]*2), timestep)\n  with torch.no_grad():\n    pred_uncond, pred_txt = unet(inp, timestep, encoder_hidden_states=embeddings).sample.chunk(2)\n  pred = pred_uncond + g_scale * (pred_txt - pred_uncond)\n  return sched.step(pred, timestep, latent).prev_sample\n\n\n\nDecompress Latents\n\ndef decompress_lat(latent):\n  with torch.no_grad(): img = vae.decode(1/0.18215*latent).sample\n  img = (img / 2 + 0.5).clamp(0, 1)\n  img = img[0].detach().cpu().permute(1, 2, 0).numpy()\n  return (img * 255).round().astype('uint8')\n\n\n\nPutting it All Together\n\nprompt = ['An antique 18th century painting of a gorilla eating a plate of chips.']\nembs = set_embs()\nlat = set_lat()\nfor i, ts in enumerate(tqdm(sched.timesteps)): lat = denoise(lat, embs, ts)\nimg = decompress_lat(lat)\nImage.fromarray(img)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNegative Prompts.\nTo implement negative prompts, we can simply pass in a list containing the negative prompt. This will be used in place of the empty list used for the unconditional prompt.\n\ndef set_embs():\n  txt_inp = tok_seq(prompt)\n  uncond_inp = tok_seq(neg_prompt*len(prompt), max_len=txt_inp['input_ids'].shape[-1])\n\n  txt_emb = make_emb(txt_inp['input_ids'])\n  uncond_emb = make_emb(uncond_inp['input_ids'])\n  return torch.cat([uncond_emb, txt_emb])\n\ndef tok_seq(prompt, max_len=None):\n  if max_len is None: max_len = tokz.model_max_length\n  return tokz(\n      prompt,\n      padding = 'max_length',\n      max_length = max_len,\n      truncation = True,\n      return_tensors = 'pt'\n  )\n\n\nprompt = ['An antique 18th century painting of a gorilla eating a plate of chips.']\nneg_prompt = ['plate']\nembs = set_embs()\nlat = set_lat()\nfor i, ts in enumerate(tqdm(sched.timesteps)): lat = denoise(lat, embs, ts)\nimg = decompress_lat(lat)\nImage.fromarray(img)\n\n\n\n\n\n\n\n\n\n\n\nLet’s now encapsulate everything into a class, so we can much more easily further iterate.\n\n\n\nClass\nI’ll be tweaking the code above so that multiple prompts can be input.\nThis is as simple as using the length of the list of prompts as the batch size (an image is generated for each prompt).\n\nclass Diffuser:\n  def __init__(self, prompts, neg_prompt=[''], guidance=7.5, seed=100, steps=70, width=512, height=512):\n    self.prompts = prompts\n    self.bs = len(prompts)\n    self.neg_prompt = neg_prompt\n    self.g = guidance\n    self.seed = seed\n    self.steps = steps\n    self.w = width\n    self.h = height\n  \n  def diffuse(self, progress=0):\n    embs = self.set_embs()\n    lats = self.set_lats()\n    for i, ts in enumerate(tqdm(sched.timesteps)): lats = self.denoise(lats, embs, ts)\n    return self.decompress_lats(lats)\n  \n  def set_embs(self):\n    txt_inp = self.tok_seq(self.prompts)\n    neg_inp = self.tok_seq(self.neg_prompt * len(self.prompts))\n\n    txt_embs = self.make_embs(txt_inp['input_ids'])\n    neg_embs = self.make_embs(neg_inp['input_ids'])\n    return torch.cat([neg_embs, txt_embs])\n  \n  def tok_seq(self, prompts, max_len=None):\n    if max_len is None: max_len = tokz.model_max_length\n    return tokz(prompts, padding='max_length', max_length=max_len, truncation=True, return_tensors='pt')    \n  \n  def make_embs(self, input_ids):\n    return txt_enc(input_ids.to('cuda'))[0].half()\n\n  def set_lats(self):\n    torch.manual_seed(self.seed)\n    lats = torch.randn((self.bs, unet.config.in_channels, self.h//8, self.w//8))\n    sched.set_timesteps(self.steps)\n    return lats.to('cuda').half() * sched.init_noise_sigma\n\n  def denoise(self, latents, embeddings, timestep):\n    inp = sched.scale_model_input(torch.cat([latents]*2), timestep)\n    with torch.no_grad(): pred_neg, pred_txt = unet(inp, timestep, encoder_hidden_states=embeddings).sample.chunk(2)\n    pred = pred_neg + self.g * (pred_txt - pred_neg)\n    return sched.step(pred, timestep, latents).prev_sample\n\n  def decompress_lats(self, latents):\n    with torch.no_grad(): imgs = vae.decode(1/0.18215*latents).sample\n    imgs = (imgs / 2 + 0.5).clamp(0, 1)\n    imgs = [img.detach().cpu().permute(1, 2, 0).numpy() for img in imgs]\n    return [(img*255).round().astype('uint8') for img in imgs]\n\n  def update_params(self, **kwargs):\n    allowed_params = ['prompts', 'neg_prompt', 'guidance', 'seed', 'steps', 'width', 'height']\n    for k, v in kwargs.items():\n      if k not in allowed_params:\n        raise ValueError(f\"Invalid parameter name: {k}\")\n      if k == 'prompts':\n        self.prompts = v\n        self.bs = len(v)\n      else:\n        setattr(self, k, v)\n\nNow creating a diffuser is as simple as this!\n\nprompts = [\n    'A lightning bolt striking a jumbo jet; 4k; photorealistic',\n    'A toaster in the style of Jony Ive; modern; different; apple; form over function'\n]\ndiffuser = Diffuser(prompts, seed=42)\nimgs = diffuser.diffuse()\n\n\n\n\n\nImage.fromarray(imgs[0])\n\n\n\n\n\n\n\n\n\nImage.fromarray(imgs[1])\n\n\n\n\n\n\n\n\nLet’s remove the wooden background from the second image.\n\nprompt = [prompts[1]]\ndiffuser.update_params(prompts=prompt, neg_prompt='wood')\nImage.fromarray(diffuser.diffuse()[0])\n\n\n\n\n\n\n\n\n\n\n\nNow that we have a class, we can easily add more functionality to our diffuser."
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#extra-functionality",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#extra-functionality",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Extra Functionality",
    "text": "Extra Functionality\n\nCallbacks\nLet’s make the diffuser output how the generated image looks like at each step interval (e.g., every 5 steps), if specified so.\nTo do so, we can simply tweak the diffuser.diffuse() method by make it output the latent at each desired interval.\ndef diffuse(self, interval=0)\n  embs = self.set_embs()\n  lats = self.set_lats()\n1  if interval &gt; 0:\n2    row = []\n    for i, ts in enumerate(tqdm(sched.timesteps)):\n      lats = self.denoise(lats, embs, ts)\n3      if (i % progress) == 0:\n        row.append(self.decompress_lats(lats)[0])\n4    row = np.concatenate(row, axis=1)\n5    display(Image.fromarray(row))\n  else:\n    for i, ts in enumerate(tqdm(sched.timesteps)): lats = self.denoise(lats, embs, ts)\n  return self.decompress_lats(lats)\n\n1\n\nWe first check if callbacks are desired (we can’t save how the latents looked like every 0 intervals).\n\n2\n\nAn empty list is created to store the images.\n\n3\n\nWe check if we have reached our desired interval. If the current loop number matches the interval, it should divide the interval cleanly.\n\n4\n\nWe smoosh all images into one long line.\n\n5\n\nThe image is displayed.\n\n\n\n\nDiffuser Class Redefined\nclass Diffuser:\n  def __init__(self, prompts, neg_prompt=[''], guidance=7.5, seed=100, steps=70, width=512, height=512):\n    self.prompts = prompts\n    self.bs = len(prompts)\n    self.neg_prompt = neg_prompt\n    self.g = guidance\n    self.seed = seed\n    self.steps = steps\n    self.w = width\n    self.h = height\n  \n  def diffuse(self, interval=0):\n    embs = self.set_embs()\n    lats = self.set_lats()\n    if interval &gt; 0:\n      row = []\n      for i, ts in enumerate(tqdm(sched.timesteps)):\n        lats = self.denoise(lats, embs, ts)\n        if (i % interval) == 0: row.append(self.decompress_lats(lats)[0])\n      row = np.concatenate(row, axis=1)\n      display(Image.fromarray(row))\n    else: \n      for i, ts in enumerate(tqdm(sched.timesteps)): lats = self.denoise(lats, embs, ts)\n    return self.decompress_lats(lats)\n  \n  def set_embs(self):\n    txt_inp = self.tok_seq(self.prompts)\n    neg_inp = self.tok_seq(self.neg_prompt * len(self.prompts))\n\n    txt_embs = self.make_embs(txt_inp['input_ids'])\n    neg_embs = self.make_embs(neg_inp['input_ids'])\n    return torch.cat([neg_embs, txt_embs])\n  \n  def tok_seq(self, prompts, max_len=None):\n    if max_len is None: max_len = tokz.model_max_length\n    return tokz(prompts, padding='max_length', max_length=max_len, truncation=True, return_tensors='pt')    \n  \n  def make_embs(self, input_ids):\n    return txt_enc(input_ids.to('cuda'))[0].half()\n\n  def set_lats(self):\n    torch.manual_seed(self.seed)\n    lats = torch.randn((self.bs, unet.config.in_channels, self.h//8, self.w//8))\n    sched.set_timesteps(self.steps)\n    return lats.to('cuda').half() * sched.init_noise_sigma\n\n  def denoise(self, latents, embeddings, timestep):\n    inp = sched.scale_model_input(torch.cat([latents]*2), timestep)\n    with torch.no_grad(): pred_neg, pred_txt = unet(inp, timestep, encoder_hidden_states=embeddings).sample.chunk(2)\n    pred = pred_neg + self.g * (pred_txt - pred_neg)\n    return sched.step(pred, timestep, latents).prev_sample\n\n  def decompress_lats(self, latents):\n    with torch.no_grad(): imgs = vae.decode(1/0.18215*latents).sample\n    imgs = (imgs / 2 + 0.5).clamp(0, 1)\n    imgs = [img.detach().cpu().permute(1, 2, 0).numpy() for img in imgs]\n    return [(img*255).round().astype('uint8') for img in imgs]\n\n  def update_params(self, **kwargs):\n    allowed_params = ['prompts', 'neg_prompt', 'guidance', 'seed', 'steps', 'width', 'height']\n    for key, value in kwargs.items():\n      if key not in allowed_params:\n        raise ValueError(f\"Invalid parameter name: {key}\")\n      if key == 'prompts':\n        self.prompts = value\n        self.bs = len(value)\n      else:\n        setattr(self, key, value)\n\n\n\nprompt = ['A toaster in the style of Jony Ive; modern; realistic; different; apple; form over function']\ndiffuser = Diffuser(prompts=prompt, neg_prompt=['wood'], seed=42)\nImage.fromarray(diffuser.diffuse(interval=7)[0])"
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#conclusion",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#conclusion",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Conclusion",
    "text": "Conclusion\nAnd there you have it! All that’s happening is:\n\nA compressed, noisy image is generated.\nThe noise in the image is predicted.\nThe predicted noise is subtracted.\nThis is repeated until desired.\nThe final image is decompressed.\n\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/27_pulling_back_the_curtain_on_vlm_attention.html",
    "href": "forblog/posts/27_pulling_back_the_curtain_on_vlm_attention.html",
    "title": "Pulling Back the Curtain on VLM Attention",
    "section": "",
    "text": "In this converation, I attempted to understand how this notebook, by user zjysteven visualized the attention placed on an image by a VLM. My main confusion stemmed around the two different attentions that are calculated in the notebook."
  },
  {
    "objectID": "forblog/posts/27_pulling_back_the_curtain_on_vlm_attention.html#foreword",
    "href": "forblog/posts/27_pulling_back_the_curtain_on_vlm_attention.html#foreword",
    "title": "Pulling Back the Curtain on VLM Attention",
    "section": "",
    "text": "In this converation, I attempted to understand how this notebook, by user zjysteven visualized the attention placed on an image by a VLM. My main confusion stemmed around the two different attentions that are calculated in the notebook."
  },
  {
    "objectID": "forblog/posts/27_pulling_back_the_curtain_on_vlm_attention.html#the-confusion-of-two-attentions",
    "href": "forblog/posts/27_pulling_back_the_curtain_on_vlm_attention.html#the-confusion-of-two-attentions",
    "title": "Pulling Back the Curtain on VLM Attention",
    "section": "The Confusion of Two Attentions",
    "text": "The Confusion of Two Attentions\nWe observed the code first calculating an llm_attn_matrix. At the time, we thought: “Great, this must cover all attention, including both text and images.” But then, the code computed a separate vis_attn_matrix.\n“Why recalculate visual attention?” you asked. “Didn’t we already calculate attention for all tokens, including the image tokens?” This was a brilliant question—and the key to understanding the entire process. We gradually realized these two attention types originate from different parts of the model and address fundamentally distinct questions."
  },
  {
    "objectID": "forblog/posts/27_pulling_back_the_curtain_on_vlm_attention.html#the-language-model-the-clever-manager",
    "href": "forblog/posts/27_pulling_back_the_curtain_on_vlm_attention.html#the-language-model-the-clever-manager",
    "title": "Pulling Back the Curtain on VLM Attention",
    "section": "The Language Model, the Clever “Manager”",
    "text": "The Language Model, the Clever “Manager”\nWe first grasped the role of the language model’s (LLM) attention. It acts like a strategic manager. When the model generates a word like “cat,” this “manager” examines the question and image, then decides: “To output ‘cat,’ the image token representing ‘ears’ is most critical, while ‘whiskers’ is moderately important.” Thus, the LLM’s attention answers the “which” question. It assigns an importance score to each image token. But here was the catch: these scores are “coarse.” They reveal which large patch matters but not what within the patch the model focuses on. Plotting this alone would color entire patches uniformly, yielding unusable visualizations."
  },
  {
    "objectID": "forblog/posts/27_pulling_back_the_curtain_on_vlm_attention.html#the-vision-encoder-the-diligent-analyst",
    "href": "forblog/posts/27_pulling_back_the_curtain_on_vlm_attention.html#the-vision-encoder-the-diligent-analyst",
    "title": "Pulling Back the Curtain on VLM Attention",
    "section": "The Vision Encoder, the Diligent “Analyst”",
    "text": "The Vision Encoder, the Diligent “Analyst”\nSo what does the vision encoder’s (ViT) attention do? Initially, we mistakenly assumed it also knew the question. But we soon learned it works like a meticulous “analyst” that completes its job before the question ever arrives. Its task is to analyze the image in extreme detail, completely independent of the question. It examines every small image patch and generates a “map” for each, revealing how that patch relates to all others. The ViT’s attention thus answers the “what” question—outputting detailed maps without scores. “But if it doesn’t know the question,” you asked, “how would it highlight relevant features? It could focus on irrelevant elements.” You were absolutely right. The ViT alone cannot determine what matters for the question; it merely catalogues all interesting structures in the image."
  },
  {
    "objectID": "forblog/posts/27_pulling_back_the_curtain_on_vlm_attention.html#the-power-of-collaboration",
    "href": "forblog/posts/27_pulling_back_the_curtain_on_vlm_attention.html#the-power-of-collaboration",
    "title": "Pulling Back the Curtain on VLM Attention",
    "section": "The Power of Collaboration",
    "text": "The Power of Collaboration\nFinally, we understood how everything integrates. It’s a two-step process, like manager and analyst collaborating:\n\nThe diligent analyst (ViT) first prepares a detailed image analysis report—containing a unique attention “map” for every patch.\nThe clever manager (LLM) reads the question, consults this report, and selects which maps to use (and their weights).\nWe, as external observers, then combine these selected maps according to the LLM’s weights to produce the final, clear attention heatmap.\n\nThe resulting visualization isn’t generated by any single component—it emerges from the symbiosis of both systems, harmonized by our interpretation."
  },
  {
    "objectID": "forblog/posts/27_pulling_back_the_curtain_on_vlm_attention.html#end-of-narration",
    "href": "forblog/posts/27_pulling_back_the_curtain_on_vlm_attention.html#end-of-narration",
    "title": "Pulling Back the Curtain on VLM Attention",
    "section": "End of Narration",
    "text": "End of Narration\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/27_pulling_back_the_curtain_on_vlm_attention.html#frequently-asked-questions-faq",
    "href": "forblog/posts/27_pulling_back_the_curtain_on_vlm_attention.html#frequently-asked-questions-faq",
    "title": "Pulling Back the Curtain on VLM Attention",
    "section": "Frequently Asked Questions (FAQ)",
    "text": "Frequently Asked Questions (FAQ)\nQ: Why do we need two attention mechanisms (LLM’s and ViT’s)? A: Because they answer two distinct but necessary questions. LLM attention is “coarse,” identifying which patch matters for word generation. ViT attention is “fine,” providing spatial “maps” for each patch to reveal internal image structures.\nQ: If the vision encoder doesn’t know my question, isn’t its attention potentially irrelevant? A: Correct. The ViT objectively analyzes the image, cataloguing all salient structures unrelated to the question. The LLM then acts as a filter, selecting only ViT outputs relevant to the question.\nQ: Why not use only the LLM’s attention for plotting? Doesn’t it already know what’s important? A: LLM attention assigns scores to entire patches but lacks spatial resolution. Plotting it alone would color entire patches uniformly, producing low-detail visualizations. ViT maps provide the necessary granularity.\nQ: So the vision encoder generates one attention map per image patch? How do we display so many maps? A: Exactly. For a 576-patch image, the ViT produces 576 maps. The final heatmap is a weighted average of all 576 maps, with weights derived from the LLM’s importance scores for each patch."
  },
  {
    "objectID": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html",
    "href": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html",
    "title": "A No Nonsense Guide on how to use an M-Series Mac GPU with PyTorch",
    "section": "",
    "text": "This blog post was updated on Saturday, 28 January 2023.\nIf you have one of those fancy Macs with an M-Series chip (M1/M2, etc.), here’s how to make use of its GPU in PyTorch for increased performance.\nIt’s a bit annoying and a little tedious, but here we go."
  },
  {
    "objectID": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#requirements",
    "href": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#requirements",
    "title": "A No Nonsense Guide on how to use an M-Series Mac GPU with PyTorch",
    "section": "1 Requirements",
    "text": "1 Requirements\n\nHave an M-Series chip\nHave at least PyTorch 1.12\nHave at least macOS Monterey 12.3"
  },
  {
    "objectID": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#installing-pytorch",
    "href": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#installing-pytorch",
    "title": "A No Nonsense Guide on how to use an M-Series Mac GPU with PyTorch",
    "section": "2 Installing PyTorch",
    "text": "2 Installing PyTorch\nInstall PyTorch as you usually would. Just make sure it’s PyTorch 1.12.\n# Installing with Pip.\n$ pip3 install torch torchvision torchaudio\n\n# Installing using Conda.\n$ conda install pytorch torchvision torchaudio -c pytorch\nBy using these commands, the latest version of the library is installed so there is no need to specify the version number.\nHowever, if you have an existing installation, you can run the following Pip command instead.\n$ pip3 install --upgrade torch torchvision torchaudio"
  },
  {
    "objectID": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#import-pytorch",
    "href": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#import-pytorch",
    "title": "A No Nonsense Guide on how to use an M-Series Mac GPU with PyTorch",
    "section": "3 Import PyTorch",
    "text": "3 Import PyTorch\n\nimport torch"
  },
  {
    "objectID": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#check-requirements-are-met",
    "href": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#check-requirements-are-met",
    "title": "A No Nonsense Guide on how to use an M-Series Mac GPU with PyTorch",
    "section": "4 Check Requirements are Met",
    "text": "4 Check Requirements are Met\nBelow is a convenient code snippet taken from the PyTorch documentation that checks whether requirements are met.\n\nif not torch.backends.mps.is_available():\n    if not torch.backends.mps.is_built():\n        print(\"MPS not available because the current PyTorch install was not built with MPS enabled.\")\n    else:\n        print(\"MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.\")\n\nIf neither of the two above messages print, you’re good to go!"
  },
  {
    "objectID": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#the-annoying-part-enabling-the-gpu",
    "href": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#the-annoying-part-enabling-the-gpu",
    "title": "A No Nonsense Guide on how to use an M-Series Mac GPU with PyTorch",
    "section": "5 The Annoying Part: Enabling the GPU",
    "text": "5 The Annoying Part: Enabling the GPU\nAs far as I know, you must explicitly enable the use of the GPU for whatever model or tensor you wish to use the GPU for.\nThere are different ways you can do this.\nUse a string.\n\nt = torch.tensor([1, 2, 3], device='mps')\n\n\n\n\ntensor([1, 2, 3], device='mps:0')\n\n\n\nStore as a variable.\n\ndevice='mps'\nt = torch.tensor([1, 2, 3], device=device)\n\n\n\n\ntensor([1, 2, 3], device='mps:0')\n\n\n\nConvert existing objects.\n\nt = torch.tensor([1, 2, 3])\nt.to(device)\n\n\n\n\ntensor([1, 2, 3], device='mps:0')\n\n\n\nNote that converting existing objects creates a copy and does not modify the original.\n\nt\n\n\n\n\ntensor([1, 2, 3])\n\n\n\nThough the above operations have been performed on tensors, they can also be performed on models."
  },
  {
    "objectID": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#points-to-note",
    "href": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#points-to-note",
    "title": "A No Nonsense Guide on how to use an M-Series Mac GPU with PyTorch",
    "section": "6 Points to Note",
    "text": "6 Points to Note\n\nGPU enabled means operations are done on the GPU.\nA GPU enabled tensor can only perform operations with another GPU enabled tensor.\nAs of writing this, GPU support is still in its early stages. So certain features are unsupported and further optimizations await."
  },
  {
    "objectID": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#relevant-links",
    "href": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#relevant-links",
    "title": "A No Nonsense Guide on how to use an M-Series Mac GPU with PyTorch",
    "section": "Relevant Links",
    "text": "Relevant Links\nRelevant links:\n\nInstalling PyTorch: https://pytorch.org/get-started/locally/\nDocs on using GPU: https://pytorch.org/docs/stable/notes/mps.html\nPerformance gains (note that nightly builds are no longer needed): https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/"
  },
  {
    "objectID": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#closing-words",
    "href": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#closing-words",
    "title": "A No Nonsense Guide on how to use an M-Series Mac GPU with PyTorch",
    "section": "Closing Words",
    "text": "Closing Words\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html",
    "title": "Detecting Floods for Disaster Relief",
    "section": "",
    "text": "You can find this notebook on Kaggle here.\nThis article was updated on Friday, 11 November 2022.\nThe model that will be created in this notebook can detect whether an area shown in an image is flooded or not. The idea for creating this model has been spurred from the recent floodings in Pakistan.\nSuch models can prove useful in flood relief, helping to detect which areas need immediate focus.\nThe dataset used to train this model is Louisiana flood 2016, uploaded by Kaggle user Rahul T P, which you can view here.\nThe fastai library, a high level PyTorch library, has been used.\nOne of the points of this notebook is to showcase how simple it is to create powerful models. That said, this notebook is not a tutorial or guide.\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html#sort-data.",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html#sort-data.",
    "title": "Detecting Floods for Disaster Relief",
    "section": "Sort data.",
    "text": "Sort data.\nThe data in the dataset needs to be organized into train and valid folders. Each will contain the same subfolders, 0 and 1, which will be used to label the data. A label of 0 indicates the area shown in the image is not flooded, while a label of 1 indicates the area shown in the image is flooded.\nThe images in the dataset itself has been organized as follows:\n    If no underscore is in the file name, the image shows the area before or after the flood.\n    If an underscore is in the file name, the image shows the area during the flood:\n\nIf a zero follows the underscore, the area was not flooded.\nIf a one follows the underscore, the area was flooded.\n\nCreating the necessary paths.\n\nworking_path = Path.cwd(); print(working_path)\nfolders = ('train', 'valid')\nlabels = ('0', '1')\n\n/kaggle/working\n\n\n\ninput_path = Path('/kaggle/input')\ntrain_image_paths = sorted(input_path.rglob('train/*.png'))\nvalid_image_paths = sorted(input_path.rglob('test/*.png'))\nlen(train_image_paths), len(valid_image_paths)\n\n(270, 52)\n\n\nCreating the necessary directories.\n\nfor folder in folders:\n    if not (working_path/folder).exists():\n        (working_path/folder).mkdir()\n    for label in labels:\n        if not (working_path/folder/label).exists():\n            (working_path/folder/label).mkdir()\n\nMove images to new directories.\n\ntry:\n    for image_path in train_image_paths:\n        if '_1' in image_path.stem:\n            with (working_path/'train'/'1'/image_path.name).open(mode='xb') as f:\n                f.write(image_path.read_bytes())\n        else:\n            with (working_path/'train'/'0'/image_path.name).open(mode='xb') as f:\n                f.write(image_path.read_bytes())\nexcept FileExistsError:\n    print(\"Training images have already been moved.\")\nelse:\n    print(\"Training images moved.\")\n\nTraining images moved.\n\n\n\ntry:\n    for image_path in valid_image_paths:\n        if '_1' in image_path.stem:\n            with (working_path/'valid'/'1'/image_path.name).open(mode='xb') as f:\n                f.write(image_path.read_bytes())\n        else:\n            with (working_path/'valid'/'0'/image_path.name).open(mode='xb') as f:\n                f.write(image_path.read_bytes())\nexcept FileExistsError:\n    print(\"Testing images have already been moved.\")\nelse:\n    print(\"Testing images moved.\")\n\nTesting images moved.\n\n\nCheck that images have been moved.\n\ntraining_images = get_image_files(working_path/'train'); print(len(training_images))\n\n270\n\n\n\nImage.open(training_images[0])\n\n\n\n\n\n\n\n\n\nvalidation_images = get_image_files(working_path/'valid'); print(len(validation_images))\n\n52\n\n\n\nImage.open(validation_images[-1])"
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html#load-data",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html#load-data",
    "title": "Detecting Floods for Disaster Relief",
    "section": "Load data",
    "text": "Load data\nCreate the training and validation dataloaders through fastai’s quick and easy DataBlock class.\n\ndataloaders = DataBlock(\n    blocks = (ImageBlock, CategoryBlock),\n    get_items = get_image_files,\n    splitter = GrandparentSplitter(),\n    get_y = parent_label,\n    item_tfms = [Resize(192, method='squish')]\n).dataloaders(working_path, bs=32)\n\nCheck that data has been loaded correctly.\n\ndataloaders.show_batch(max_n=8)"
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html#instantiate-and-train-model",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html#instantiate-and-train-model",
    "title": "Detecting Floods for Disaster Relief",
    "section": "Instantiate and Train Model",
    "text": "Instantiate and Train Model\n\nlearner = vision_learner(dataloaders, resnet18, metrics=error_rate)\nlearner.fine_tune(9)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.919323\n1.118264\n0.365385\n00:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.490039\n0.628054\n0.250000\n00:02\n\n\n1\n0.367996\n0.411558\n0.192308\n00:02\n\n\n2\n0.266664\n0.472146\n0.192308\n00:02\n\n\n3\n0.203069\n0.256436\n0.115385\n00:03\n\n\n4\n0.158453\n0.127106\n0.076923\n00:03\n\n\n5\n0.124499\n0.095927\n0.038462\n00:02\n\n\n6\n0.098409\n0.089279\n0.038462\n00:03\n\n\n7\n0.079600\n0.093277\n0.038462\n00:02\n\n\n8\n0.064886\n0.090372\n0.038462\n00:02\n\n\n\n\n\nNice! A relatively low error rate for no tweaking."
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html#visualizing-mistakes",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html#visualizing-mistakes",
    "title": "Detecting Floods for Disaster Relief",
    "section": "Visualizing Mistakes",
    "text": "Visualizing Mistakes\nWe have to see how the model is getting confuzzled.\n\ninterp = ClassificationInterpretation.from_learner(learner)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnly a couple of mistakes. Let’s see what they are.\n\ninterp.plot_top_losses(5, nrows=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNothing has been mislabeled, but the first one is especially tricky to determine, even for human eyes."
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html#model-inference",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html#model-inference",
    "title": "Detecting Floods for Disaster Relief",
    "section": "Model Inference",
    "text": "Model Inference\nLet’s test the model on some images of the recent flooding in Pakistan.\n\ndef infer_image(image_path):\n    display(Image.open(image_path))\n    label, _, probabilities = learner.predict(PILImage(PILImage.create(image_path)))\n    if label == '0':\n        print(f\"The area shown in the image is not flooded with probability {probabilities[0]*100:.2f}%.\")\n    elif label == '1':\n        print(f\"The area shown in the image is flooded with probability {probabilities[1]*100:.2f}%.\")\n    else:\n        print(\"Unknown label assigned to image.\")\n\n\ninfer_image(input_path/'floodclassifiertestset'/'1'/'1.jpeg')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe area shown in the image is not flooded with probability 65.65%.\n\n\nNot bad!\nLet’s try it on another image.\n\ninfer_image(input_path/'floodclassifiertestset'/'1'/'2.jpg')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe area shown in the image is flooded with probability 99.90%.\n\n\nThe label for this image is kind of meaningless. This is an image of a vast area of land, so certain areas could be flooded, while others are not. That said, it could be used to determine whether there is flooding in the image.\n\ninfer_image(input_path/'floodclassifiertestset'/'1'/'3.jpg')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe area shown in the image is flooded with probability 99.99%.\n\n\nThe model performed really well in this case: the input image is shown at a different angle. The images in the training set only show areas from a top-down view.\n\ninfer_image(input_path/'floodclassifiertestset'/'1'/'4.jpg')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe area shown in the image is not flooded with probability 64.56%.\n\n\nOver here, the limitations of the current state of the model can be seen. The model is not performing well on images where the view is more parallel to the ground, since the images in the training set are all top-down.\nLet’s do two more images.\n\ninfer_image(input_path/'floodclassifiertestset'/'1'/'5.jpg')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe area shown in the image is flooded with probability 99.94%.\n\n\n\ninfer_image(input_path/'floodclassifiertestset'/'1'/'6.jpg')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe area shown in the image is flooded with probability 100.00%.\n\n\nThe model is working well with images of different sizes too, and has given this image a very high, correct confidence."
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html#improving-the-model.",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html#improving-the-model.",
    "title": "Detecting Floods for Disaster Relief",
    "section": "Improving the model.",
    "text": "Improving the model.\nLet’s see if we can get the model’s performance to improve on the following image through augmenting the training set.\n\nImage.open(input_path/'floodclassifiertestset'/'1'/'4.jpg')\n\n\n\n\n\n\n\n\n\naugmented_dataloaders = DataBlock(\n    blocks = (ImageBlock, CategoryBlock),\n    get_items = get_image_files,\n    splitter = GrandparentSplitter(),\n    get_y = parent_label,\n    item_tfms = RandomResizedCrop(192, min_scale=0.5),\n    batch_tfms=aug_transforms()\n).dataloaders(working_path, bs=32)\n\n\naugmented_dataloaders.show_batch(max_n=8)\n\n\n\n\n\n\n\n\n\naugmented_learner = vision_learner(augmented_dataloaders, resnet18, metrics=error_rate)\naugmented_learner.fine_tune(9)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.161182\n0.835870\n0.365385\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.442552\n0.686252\n0.288462\n00:03\n\n\n1\n0.417739\n0.411907\n0.153846\n00:02\n\n\n2\n0.346400\n0.316388\n0.057692\n00:03\n\n\n3\n0.306782\n0.213407\n0.076923\n00:02\n\n\n4\n0.251947\n0.199586\n0.076923\n00:02\n\n\n5\n0.209951\n0.141818\n0.057692\n00:02\n\n\n6\n0.188433\n0.116713\n0.057692\n00:03\n\n\n7\n0.169689\n0.125078\n0.057692\n00:02\n\n\n8\n0.151843\n0.131188\n0.057692\n00:02\n\n\n\n\n\nLet’s try the new model out.\n\ndisplay(Image.open(input_path/'floodclassifiertestset'/'1'/'4.jpg'))\nlabel, _, probabilities = augmented_learner.predict(PILImage(PILImage.create(input_path/'floodclassifiertestset'/'1'/'4.jpg')))\nif label == '0':\n    print(f\"The area shown in the image is not flooded with probability {probabilities[0]*100:.2f}%.\")\nelif label == '1':\n    print(f\"The area shown in the image is flooded with probability {probabilities[1]*100:.2f}%.\")\nelse:\n    print(\"Unknown label assigned to image.\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe area shown in the image is flooded with probability 99.91%.\n\n\nDang, impressive! The correct label and with excellent confidence!\nBefore we get too excited though, we should check the performance on the model with the previous images.\n\ntest_dataloader = learner.dls.test_dl([image_path for image_path in sorted((input_path/'floodclassifiertestset').rglob('*.*'))])\n\n\nprobabilities, _, labels = augmented_learner.get_preds(dl=test_dataloader, with_decoded=True)\n\n\n\n\n\n\n\n\n\nprint(\"Images are numbered horizontally.\")\ntest_dataloader.show_batch()\nfor probability, label, image_number in zip(probabilities, labels, range(1, 7)):\n    if label == 1:\n        print(f\"Image {image_number} is flooded with a probability of {probability[1]*100:.2f}%.\")\n    elif label == 0:\n        print(f\"Image {image_number} is not flooded with a probability of {probability[0]*100:.2f}%.\")\n    else:\n        print(f\"Image {image_number} has been assigned an unknown label.\")\n\nImages are numbered horizontally.\nImage 1 is flooded with a probability of 95.94%.\nImage 2 is flooded with a probability of 99.92%.\nImage 3 is flooded with a probability of 91.34%.\nImage 4 is flooded with a probability of 99.71%.\nImage 5 is flooded with a probability of 100.00%.\nImage 6 is flooded with a probability of 100.00%.\n\n\n\n\n\n\n\n\n\nDrastically improved probabilities! A little augmentation can go a long way."
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html#takeaways",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html#takeaways",
    "title": "Detecting Floods for Disaster Relief",
    "section": "Takeaways",
    "text": "Takeaways\nThis model was trained on only 270 images and minimal code. Accessbility and abstraction to the field of machine learning has come a long, long way. Given the right data and the right pretrained model, a powerful model can be produced in less than an hour, if not half.\nThis is important: in disasters such as floods, the time taken to produce the logistics required for relief can be drastically reduced. It is also important because the barrier of entry to this field is dramatically lowered; more people can create powerful models, in turn producing better solutions.\nHowever, there could be some improvements and additions made to the model:\n\nInclude a third class to the model. Images that are not flooded, but show signs of having been flooded would be assigned this class. The dataset used for this model includes such images.\nTrain the model on images that include a variety of geographic locations and dwellings. The current dataset only contains images taken in a lush, green area with plenty of trees; infrastructure looks a certain way; the color of the floodwater is also dependent on the surroundings. All this makes the model good a prediciting whether an image is flooded for images with certain features.\n\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/21_reflecting_on_my_internships.html",
    "href": "forblog/posts/21_reflecting_on_my_internships.html",
    "title": "What I Learned during my Second and Third Internships",
    "section": "",
    "text": "I recently completed my second and third internships. These were quite hands on, and involved tasks such as finetuning LLMs to generate Cantonese lyrics to the topic and tones specified. Along the way, I learned a lot. Three months ago, I would not have been able to do what I did. You learn by doing.\nLet’s list out some of the things I learned (in no particular order)."
  },
  {
    "objectID": "forblog/posts/21_reflecting_on_my_internships.html#conclusion",
    "href": "forblog/posts/21_reflecting_on_my_internships.html#conclusion",
    "title": "What I Learned during my Second and Third Internships",
    "section": "Conclusion",
    "text": "Conclusion\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/12_stable_diffusion_summarized.html",
    "href": "forblog/posts/12_stable_diffusion_summarized.html",
    "title": "Stable Diffusion, Summarized",
    "section": "",
    "text": "This post was edited on Sunday, 30 April 2023\nHere, I explain the workings of stable diffusion at a high level."
  },
  {
    "objectID": "forblog/posts/12_stable_diffusion_summarized.html#components",
    "href": "forblog/posts/12_stable_diffusion_summarized.html#components",
    "title": "Stable Diffusion, Summarized",
    "section": "Components",
    "text": "Components\nA diffuser contains four main components\n\nThe text encoder\nThe image encoder\nThe autoencoder (VAE autoencoder)\nThe neural network (U-net)\n\n\n\n\n\n\nflowchart TB\n    A{{Diffuser}}\n    B([U-net])\n    C([VAE Autoencoder])\n    D([Text Encoder])\n    E([Image Encoder])\n\n    A --&gt; D & E & C & B"
  },
  {
    "objectID": "forblog/posts/12_stable_diffusion_summarized.html#training",
    "href": "forblog/posts/12_stable_diffusion_summarized.html#training",
    "title": "Stable Diffusion, Summarized",
    "section": "Training",
    "text": "Training\nI’ll explain the training process in terms of a single image.\nWhen all components shown above are put into their respective places, the overall training process looks like this.\n\n\n\n\n\n\nflowchart LR\n    subgraph A [Feature Vector Creation]\n        id1([Text Encoder])\n        id2([Image Encoder])\n    end\n\n    subgraph B [Image Compression]\n        id3([VAE Autoencoder])\n    end\n\n    subgraph C [Noise Removal]\n        id4([U-net])\n    end\n\n    subgraph D [Image Decompression]\n        id5([VAE Autoencoder])\n    end\n\n    id7[Input Image Description] & id6[Input Image] --&gt; A --&gt; id9[Feature Vector]\n    id6 --&gt; B --noise added to image--&gt; id10[Noisy Latent]\n    id9 & id10 --&gt; C --&gt; id11[Less Noisy Latent] --&gt;  C\n    id11 --&gt; D --&gt; id8[Generated Image]\n\n\n\n\n\n\n\nLet’s break it down.\n\nFeature Vector Creation\n\n\n\n\n\n\nflowchart TB\n    subgraph B [ ]\n        direction LR\n        id1[Input Image]\n        id2[Input Image Description]\n        subgraph A [Feature Vector Creation]\n            id3([Text Encoder])\n            id4([Image Encoder])\n        end\n        id2 & id1 --&gt; A --&gt; id11[Feature Vector]\n    end\n    style B fill:#FFF, stroke:#333,stroke-width:3px\n\n    subgraph C [ ]\n        direction LR\n        id5[Input Image]\n        id7[Input Image Descripton]\n        id5 --&gt; id6\n        id7 --&gt; id8\n        subgraph D [Feature Vector Creation]\n            id6([Image Encoder])\n            id8([Text Encoder])\n            id6 & id8 --&gt; id9[CLIP Embedding]\n        end\n        id9 --&gt; id10[Feature Vector]\n    end\n    style C fill:#FFF, stroke:#333,stroke-width:3px\n\n    B --&gt; C\n    B --&gt; C\n    B --&gt; C\n\n\n\n\n\n\n\nWe start with an image and its description. The image encoder takes the image and produces a feature vector — a vector with numerical values that describe the image in some way. The text encoder takes the image’s description and similarly produces a feature vector.\nThese two feature vectors are then stored in what’s known as a CLIP embedding. An embedding is simply a table where each row is an item and each column describes the items in some way. In this case, the rows represent feature vectors, and the columns are each feature in the vector.\nBoth encoders keep producing feature vectors until they are as similar as possible.\n\n\nImage Compression\n\n\n\n\n\n\nflowchart TB\n    subgraph A [ ]\n        id2[Input Image] --&gt; id1\n        subgraph B [Image Compression]\n            direction LR\n            id1([VAE Autoencoder])\n        end\n        id1 --&gt; id7[Latent]\n    end\n    style A fill:#FFF, stroke:#333,stroke-width:3px\n\n    subgraph C[ ]\n        direction LR\n        id3[Input Image]\n        subgraph D [Image Compression]\n            id4([VAE Encoder])\n            id5([VAE Decoder])\n        end\n        id3 --&gt; id4 --&gt; id6[Latent]\n    end\n    style C fill:#FFF, stroke:#333,stroke-width:3px\n\n    A & A & A --&gt; C\n\n\n\n\n\n\n\nOnce the feature vectors have been produced, the image is compressed by the VAE autoencoder. Some noise is then tossed onto the image.\nThe VAE autoencoder contains an encoder and a decoder. The encoder handles compression whereas the decoder handles decompression.\nThe compressed noisy image is now known as the latent. The image is compressed for faster computation, as there would be fewer pixels to compute on.\n\n\nNoise Removal\n\n\n\n\n\n\nflowchart TB\n    subgraph A [ ]\n        id1[Feature Vector] & id2[Noisy Latent] --&gt; id3\n        subgraph B [Noise Removal]\n            direction LR\n            id3([U-net]) --&gt; id4[Noise]\n        end\n        id4 --with learning rate--&gt; id5[Less Noisy Latent] --&gt; id3\n    end\n    style A fill:#FFF, stroke:#333,stroke-width:3px \n\n    subgraph C [ ]\n        id6[Feature Vector] & id7[Noisy Latent] --&gt; id8\n        subgraph Noise Removal\n            direction LR\n            id8([U-net])\n        end\n        id8 --&gt; id9([Less Noisy Latent]) --&gt; id8\n    end\n    C & C & C --&gt; A\n    style C fill:#FFF, stroke:#333,stroke-width:3px\n\n\n\n\n\n\n\nThe latent, together with its feature vector, is now input to the U-net. Instead of predicting what the original, un-noisy image was, the U-net predicts the noise that was tossed onto the image.\nOnce it outputs the predicted noise, that noise is subtracted from the latent in conjunction with the learning rate. This new, less noisy latent is now input again and the process repeats until desired.\n\n\nImage Decompression\n\n\n\n\n\n\nflowchart TB\n    subgraph A [ ]\n        direction LR\n        id2[Input Image] --&gt; id1\n        subgraph B [Image Decompression]\n            id1([VAE Autoencoder])\n        end\n        id1 --&gt; id7[Latent]\n    end\n    style A fill:#FFF, stroke:#333,stroke-width:3px\n\n    subgraph C [ ]\n        direction LR\n        id3[Less Noisy Latent] --&gt; id5\n        subgraph D [Image Decompression]\n            id4([VAE Encoder])\n            id5([VAE Decoder])\n        end\n        id5 --&gt; id6[Generated Image]\n    end\n    style C fill:#FFF, stroke:#333,stroke-width:3px\n\n    A & A & A --&gt; C\n\n\n\n\n\n\n\nThe latent is now decompressed through the VAE autoencoder’s decoder.\nWe now have a generated image!"
  },
  {
    "objectID": "forblog/posts/12_stable_diffusion_summarized.html#inference",
    "href": "forblog/posts/12_stable_diffusion_summarized.html#inference",
    "title": "Stable Diffusion, Summarized",
    "section": "Inference",
    "text": "Inference\nWhen using a diffuser for inference, the diffuser typically begins with a purely noisey latent. The diffuser uses the input prompt to guide the removal of noise from the latent, until the latent resembles what is desired."
  },
  {
    "objectID": "forblog/posts/12_stable_diffusion_summarized.html#conclusion",
    "href": "forblog/posts/12_stable_diffusion_summarized.html#conclusion",
    "title": "Stable Diffusion, Summarized",
    "section": "Conclusion",
    "text": "Conclusion\nAnd that’s all there is to it!\nWe take an image and its prompt, and create a feature vector out of them. The image is compressed and noise is then added to it. The latent and the feature vector are input to a U-net which then predicts the noise in the latent. The predicted noise is subtracted from the latent, which is then input back to the U-net. After the desired number of steps has lapsed, the latent is decompressed and the generated image is ready!\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/24_the_state_of_pose_estimation.html#foreword",
    "href": "forblog/posts/24_the_state_of_pose_estimation.html#foreword",
    "title": "The State of Pose Estimation",
    "section": "Foreword",
    "text": "Foreword\nIn this post, I have annotated a presentation I gave at my univesity’s multimodal intelligence lab. I’m a newcomer to general purpose robotics, so I had a different mindset and perspective going in.\nAt the end of this post, I provide another perspective that arose as I wrote these annotations."
  },
  {
    "objectID": "forblog/posts/24_the_state_of_pose_estimation.html#annotated-presentation",
    "href": "forblog/posts/24_the_state_of_pose_estimation.html#annotated-presentation",
    "title": "The State of Pose Estimation",
    "section": "Annotated Presentation",
    "text": "Annotated Presentation\n\nHere, I introduce the task assigned to me, which focused on addressing the limitations of VLMs in understanding object poses through fine-tuning on annotated videos.\n\nThe core problem is that current vision-language models (VLMs) struggle with understanding object poses. When queried about an object’s orientation or directional changes, VLMs often fail when even simple complexity is introduced or they forget the object’s prior states.\n\nThe goal is to fine-tune a VLM on videos annotated with pose information to improve its performance in understanding and reasoning about object poses.\n\nTwo main obstacles exist: data and pose models.\n\nA major obstacle is the lack of existing annotated video datasets with object pose information. Self-recorded samples suffer from limited diversity and aren’t suitable for achieving generalizability. While off-the-shelf video datasets exist, they can’t be easily processed due to pose model requirements.\n\nPose estimation models present significant friction due to their specialized input requirements (camera data, reference images, 3D models) and fragmented ecosystem. There’s no standardized text-to-pose interface, requiring manual masking and setup of different libraries/environments for each model.\n\nThe current workflow involves too much friction and insufficient iteration, leading to a lack of confidence in the self-created dataset approach.\n\nQuick iteration is key. Quick iteration is key. Quick iteration is key.\n\nThe workflow involves multiple steps with numerous variables: recording videos (sample count, duration, frames, pose complexity, object count), segmenting videos into frames, pose annotation (RGB, depth, mask frames, camera metadata), text annotation (QnA pairs), and finally fine-tuning. Each step introduces significant time investment and uncertainty.\n\nThe creation of this dataset is a whole project of its own, with the pose model itself being the core component.\n\nSo I suggested a different direction: why not test the underlying theory but with those pieces of data that have less friction.\n\n\nGiven the challenges, a different direction maintaining the same core theory is proposed: testing whether annotating images (with RGBD or other data) improves VLM performance on specific tasks, rather than focusing on pose annotations.\n\nI also did a few experiments of my own, before I was assigned this project, on RGBD images to test for spatial understanding. An off-the-shelf VLM scored ~28% on a given benchmark, a 100 QnA pair finetune scored ~34%, and a 2000 QnA pair finetune scored ~12%.\nMy current intuition for the discrepency on the larger sample size is that while there were 2000 samples, there were only 300 unique RGBD images. This is contrary to the 100 QnA pair set which had 100 unique images. The suggests diversity is indeed important, as I pointed out earlier in my presentation.\n\nBenefits a switching approaches is quicker iteration.\n\nThis new direction enables quicker iteration, allowing me to try more ideas out. I had set up a pipeline in which I was able to conduct those 3 prior benchmarks in a couple of days. With quicker iteration, I was able to already test for diversity. With quicker iteration, I could also test out other ideas, such as: should I keep the RGB and depth images separate? should I overlay the depth image over the RGB image? does using multiple view points of a scene help increase spatial understanding?\n\nSo I am brought to my current perspective on the current state of general purpose robotics: better, more impactful directions exist. Directions that have small steps, but allow for interesting, quick iterations and explorations.\n\nThis is how I currently see the field. I see to many papers and people jumping to fancy pipelines, rather than iterating on the core pieces that underpin those fancy pipelines.\n\nI’m surprised that the primary reason that these general purpose pipelines use VLMs for suck at the very specific reason. (I know this meme doesn’t match the original intent of the xkcd meme, but it appeared in my mind.)\n\nThe field seems to be jumping to complex pipeline integrations rather than conducting small, meaningful iteration experiments. There’s a lack of basic infrastructure for quick experimentation, such as pose estimation models being problematic. Without these basic components and pieces being ironed out, the more interesting directions cannot be explored.\nBetter directions exist that focus on fundamental improvements through systematic testing rather than complex integrations."
  },
  {
    "objectID": "forblog/posts/24_the_state_of_pose_estimation.html#closing-words",
    "href": "forblog/posts/24_the_state_of_pose_estimation.html#closing-words",
    "title": "The State of Pose Estimation",
    "section": "Closing Words",
    "text": "Closing Words\nAs I annotated this presention, it just occured to me: overlay pose annotations on videos perhaps might not be needed at all. Current vision models and VLMs can pick out objects in images without the need for, say, a bounding box and label around the object in question. So perhaps, what needs to be done is that, perhaps, QnA pairs are created in a way that allows for the VLM to learn the concept of spaces, depth, poses, etc. The answer describes the manipulation of the object, the change in pose of the object, and so on.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/16_einstein_summation_notation.html",
    "href": "forblog/posts/16_einstein_summation_notation.html",
    "title": "Intuitively Approaching Einstein Summation Notation",
    "section": "",
    "text": "This post covers einstein summation notation syntax in terms of programming languages.\nEinstein summation notation (or einsum notation for short) is a handy way to write various matrix operations in a succinct, universal manner. With it, you can probably forget all the various symbols and operators there are and stick to one common syntax, that once understood, can be more intuitive.\nFor example, matrix multiplication can be written as ik, kj -&gt; ij and a transpose of a matrix can be written as ij -&gt; ji.\nLet’s figure this out."
  },
  {
    "objectID": "forblog/posts/16_einstein_summation_notation.html#general-rules",
    "href": "forblog/posts/16_einstein_summation_notation.html#general-rules",
    "title": "Intuitively Approaching Einstein Summation Notation",
    "section": "General Rules",
    "text": "General Rules\nThe following are two general rules one can use to quickly write einsum notation.\n\nRepeating letters between input arrays means that values along those axes will be multiplied together.\n\n\nOmitting a letter from the output means that values along that axis will be summed.\n\nHowever, I don’t find these rules intuitive, even a little confusing. Why?\nMatrices have the order of row by column. A 2x3 matrix has 2 rows and 3 columns. When we perform matrix multiplication, we take the dot product of each row in the first matrix with each column in the second matrix.\nHowever, when the einsum rules above — specifically the first rule — are used to denote matrix multiplication (\\(ik, kj \\rightarrow ij\\), as depicted below), the order of a matrix appears to change.\n\nIn order for the einsum rules and the definition of matrix multiplcation above to stay consistent, \\(k\\) now denotes the rows in the first matrix and columns in the second matrix, thereby changing the order of a matrix to column by row.\nBut even if we let \\(k\\) denote the columns in the first matrix, we end up doing dot products with each column in the first matrix and with each row in the second matrix.\nNot intuitive."
  },
  {
    "objectID": "forblog/posts/16_einstein_summation_notation.html#a-more-intuitive-way",
    "href": "forblog/posts/16_einstein_summation_notation.html#a-more-intuitive-way",
    "title": "Intuitively Approaching Einstein Summation Notation",
    "section": "A More Intuitive Way",
    "text": "A More Intuitive Way\nThe key to understanding einsum notation is to not think of axes, but of iterators. For example, \\(i\\) is an iterator that returns the rows of a matrix. \\(j\\) is an iterator that returns the columns of a matrix.\nLet’s begin with a relatively more simple example: the hadamard product (also known as the elementwise product or elementwise multiplication.)\n\nHadamard Product\nWe have the following two matrices.\n\\[\nA\n=\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix},\nB\n=\n\\begin{bmatrix}\n9 & 8 & 7 \\\\\n6 & 5 & 4 \\\\\n3 & 2 & 1\n\\end{bmatrix}\n\\]\nTo access the element 8 in matrix \\(A\\), we need to return the second row and first column1. This can be denoted as \\(a_{21}\\). The first digit in the subscript refers to the row and the second refers to the column. We can refer to any entry generally as \\(a_{ij}\\).\n1 This assumes the matrix is zero indexed. This means \\(\\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix}\\) is the zeroth row of \\(A\\).Taking the hadamard product looks like this.\n\\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix}\n\\odot\n\\begin{bmatrix}\n9 & 8 & 7 \\\\\n6 & 5 & 4 \\\\\n3 & 2 & 1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 \\cdot 9 & 2 \\cdot 8 & 3 \\cdot 7 \\\\\n4 \\cdot 6 & 5 \\cdot 5 & 6 \\cdot 4 \\\\\n7 \\cdot 3 & 8 \\cdot 2 & 9 \\cdot 1\n\\end{bmatrix}\n=\nC\n\\]\n\n\n\n\n\n\nAn alternative way to look at it…\n\n\n\n\n\n\n\n\n\n\n\nDon’t dwell too much on this; it may help to refer back to this later to help understand the einsum notation below.\n\n\n\n\\[\n\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\odot \\begin{bmatrix} 9 & 8 & 7 \\\\ 6 & 5 & 4 \\\\ 3 & 2 & 1 \\end{bmatrix} = \\begin{bmatrix} a_{00}b_{00} & a_{01}b_{01} & a_{02}b_{02} \\\\ a_{10}b_{10} & a_{11}b_{11} & a_{12}b_{12} \\\\ a_{20}b_{20} & a_{21}b_{21} & a_{22}b_{22} \\\\ \\end{bmatrix} = C\n\\]\n\n\n\nIn words, what’s happening is that we’re looping through all the rows of \\(A\\) and \\(B\\). For each row, we also loop through each column and multiply those columns together.\n\n\\(\\text{for row in } A \\text{ and } B\\)\n  \\(\\text{for col in } A \\text{ and } B\\)\n    \\(a_{\\text{rowcol}} \\cdot b_{\\text{rowcol}} = c_{\\text{rowcol}}\\)\n\n\n\\(\\text{for } i \\text{ in } A \\text{ and } B\\)\n  \\(\\text{for } j \\text{ in } A \\text{ and } B\\)\n    \\(a_{ij} \\cdot b_{ij} = c_{ij}\\)\n\nLet’s focus on that last line above.\n\\[a_{ij} \\cdot b_{ij} = c_{ij}\\]\nThis line represents elementwise multiplication. For each row \\(i\\) in \\(A\\) and \\(B\\), we iterate through each column \\(j\\) in those rows, and take their product.\nIn einsum notation, we can more succinctly write this as \\(ij, ij \\rightarrow ij\\). This has 4 parts.\n\n\\(ij\\) refers to the iterators working on the rows and columns of \\(A\\) — \\(i\\) works on the rows and \\(j\\) works on the columns.\n\\(ij\\) refers to the exact same iterators working on \\(B\\).\n\\(\\rightarrow\\) tells us an output will be returned.\n\\(ij\\) refers to the exact same iterators that will be responsble for making up the output matrix \\(C\\). The location \\(ij\\) says where the product of two elements will be located in \\(C\\).\n\n\n\nMatrix Multiplication\nLet’s cover matrix multiplication in the same manner as above.\n\\[\nA\n=\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n,\nB\n=\n\\begin{bmatrix}\n5 & 6 \\\\\n7 & 8\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n5 & 6 \\\\\n7 & 8\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n(1 \\cdot 5) + (2 \\cdot 7) & (1 \\cdot 6) + (2 \\cdot 8) \\\\\n(3 \\cdot 5) + (4 \\cdot 7) & (3 \\cdot 6) + (4 \\cdot 8)\n\\end{bmatrix}\n=\nC\n\\]\n\n\n\n\n\n\n\nAn alternative way to look at it…\n\n\n\n\n\n\n\n\n\n\n\nDon’t dwell too much on this; it may help to refer back to this later to help understand the einsum notation below.\n\n\n\n\\[\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n5 & 6 \\\\\n7 & 8\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n(a_{00}b_{00} + a_{01}b_{10}) & (a_{00}b_{01} + a_{01}b_{11}) \\\\\n(a_{10}b_{00} + a_{11}b_{10}) & (a_{10}b_{01} + a_{11}b_{11})\n\\end{bmatrix}\n=\nC\n\\]\n\n\n\nMatrix multiplication simply involves taking the dot product of each row in the first matrix with each column in the second matrix.\nWe’ll need to use 3 iterators for this: one iterator \\(i\\) to loop through the rows of \\(A\\), another iterator \\(j\\) to loop through the columns of \\(B\\), and a third iterator \\(k\\) to loop through the elements in a row and column.\n\n\\(\\text{for row in } A\\)\n  \\(\\text{for col in } B\\)\n    \\(\\text{for ele in } A_{\\text{row}} \\text{ and } B_{\\text{col}}\\)\n       \\(a_{\\text{rowele}} \\cdot b_{\\text{elecol}} \\mathrel{+}= c_{\\text{rowcol}}\\)\n\n\n\\(\\text{for } i \\text{ in } A\\)\n  \\(\\text{for } j \\text{ in } B\\)\n     \\(\\text{for } k \\text{ in } A_{i} \\text{ and } B_{j}\\)\n       \\(a_{ik} \\cdot b_{kj} \\mathrel{+}= c_{ij}\\)\n\nLet’s focus in on the last line above.\n\\[\na_{ik} \\cdot b_{kj} \\mathrel{+}= c_{ij}\n\\]\nThis can more succinctly be written in einsum notation as \\(ik, kj \\rightarrow ij\\) — for each row \\(i\\) in \\(A\\), and for each column \\(j\\) in \\(B\\), iterate through each element \\(k\\), take their product, and sum the those products. The location of the output of the dot product in the output matrix \\(C\\) is \\(c_{ij}\\)."
  },
  {
    "objectID": "forblog/posts/16_einstein_summation_notation.html#various-examples",
    "href": "forblog/posts/16_einstein_summation_notation.html#various-examples",
    "title": "Intuitively Approaching Einstein Summation Notation",
    "section": "Various Examples",
    "text": "Various Examples\n\n1D Operations\n\\[\nA = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, B = \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix}\n\\]\n\nReturning a View of \\(A\\)\n\nPseudocode\nFor each row \\(i\\), output the row.\n\n\nEinsum Notation\n\\(i \\rightarrow i\\)\n\n\n\n\nSumming the Values of \\(A\\)\n\nPseudocode\nIterate through each row \\(i\\), and sum all rows.\n\n\nEinsum Notation\n\\(i \\rightarrow \\phantom{i}\\)\n\n\n\n\n\n\nA scalar is output, hence no output iterator.\n\n\n\n\n\n\n\nHadamard Product of \\(A\\) and \\(B\\)\n\nPseudocode\nFor each row \\(i\\) in \\(A\\) and \\(B\\), multiply them together.\n\n\nEinsum Notation\n\\(i, i \\rightarrow i\\)\n\n\n\n\nDot Product of \\(A\\) and \\(B\\)\n\nPseudocode\nFor each row \\(i\\) in \\(A\\) and \\(B\\), multiply them together, and sum the products.\n\n\nEinsum Notation\n\\(i, i \\rightarrow \\phantom{i}\\)\n\n\n\n\n\n\nA scalar is output, hence no output iterator.\n\n\n\n\n\n\n\nOuter Product of \\(A\\) and \\(B\\)\n\nPseudocode\nFor each row \\(i\\) in \\(A\\), multiply it with each row \\(j\\) in \\(B\\).\n\n\nEinsum Notation\n\\(i, j \\rightarrow ij\\)\n\n\n\n\n\n\nExpanded example\n\n\n\n\n\nThe outer product involves multiplying each element in \\(A\\) with all elements in \\(B\\).\n\n\\(\\text{for row in } A\\)\n  \\(\\text{for another-row in } B\\)\n     \\(a_{\\text{row}} \\cdot b_{\\text{another-row}} = c_{\\text{rowanother-row}}\\)\n\n\n\\(\\text{for } i \\text{ in } A\\)\n   \\(\\text{for } j \\text{ in } B\\)\n     \\(a_{i} \\cdot b_{j} = c_{ij}\\)\n\n\n\n\n\n\n\n\n2D Operations\n\\[\nA\n=\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix}\n,\nB\n=\n\\begin{bmatrix}\n9 & 8 & 7 \\\\\n6 & 5 & 4 \\\\\n3 & 2 & 1\n\\end{bmatrix}\n\\]\n\nReturn a View of \\(A\\)\n\nPseudocode\nFor each row \\(i\\), iterate through each column \\(j\\) and output it.\n\n\nEinsum Notation\n\\(ij \\rightarrow ij\\)\n\n\n\n\nTranspose \\(A\\)\n\nPseudocode\nFor each row \\(i\\), iterate through each column \\(j\\) and output it in \\(C\\) at row \\(j\\) and column \\(i\\).\n\n\nEinsum Notation\n\\(ij \\rightarrow ji\\)\n\n\n\n\nReturn the Main Diagonal of \\(A\\)\n\nPseudocode\nFor each row \\(i\\), iterate through each column \\(i\\) and output it.\n\n\nEinsum Notation\n\\(ii \\rightarrow i\\)\n\n\n\n\nObtain the Trace of \\(A\\)\n\nPseudocode\nFor each row \\(i\\), iterate through each column \\(i\\) and sum them.\n\n\nEinsum Notation\n\\(ii \\rightarrow \\phantom{i}\\)\n\n\n\n\n\n\nA scalar is output, hence no output iterator.\n\n\n\n\n\n\n\nSum the Rows of \\(A\\)\n\nPseudocode\nFor each row \\(i\\), iterate through each column \\(j\\) and sum them.\n\n\nEinsum Notation\n\\(ij \\rightarrow i\\)\n\n\n\n\nSum the Columns of \\(A\\)\n\nPseudocode\nFor each column \\(j\\), iterate through each row \\(i\\) and sum them.\n\n\nEinsum Notation\n\\(ij \\rightarrow j\\)\n\n\n\n\nHadamard Product of \\(A\\) and \\(B\\)\n\nPseudocode\nFor each row \\(i\\) in \\(A\\) and \\(B\\), iterate throuch each column \\(j\\), and take their product.\n\n\nEinsum Notation\n\\(ij, ij \\rightarrow ij\\)\n\n\n\n\nHadamard Product of \\(A\\) and \\(B\\) Transposed (\\(A \\odot B^{T}\\))\n\nPseudocode\nFor each row \\(i\\) in \\(A\\), and for each row \\(j\\) in \\(B\\), iterate through each column \\(j\\) in \\(A\\) and each column \\(i\\) in \\(B\\), and take their product.\n\n\nEinsum Notation\n\\(ij, ji \\rightarrow ij\\)\n\n\n\n\nMatrix Product of \\(A\\) and \\(B\\)\n\nPseudocode\nFor each row \\(i\\) in \\(A\\), and for each column \\(j\\) in \\(B\\), iterate through each element \\(k\\), take their product, and then sum those products.\n\n\nEinsum Notation\n\\(ik, kj \\rightarrow ij\\)\n\n\n\n\nEach Row of \\(A\\) Multiplied with \\(B\\)\n\nPseudocode\n\nFor each row \\(i\\) in \\(A\\), and for each row \\(j\\) in \\(B\\), iterate through each column \\(k\\) and take their product.\n\n\nEinsum Notation\n\\(ik, jk \\rightarrow ijk\\)\n\n\n\n\n\n\nA three dimensional tensor is output, hence the three output iterators.\n\n\n\n\n\n\n\nEvery Element of \\(A\\) Multiplied with \\(B\\)\n\nPseudocode\n\nFor each row \\(i\\) in \\(A\\), iterate through each column \\(j\\) and multiply it with each row \\(k\\) in \\(B\\) by iterating through each column \\(l\\) in that row \\(k\\).\n\n\nEinsum Notation\n\\(ij, kl \\rightarrow ijkl\\)\n\n\n\n\n\n\nA four dimensional tensor is output, hence the four output iterators."
  },
  {
    "objectID": "forblog/posts/16_einstein_summation_notation.html#conclusion",
    "href": "forblog/posts/16_einstein_summation_notation.html#conclusion",
    "title": "Intuitively Approaching Einstein Summation Notation",
    "section": "Conclusion",
    "text": "Conclusion\nAnd that’s that! The key is to think in terms of iterators that return locations in a matrix.\nIt may help to implement the operations above by yourself through pencil and paper, and in a programming languge too.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "bitsandbobs/posts/2025-05-11-A.html",
    "href": "bitsandbobs/posts/2025-05-11-A.html",
    "title": "春节期间怎么祝福朋友",
    "section": "",
    "text": "见到了朋友\n\n新年好\n春节好\n\n\n\n去某个长辈家里\n\n给您拜年了\n\n\n\n大年/正月初五时\n\n祝你财源广进\n\n\n\n去吃饭买东西\n\n对老板说，“祝你生意兴隆”\n\n\n\n如果不知道\n\n“祝你 X 年大吉”，X 就是年份\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "bitsandbobs/posts/2025-01-02-A.html",
    "href": "bitsandbobs/posts/2025-01-02-A.html",
    "title": "没几个",
    "section": "",
    "text": "没几个 is a way of saying a “very few” of something, rather than “not few”.\n我认识的人很多，但没几个好朋友。\nFrom the Immersive Chinese App.\n\n\n\n Back to top"
  },
  {
    "objectID": "bitsandbobs/posts/2025-07-15-A.html",
    "href": "bitsandbobs/posts/2025-07-15-A.html",
    "title": "Nuances in Shell Scripting",
    "section": "",
    "text": "Variables are denoted by $\nfoo = bar fails since spaces denote arguments\necho '$foo' outputs $foo\necho \"$foo\" outputs bar"
  },
  {
    "objectID": "bitsandbobs/posts/2025-07-15-A.html#variables",
    "href": "bitsandbobs/posts/2025-07-15-A.html#variables",
    "title": "Nuances in Shell Scripting",
    "section": "",
    "text": "Variables are denoted by $\nfoo = bar fails since spaces denote arguments\necho '$foo' outputs $foo\necho \"$foo\" outputs bar"
  },
  {
    "objectID": "bitsandbobs/posts/2025-07-15-A.html#reserved-variables",
    "href": "bitsandbobs/posts/2025-07-15-A.html#reserved-variables",
    "title": "Nuances in Shell Scripting",
    "section": "Reserved Variables",
    "text": "Reserved Variables\n\nReserve variables include, but are not limited to:\n\n$0: name of the script\n$1 to $9: arguments passed to the script\n\n$#: number of arguments passed to the script\n$@: all arguments passed to the script\n$?: exit status of the last command\n$_: last argument of the previous command; can also be accessed by Esc + . or alt + .\n$$: process ID (pid) of the current script\n!!: rerun the entire last command"
  },
  {
    "objectID": "bitsandbobs/posts/2025-07-15-A.html#command-and-process-substitutions",
    "href": "bitsandbobs/posts/2025-07-15-A.html#command-and-process-substitutions",
    "title": "Nuances in Shell Scripting",
    "section": "Command and Process Substitutions",
    "text": "Command and Process Substitutions\n\nCommand/process substitutions are like f-strings in Python\nDifference between command and process substitution is that the latter will place the output in a temporary file\nCommand substitution: $(command)\nProcess substitution: &lt;(command)"
  },
  {
    "objectID": "bitsandbobs/posts/2025-07-15-A.html#comparisons-and-brace-expansions",
    "href": "bitsandbobs/posts/2025-07-15-A.html#comparisons-and-brace-expansions",
    "title": "Nuances in Shell Scripting",
    "section": "Comparisons and Brace Expansions",
    "text": "Comparisons and Brace Expansions\n\nWhen doing comparisons in bash, use [[ ]] rather than [ ]; apparently, chances of mistakes are lower this way\nUse {} for common substrings in commands\n\nconvert image.{png,jpg}\ncp /path/to/project/{foo,bar,baz}.sh /newpath\nmv *{.py,.sh} folder\ntouch {foo,bar}/{a..h}"
  },
  {
    "objectID": "bitsandbobs/posts/2025-07-15-A.html#functions-vs-scripts",
    "href": "bitsandbobs/posts/2025-07-15-A.html#functions-vs-scripts",
    "title": "Nuances in Shell Scripting",
    "section": "Functions vs Scripts",
    "text": "Functions vs Scripts\n\nFunctions are executed in the current environment, whereas scripts aren’t\nThus, functions can modify environment variables whereas scripts can’t\nScripts can only access environment variables that are exported with export"
  },
  {
    "objectID": "bitsandbobs/posts/2025-02-13-A.html",
    "href": "bitsandbobs/posts/2025-02-13-A.html",
    "title": "Deepseek R1 in 2 bullets",
    "section": "",
    "text": "Am currently reading through the research paper. From my current understanding:\n\nR1-Zero is pure RL, with GRPO as the policy\nR1 is unpure RL, with GRPO as the policy, with some SFT in the form of cold start data, and further refinement stages\n\n\n\n\n Back to top"
  },
  {
    "objectID": "bitsandbobs/posts/2025-01-27-A.html",
    "href": "bitsandbobs/posts/2025-01-27-A.html",
    "title": "不 vs 没",
    "section": "",
    "text": "In a nutshell.\n不: - Negates present and future - Negates habitual actions and adjectives\n没: - Negates past - Negates something that hasn’t happened\n\n\n\n Back to top"
  },
  {
    "objectID": "bitsandbobs/posts/2025-07-05-A.html",
    "href": "bitsandbobs/posts/2025-07-05-A.html",
    "title": "Shell hooks, uv, and Conda.",
    "section": "",
    "text": "A shell hook is a piece of shell script that is called before or after the shell displays the prompt.\nI want to use conda and uv together. Conda’s great as swithcing environments on the fly, and uv is great for speed. However, while uv pip install makes use of the current conda environment, uv sync and uv add don’t\nI came across this rc script from Benjamin Warner that sets the environment variable uv uses to allow it to work with conda.\n# Function to update UV_PROJECT_ENVIRONMENT when Conda environment changes\nconda_auto_env() {\n    if [[ -n \"$CONDA_PREFIX\" ]]; then\n        export UV_PROJECT_ENVIRONMENT=\"$CONDA_PREFIX\"\n    else\n        unset UV_PROJECT_ENVIRONMENT\n    fi\n}\n\n# Run the function initially to set the variable immediately\nconda_auto_env\n\n# Hook into shell prompts\nif [[ -n \"$ZSH_VERSION\" ]]; then\n    # Ensure add-zsh-hook is available\n    autoload -Uz add-zsh-hook\n    # Avoid duplicate hooks\n    if ! [[ -v _conda_update_hook_added ]]; then\n        add-zsh-hook precmd conda_auto_env\n        _conda_update_hook_added=1\n    fi\nelif [[ -n \"$BASH_VERSION\" ]]; then\n    # Ensure PROMPT_COMMAND is updated only once\n    if [[ -z \"$PROMPT_COMMAND\" ]]; then\n        PROMPT_COMMAND=\"conda_auto_env\"\n    elif [[ \"$PROMPT_COMMAND\" != *\"conda_auto_env\"* ]]; then\n        PROMPT_COMMAND=\"conda_auto_env; $PROMPT_COMMAND\"\n    fi\nfi\nProgramming the shell is new to me, and I wanted to understand what exactly this script was doing. So after chatting with a LLM, I discovered the world of shell hooks.\nThe script begins by first settings the UV_PROJECT_ENVIRONMENT to be the path that $CONDA_PREFIX uses.\nconda_auto_env() {\n    if [[ -n \"$CONDA_PREFIX\" ]]; then\n        export UV_PROJECT_ENVIRONMENT=\"$CONDA_PREFIX\"\n    else\n        unset UV_PROJECT_ENVIRONMENT\n    fi\n}\nThen, a precmd shell hook is created. So the moment before a new prompt appears in the shell, conda_auto_env is run to set the uv environment to be the same as the conda environment.\n# Ensure add-zsh-hook is available\nautoload -Uz add-zsh-hook\n# Avoid duplicate hooks\nif ! [[ -v _conda_update_hook_added ]]; then\n    add-zsh-hook precmd conda_auto_env\n    _conda_update_hook_added=1\nfi\n\n\n\n Back to top"
  },
  {
    "objectID": "bitsandbobs/posts/2025-05-01-A.html",
    "href": "bitsandbobs/posts/2025-05-01-A.html",
    "title": "茶好客常来",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "bitsandbobs/posts/2025-06-21-A.html",
    "href": "bitsandbobs/posts/2025-06-21-A.html",
    "title": "Terminal know-how.",
    "section": "",
    "text": "$PATH outputs the paths of all programs that can be executed directly by the name of the program. That is, it’s not needed to type the fully path of the program every time.\nwhich lets you know which path is in use.\nShift, rename, copy, and delete files with mv, cp, rv, and rm.\nStuck? Run man with the program name.\nPass data between files and programs with &gt; and &lt;.\nPass data between programs with |.\nRemember cd - and cd ~.\nUse chmod to change file permissions.\nUse touch to create files, and change certain file metadata.\nUse cat to view file contents, and concatenate files.\n!# (shebang) calls for bash scripts to be interpreted.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "bitsandbobs/posts/2025-06-03-A.html",
    "href": "bitsandbobs/posts/2025-06-03-A.html",
    "title": "Cognitive Maps",
    "section": "",
    "text": "In biology, cognitive maps are what provide us with spatial knowledge and perception. These maps are comprised of 4 cells that in conjuction provide us with a sense of spatial perception: place cells, grid cells, direction cells, and boundary cells. Place cells let us know when objects in a given location. Grid cells provide a coordinate-like system for mapping the environment.\nIn the field of AI, one of the most common implementations of cognitive maps is akin to an embedding specialized for space. Each row is an object and each column is a location in a given space. Thus, a corresponding cell tells us whether a given object is at a given location.\n\n\n\n Back to top"
  },
  {
    "objectID": "bitsandbobs/index.html",
    "href": "bitsandbobs/index.html",
    "title": "These are bits and bobs.",
    "section": "",
    "text": "Bits and bobs I learned along the way. Maybe you'll find them useful too.\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDescription\n\n\n\nDate\n\n\n\n\n\n\n\n\nNifty Shell Tools\n\n\nPocket tools to keep in your pocket.\n\n\n15 July 2025\n\n\n\n\n\n\nNuances in Shell Scripting\n\n\nShell’s just another programming language.\n\n\n15 July 2025\n\n\n\n\n\n\nShell hooks, uv, and Conda.\n\n\nHooking uv and conda together.\n\n\n05 July 2025\n\n\n\n\n\n\nRetrieval Augmented Generation in a nutshell.\n\n\nDon’t let jargon fool you.\n\n\n24 June 2025\n\n\n\n\n\n\nTerminal know-how.\n\n\nKnow the basics of manipulating your shell.\n\n\n20 June 2025\n\n\n\n\n\n\nCognitive Maps\n\n\nCognitive maps in the context of biology and AI.\n\n\n03 June 2025\n\n\n\n\n\n\n春节期间怎么祝福朋友\n\n\nHow to greet others during the Spring Festival.\n\n\n11 May 2025\n\n\n\n\n\n\n茶好客常来\n\n\n只要东西好，顾客就会喜欢。\n\n\n01 May 2025\n\n\n\n\n\n\n饭后百步走，活到九十九\n\n\nA walk after dinner makes one live to 99.\n\n\n28 April 2025\n\n\n\n\n\n\n鱼与熊掌\n\n\nYou can’t have your cake and eat it too.\n\n\n03 April 2025\n\n\n\n\n\n\nSeeking Deep Thoughts\n\n\nThoughts that came to mind as I’m reading the Deepseek Maths paper\n\n\n18 February 2025\n\n\n\n\n\n\nDeepseek R1 in 2 bullets\n\n\nDeepseek R1 explained in a literal walnut shell\n\n\n13 February 2025\n\n\n\n\n\n\nRobo-ABC: Affordance Generalization Beyond Categories via Semantic Correspondence for Robot Manipulation\n\n\nTransfering the perception of an object, to other objects.\n\n\n28 January 2025\n\n\n\n\n\n\n不 vs 没\n\n\nThe age old connundrum.\n\n\n27 January 2025\n\n\n\n\n\n\nThinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces\n\n\nA new dimension to current models is needed to understand space.\n\n\n23 January 2025\n\n\n\n\n\n\nOmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints\n\n\nUsing an ensemble to overcome the lack of 3D understanding\n\n\n21 January 2025\n\n\n\n\n\n\n小和年轻\n\n\n小 and 年轻 carry different contations for age.\n\n\n14 January 2025\n\n\n\n\n\n\n没几个\n\n\nThe function of 没几个\n\n\n02 January 2025\n\n\n\n\n\n\nNo matching items\n\n  \n\n\n\nLoading…\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Salman Naqvi",
    "section": "",
    "text": "An explorer who’s lived in 4 countries and traveled to 15. A master of 3 different language scripts. An enthusiast of science and technology, proudly wearing the badge of “sci-tech geek”. A curious soul unraveling the realms of AI, intricacies of 3D computer graphics, fueled by an insatiable passion for learning. Onward and upward, with tenacity and sincerity.\n\n\n\nProjects // 有趣项目Experience // 经验Skills // 技能扫一扫\n\n\n\nTrained an LLM in 1.5 months that could output Cantonese lyrics that were constrained by the desired 6-9 Cantonese tones.\nReimplemented the DiffEdit research paper. Edit portions of an image via text only.\nReimplementing the PyTorch library myself.\nMaking back propagation an inherently easy topic to grasp.\nOptimizing algorithms to run quicker on a GPU.\n\n\n\nEmbodied AI Researcher | Oct. 2024 – Current\nMultimodal Intelligence Laboratory, The Education University of Hong Kong\n\nEmbodied AI\nGeneral purpose robot manipulation and spatial intelligence\nUnder Dr. Fu Hong\n\nAI Engineer | Jul. 2024 – Aug.2024\nKeweya Education Technology Limited – HKSTP Incubatee\n\nProject lead\nStudying, implementing, customizing diffusion models, and app development\nReview and analysis of research papers\n\nLLM Researcher | Jun. 2024 – Jul. 2024\nREADily Limited – HKSTP Incubatee\n\nProject co-lead\nIntensive continued pretraining of large language models\nResearch paper review and implementation\n\nTraining Coordinator | Mar. 2024\nKids4Kids Hong Kong\n\nVolunteer work\nWorkshop planning and video editing\n\nTeaching Assistant | Sep. 2020 – May. 2021\nBeaconhouse College Program\n\nTeaching classes\n1-1 student sessions\n\n\n\nLanguage\n\nEnglish: Native level fluency\nMandarin: Basic proficiency\nUrdu: Fluent\n\nTeaching\nI know how to succinctly convey concepts in an engaging manner that makes sense. I don’t read a speech from PPT slides.\nProgramming\nI have a very particular manner and philosophy to programming. It’s expository programming: literate, exploratory, and the way it should be done. See here, here, and here. Writing code is not the focus, solving the problem at hand is.\nPython, PyTorch, Hugging Face (transformers, diffusers, datasets, tokenizers, peft, accelerate, etc), fastai, Pandas, NumPy, Scikit-Learn, Gradio, Quarto, Jupyter, nbdev\n3D Computer Graphics\nBlender\nArtificial Intelligence (fast.ai)\nSelf-learning since April 2022.\n\n\n\n\n\n\n\nOther Interesting Stuff // 其他亮点Education // 教育\n\n\nFeb. 21st 2025 – Meet Salman Naqvi: the bright spark in AI and EdTech | See what the university’s written about me!\nDec. 27, 2024 – Featured among “The EdUHK Rising Stars” | See the university’s mini documentary on me!\nJun. 12, 2024 – Study Award from Hong Kong SAR Government | Hong Kong SAR Government Scholarship Fund Reaching Out Award\nDec. 7, 2023 — Invited Speaker at the Advanced Study Institute (ASI) Online Symposium | The Education University of Hong Kong and Hong Kong Baptist University\nApr. 2023 – Nov. 2023 — Co-lead Online Study Group | fast.ai Course Part 2: From Deep Learning Foundations to Stable Diffusion | Discord\nMar. 17, 2023 — Study Scholarship Offer from Kyoto University of Advanced Sciences, Kyoto, Japan | KUAS-E Scholarship\nFeb. 24, 2023 — Study Scholarship Offer from The Eduation University of Hong Kong, Hong Kong SAR, China | EdUHK Full Entrance Scholarship\nNov. 2022 – Jun. 2023 – Grew an AI community of over 1100+ members | The Dataspace | Discord\nSep. 2022 – Mar. 2023 — Completed fast.ai Course Part 1: Practical Deep Learning for Coders\nJan. 17, 2023 — Placed in Top 12% of Contestants in Data Science Competition | Playground Series Season 3, EP2 | Tabular Classification with a Stroke Prediction Dataset | Kaggle\nSep. 2020 – May 2021 — Physics Teaching Assistant | Beaconhouse College Program Potohar Campus, Islamabad, Pakistan\nSep. 2020 – May 2021 — Mathematics Teaching Assistant | Beaconhouse College Program Potohar Campus, Islamabad, Pakistan\nJan. 2020 — Concert Marketing – Raised $8,400 | Beaconhouse College Program Potohar Campus, Islamabad, Pakistan\nAug. 2019 — Received 100% Scholarship for A-Level Program | Beaconhouse College Program Potohar Campus, Islamabad, Pakistan\n2018 — Bronze Medal – Honorable Distinction Award | 15th International Kangaroo Linguistic Contest, Islamabad, Pakistan\n\n\nCurrently studying BSc (Honors) in Artificial Intelligence and Educational Technology at The Education University of Hong Kong.\nIELTS – 8.5/9.0\nMiddle School and High School – CAIE IGCSE\nPrimary and Middle School – IB Primary and Middle Years Program\n\n\n\n\n \n\n\n Back to top"
  },
  {
    "objectID": "unsubscribe.html",
    "href": "unsubscribe.html",
    "title": "Unsubscribe from ForBlog and App Playground Notifications",
    "section": "",
    "text": "Loading…\n\n\n\n\n Back to top"
  },
  {
    "objectID": "bitsandbobs/posts/2025-01-28-A.html",
    "href": "bitsandbobs/posts/2025-01-28-A.html",
    "title": "Robo-ABC: Affordance Generalization Beyond Categories via Semantic Correspondence for Robot Manipulation",
    "section": "",
    "text": "DenseMatcher is a method for generalization the “understanding” of one object to another. The best way to understand this, is to view the following images from the paper.\n\n\nIn other words, DenseMatcher computes 3D correspondences between objects.\nIt works by using SD-DINO (a combination of Stable Diffusion and DINO) to extract 2D features from different angles of the 3D asset. The features from different views are averaged, providing the feature for each vertex.\n\nHowever, as seen in the image above, the features are noisy. Therefore, the features are then refined with DiffusionNet. This is an architecture meant for meshes.\n\nAfter the features have been refined, a functional map is solved to compute correspondences.\n\nLink to paper. All images in this post are from the paper.\n\n\n\n Back to top"
  },
  {
    "objectID": "bitsandbobs/posts/2025-01-21-A.html",
    "href": "bitsandbobs/posts/2025-01-21-A.html",
    "title": "OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints",
    "section": "",
    "text": "OmniManip is a hot new paper from Agibot and Peking University. It’s about zero shot natural language robotic manipulation tasks. A current issue with this task, and current approaches the utilize VLMs, is that VLMs lack 3D spatial understanding. They’re only trained on 2D images and video after all.\nOmniManip utilizes an ensemble of models to achieve this goal. This is how it works at a high level.\nA segmentation model is used to extract relevant objects from the robot’s vision. A VLM then filters out task relevant objects, and also breaks down the input task (input as text) into multiple stages.\nNext, for each stage of the task, interaction primitives and their spatial constraints are extracted. A single view 3D generation model is used to generate meshes for all objects relevant to the task. A pose estimation model is then used to canonicalize the objects. Interaction primitives and their corresponding constraints relevant for the task are then identified by the VLM. Along the way, a LLM is used to grade various primitives and constriants.\nModels hallucinate (in particular, the VLM). The world is not static. To overcome this, a closed loop system is introduced; a self correction mechanism based on resampling, rendering and checking. This method uses realtime feedback form the VLM, which is used to detect and then correct interaction errors. A pose tracking algorithm is also used to continuously update the poses of all relevant objects, allowing the robot to be dynamically adjusted.\nLink to paper.\n\n\n\n Back to top"
  },
  {
    "objectID": "bitsandbobs/posts/2025-02-18-A.html",
    "href": "bitsandbobs/posts/2025-02-18-A.html",
    "title": "Seeking Deep Thoughts",
    "section": "",
    "text": "I started reading the Deepseek Math paper, after recently finishing the R1 paper. The following thoughts started coming to mind:\n\nR1 is successful simply because of the training process\nDeepseek Math is worked really well because of data quality\nGRPO is used only to reduce memory usage (still reading the Deepseek Math paper; I could be wrong)\n\nTakeaway is that a lot can be done by simply flipping and rearranging all the existing levers and switches we already have.\n\n\n\n Back to top"
  },
  {
    "objectID": "bitsandbobs/posts/2025-01-24-A.html",
    "href": "bitsandbobs/posts/2025-01-24-A.html",
    "title": "Retrieval Augmented Generation in a nutshell.",
    "section": "",
    "text": "RAG is simply a bunch of cosine similarity checks performed with the user’s query and the contexts provided to the LLM. This allows the LLM to determine which contexts are most useful for the LLM to answer the user’s query.\n\n\n\n Back to top"
  },
  {
    "objectID": "bitsandbobs/posts/2025-04-03-A.html",
    "href": "bitsandbobs/posts/2025-04-03-A.html",
    "title": "鱼与熊掌",
    "section": "",
    "text": "Full version: 鱼与熊掌不可兼得.\nIn other words, you can’t have the fish and the bear’s paw too.\n\n\n\n Back to top"
  },
  {
    "objectID": "bitsandbobs/posts/2025-04-28-A.html",
    "href": "bitsandbobs/posts/2025-04-28-A.html",
    "title": "饭后百步走，活到九十九",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "bitsandbobs/posts/2025-01-23-A.html",
    "href": "bitsandbobs/posts/2025-01-23-A.html",
    "title": "Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces",
    "section": "",
    "text": "This paper tackles the issue that is the lack of spatial understanding and spatial reasoning that MLLM/VLMs.\nIn this paper, the authors created a benchmark for spatial reasoning, consisting of over 5000 QnA pairs from 288 indoor videos. A variety of tasks are covered, including configurational (e.g,. object count, route planning), measurement estimation (e.g., room size), and spatiotempral reasoning (e.g., appearance order). 79% accuracy on the benchmark is needed to reach human level awareness. The authors found that their MLLM could only reach 49%.\nThe resulting analysis from the paper shows that 71% of errors stem from spatial reasoning, rather than perception or language understanding. By generating spatial layout (cognitive maps), accuracy on distance related tasks improved by 10%. This outperforms linguistic prompting techniques such as CoT. In fact, CoT degraded performance on the benchmark.\nIt was also found that MLLMs have strong local spatial awareness, but struggle with global awareness.\nWeaknesses in the study include reliance on 3D datasets that may contain annotation errors, video processing and langauge model evaluation being resource intensive, and the study focusing on indoor scenes and not outdoor scenes.\nFuture directions encourage research into hybrid models that are both language and spatial memory, self-supervised objectives for spatial learning, and fine-tuning for VSI tasks.\nLink to paper.\n\n\n\n\n\n\nDeepseek R1 Summary\n\n\n\n\n\nStructured Summary of “Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces”\n\nContextual Overview\n\nResearch Question: Can Multimodal Large Language Models (MLLMs) exhibit human-like visual-spatial intelligence (VSI) when reasoning about 3D spaces from videos?\n\nObjective: Develop a benchmark (VSI-Bench) to evaluate MLLMs’ ability to perceive, remember, and reason about spatial relationships in real-world indoor environments.\n\nDomain: Multimodal AI, spatial reasoning, video understanding.\n\nMotivation: Visual-spatial intelligence is critical for robotics, AR/VR, and autonomous systems, but existing MLLM benchmarks focus on 2D image/text understanding. This work addresses the gap in evaluating 3D spatial reasoning from video input.\n\n\n\n\nKey Contributions\n\nVSI-Bench: A novel benchmark with 5,000+ QA pairs derived from 288 real indoor videos (from ScanNet, ScanNet++, ARKitScenes). Tasks include configurational (e.g., object count, route planning), measurement estimation (e.g., room size), and spatiotemporal reasoning (e.g., appearance order).\n\nSpatial Reasoning as Bottleneck: Analysis shows 71% of errors stem from spatial reasoning (e.g., egocentric-allocentric transformations), not perception or language understanding.\n\nCognitive Maps Enhance Performance: Explicitly generating spatial layouts (cognitive maps) improves MLLMs’ accuracy on relative distance tasks by 10%, outperforming linguistic prompting techniques like Chain-of-Thought.\n\nFailure of Linguistic Reasoning: Prevailing methods (CoT, self-consistency, Tree-of-Thoughts) degraded performance on VSI-Bench, highlighting the distinct challenges of spatial reasoning.\n\nLocal vs. Global Spatial Models: MLLMs build strong local spatial awareness (64% accuracy for adjacent objects) but struggle with global consistency.\n\n\n\n\nMethodology Deep Dive\n\nData/Resources:\n\nVideos from ScanNet (24 FPS), ScanNet++, ARKitScenes (30 FPS), standardized to 640×480 resolution.\n\nQA pairs auto-generated using templates and human annotation (for route planning).\n\n\nCore Techniques:\n\nUnified Meta-Information: Structured annotations for object counts, bounding boxes, room size, and spatial relationships.\n\nEvaluation Metrics: Accuracy (for multiple-choice), Mean Relative Accuracy (for numerical answers).\n\nCognitive Map Generation: Prompting MLLMs to output object positions on a 10×10 grid.\n\n\nValidation:\n\n15 MLLMs tested, including GPT-4o, Gemini-1.5 Pro, and open-source models (LLaVA variants).\n\nHuman baseline: 79% accuracy vs. Gemini-1.5 Pro (45.4%).\n\nBlind evaluation confirmed video input is critical (performance drops to chance level without it).\n\n\n\n\n\nStrengths and Weaknesses\n\nStrengths:\n\nComprehensive Benchmark: High-quality, diverse tasks with iterative human review.\n\nNovel Insights: Identifies spatial reasoning as the key bottleneck and demonstrates cognitive maps’ utility.\n\nReproducibility: Public code, metrics, and standardized evaluation protocols.\n\n\nWeaknesses:\n\nDataset Bias: Relies on existing 3D datasets, which may inherit annotation errors.\n\nCompute Demands: Video processing and large model evaluation are resource-intensive.\n\nLimited Generalization: Focus on indoor scenes; outdoor/embodied settings unexplored.\n\n\n\n\n\nRelevance to the Field\n\nAdvances: Establishes a foundation for evaluating and improving MLLMs’ spatial reasoning, crucial for embodied AI (e.g., robots, autonomous navigation).\n\nContrast with Prior Work: Unlike image-based or text-only spatial benchmarks, VSI-Bench uses video to mirror real-world observation.\n\nFuture Directions: Encourages research into hybrid models (language + spatial memory), self-supervised objectives for spatial learning, and fine-tuning for VSI tasks.\n\n\n\n\nKey Takeaways\n\nMLLMs lag behind humans in visual-spatial intelligence (45% vs. 79% accuracy) but show emerging capabilities.\n\nSpatial reasoning—not perception or language—is the primary challenge for MLLMs.\n\nCognitive maps improve spatial reasoning, suggesting a path toward better world modeling.\n\nLinguistic prompting (CoT, etc.) fails for spatial tasks, demanding new techniques tailored to visuospatial reasoning.\n\nLocal spatial awareness exists in MLLMs, but global consistency remains elusive.\n\n\nNon-Technical Insights for Decision-Makers:\n- VSI-Bench is a critical tool for developers aiming to build MLLMs for real-world navigation or AR/VR.\n- Spatial reasoning enhancements (e.g., cognitive maps) could bridge the gap between MLLMs and human-like spatial understanding.\n- Open-source models (e.g., LLaVA-NeXT-Video-72B) are competitive with closed-source counterparts, offering cost-effective alternatives.\nLet me know if you’d like to dive deeper into specific sections!\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "bitsandbobs/posts/2025-07-15-B.html",
    "href": "bitsandbobs/posts/2025-07-15-B.html",
    "title": "Nifty Shell Tools",
    "section": "",
    "text": "find to find files and directories, and perform actions on them\nfd, an alternative to find that provides abstractions"
  },
  {
    "objectID": "bitsandbobs/posts/2025-07-15-B.html#file-searching-tools",
    "href": "bitsandbobs/posts/2025-07-15-B.html#file-searching-tools",
    "title": "Nifty Shell Tools",
    "section": "",
    "text": "find to find files and directories, and perform actions on them\nfd, an alternative to find that provides abstractions"
  },
  {
    "objectID": "bitsandbobs/posts/2025-07-15-B.html#path-and-command-lookup",
    "href": "bitsandbobs/posts/2025-07-15-B.html#path-and-command-lookup",
    "title": "Nifty Shell Tools",
    "section": "Path and Command Lookup",
    "text": "Path and Command Lookup\n\nwhich to know which path is in use"
  },
  {
    "objectID": "bitsandbobs/posts/2025-07-15-B.html#text-searching",
    "href": "bitsandbobs/posts/2025-07-15-B.html#text-searching",
    "title": "Nifty Shell Tools",
    "section": "Text Searching",
    "text": "Text Searching\n\ngrep to match patterns in input text\nripgrep, an alternative to grep that filters generally unwanted files by default"
  },
  {
    "objectID": "bitsandbobs/posts/2025-07-15-B.html#documentation-tools",
    "href": "bitsandbobs/posts/2025-07-15-B.html#documentation-tools",
    "title": "Nifty Shell Tools",
    "section": "Documentation Tools",
    "text": "Documentation Tools\n\nman to view the manual of a tool\ntldr, an alternative to man that provides concise and practical summaries"
  },
  {
    "objectID": "bitsandbobs/posts/2025-07-15-B.html#file-operations",
    "href": "bitsandbobs/posts/2025-07-15-B.html#file-operations",
    "title": "Nifty Shell Tools",
    "section": "File Operations",
    "text": "File Operations\n\nchmod to change file permissions\ntouch to create files, and change certain file metadata\ncat to view file contents, and concatenate files"
  },
  {
    "objectID": "bitsandbobs/posts/2025-07-15-B.html#shell-history",
    "href": "bitsandbobs/posts/2025-07-15-B.html#shell-history",
    "title": "Nifty Shell Tools",
    "section": "Shell History",
    "text": "Shell History\n\nhistory to view your shell history; alternatively, most shells will allow the same functionality with ctrl + R"
  },
  {
    "objectID": "bitsandbobs/posts/2025-07-15-B.html#directory-visualization",
    "href": "bitsandbobs/posts/2025-07-15-B.html#directory-visualization",
    "title": "Nifty Shell Tools",
    "section": "Directory Visualization",
    "text": "Directory Visualization\n\ntree to visualize directories as a tree\nbroot to visualize and navigate directories in a tree-like structure"
  },
  {
    "objectID": "bitsandbobs/posts/2025-07-15-B.html#file-managers",
    "href": "bitsandbobs/posts/2025-07-15-B.html#file-managers",
    "title": "Nifty Shell Tools",
    "section": "File Managers",
    "text": "File Managers\n\nnnn and ranger as file managers"
  },
  {
    "objectID": "bitsandbobs/posts/2025-07-15-B.html#shell-enhancements",
    "href": "bitsandbobs/posts/2025-07-15-B.html#shell-enhancements",
    "title": "Nifty Shell Tools",
    "section": "Shell Enhancements",
    "text": "Shell Enhancements\n\nfzf, an add-on to search based actions that introduces fuzzy matching\nzsh-autosuggestions, an add-on to zsh that introduces autosuggestions"
  },
  {
    "objectID": "bitsandbobs/posts/2025-01-14-A.html",
    "href": "bitsandbobs/posts/2025-01-14-A.html",
    "title": "小和年轻",
    "section": "",
    "text": "小 can indicate whether one is young or old. For example, 她有一个小孩儿. However, you have to be careful when using 年轻, as it carries the connotation of being the opposite of old. If you’re saying you’re younger than someone, and you use 年轻, it also implies that the other person is old. 小 does not carry this connotation.\nFrom the Chinese Grammar Wiki (including the example sentence)..\n\n\n\n Back to top"
  },
  {
    "objectID": "forblog/index.html",
    "href": "forblog/index.html",
    "title": "Welcome to ForBlog by ForBo7",
    "section": "",
    "text": "Here you can experience my various ventures into learning, as well as read about other various useful tid-bits.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPulling Back the Curtain on VLM Attention\n\n\nUnderstanding LLM Attention and Vision Encoder Attention\n\n\nA LLM narrates our conversation.\n\n\n\n\n\n04 August 2025\n\n\nSalman Naqvi\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nGetting General Purpose Robots by Decomposing Problems and Working with Data\n\n\nroboOS®, powered by roboBrain™\n\n\nWhy these Robo series of papers are cold drink in a desert of hobbled pipelines\n\n\n\n\n\n24 July 2025\n\n\nSalman Naqvi\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nCurrent Ideas in Spatial Understanding\n\n\nWhat is good, what is not?\n\n\nA collated set of current ideas.\n\n\n\n\n\n14 July 2025\n\n\nSalman Naqvi\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nThe State of Pose Estimation\n\n\nWhat pose is pose estimation in?\n\n\nMy thoughts on pose estimation, and the wider field of general purpose robotics, as a newbie to this field.\n\n\n\n\n\n10 July 2025\n\n\nSalman Naqvi\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n中文里的一些基本数学术语\n\n\n\n\n\nSome basic mathematical expressions in Chinese.\n\n\n\n\n\n08 June 2025\n\n\nSalman Naqvi\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nHow Deepseek R1 was trained.\n\n\nCapturing the Depth\n\n\nThe training of R1 in a nutshell.\n\n\n\n\n\n25 February 2025\n\n\nSalman Naqvi\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nWhat I Learned during my Second and Third Internships\n\n\nYou Learn by Doing\n\n\nBrief summaries of various things I learned.\n\n\n\n\n\n24 September 2024\n\n\nSalman Naqvi\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nImplementing a Neural Network from Scratch\n\n\nThe DIY Guide to Digital Brain Building\n\n\nFirst I implement from scratch. Then I progressively reimplement with PyTorch. It is simpler than you think.\n\n\n\n\n\n26 May 2024\n\n\nSalman Naqvi\n\n18 min\n\n\n\n\n\n\n\n\n\n\n\nA Brief Token on Tokenizers\n\n\nThis limited time DLC post costs only 499 tokens to unlock.\n\n\nTokenizers in a high-level nutshell.\n\n\n\n\n\n24 January 2024\n\n\nSalman Naqvi\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nBackpropagation Explained using English Words*\n\n\nPropagating you all way the from the back.\n\n\n*Most words are in English.\n\n\n\n\n\n07 August 2023\n\n\nSalman Naqvi\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\nImplementing and Optimizing Meanshift Clustering\n\n\nThis post includes a cool animation.\n\n\nA guide to grouping data together — fast.\n\n\n\n\n\n21 June 2023\n\n\nSalman Naqvi\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\nIntuitively Approaching Einstein Summation Notation\n\n\nPutting it in an einsum-ple manner.\n\n\nAn alternative way to write matrix operations.\n\n\n\n\n\n06 June 2023\n\n\nSalman Naqvi\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n(Un)successfully Implementing DiffEdit\n\n\nThe (Un)expected Difficulties of Editing\n\n\nAn attempt at implementing the DiffEdit paper.\n\n\n\n\n\n29 May 2023\n\n\nSalman Naqvi\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nIterators and Generators\n\n\nIteratively Generating Iterative Generators\n\n\nIterators and generators shown by example.\n\n\n\n\n\n03 May 2023\n\n\nSalman Naqvi\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nImplementing Stable Diffusion From Its Components\n\n\nCreating a Diffuser that Diffuses Stable-y\n\n\nImplementing stable diffusion from the 🤗 🧨 library.\n\n\n\n\n\n28 April 2023\n\n\nSalman Naqvi\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\nMy Musings Through Stable Diffusion\n\n\nDiffusing the Musings\n\n\nExploring the various knobs and dials of stable diffusion.\n\n\n\n\n\n13 April 2023\n\n\nSalman Naqvi\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nStable Diffusion, Summarized\n\n\nTaking a Look at how Diffusers Dream\n\n\nA concise, high level overview on the mechanisms of stable diffusion.\n\n\n\n\n\n13 April 2023\n\n\nSalman Naqvi\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nHow to Convert Audio to Spectrogram Images\n\n\nVisualizing Sound\n\n\nA no nonsense guide to creating spectrograms from audio with PyTorch torchaudio.\n\n\n\n\n\n05 April 2023\n\n\nSalman Naqvi\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nTransformers, Simply Explained\n\n\nAutobots or Decepticons?\n\n\nAll transformers really do is fill in the blanks and autocomplete.\n\n\n\n\n\n28 February 2023\n\n\nSalman Naqvi\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nA No Nonsense Guide on how to use an M-Series Mac GPU with PyTorch\n\n\nM-Series Macs is better than saying M1/M2 Macs\n\n\nSqueezing out that extra performance.\n\n\n\n\n\n26 January 2023\n\n\nSalman Naqvi\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nAdding Subscriptions to a Quarto Site\n\n\nSubscribable Subscriptions\n\n\nA no nonsense, to the point guide to implementing subscriptions in your Quarto site.\n\n\n\n\n\n23 December 2022\n\n\nIsaac Flath, Salman Naqvi\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\nAI in a Nutshell\n\n\nThis nutshell contains very little math!\n\n\nAI models are much, much simpler than you think.\n\n\n\n\n\n04 October 2022\n\n\nSalman Naqvi\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\nDetecting Floods for Disaster Relief\n\n\nHow good are you at detecting floods?\n\n\nA rundown of the creation of my flood classifier.\n\n\n\n\n\n12 September 2022\n\n\nSalman Naqvi\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nData Quality is Important | Car Classifier\n\n\nClassy Cars\n\n\nMost of the time, data matters more than the model.\n\n\n\n\n\n04 June 2022\n\n\nSalman Naqvi\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nA No Nonsense Guide to Reading a Confusion Matrix\n\n\nAre you confused yet?\n\n\nA straight to the point guide about reading a confusion matrix.\n\n\n\n\n\n03 June 2022\n\n\nSalman Naqvi\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nMy first AI model\n\n\nCan you bare reading through this bear classifier?\n\n\nA rundown on my first attempt at creating model.\n\n\n\n\n\n28 May 2022\n\n\nSalman Naqvi\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nHow to Approach Creating AI Models\n\n\nPutting the Drive into the Train\n\n\nThere’s more to AI than just creating models.\n\n\n\n\n\n27 May 2022\n\n\nSalman Naqvi\n\n5 min\n\n\n\n\nNo matching items\n\n  \n\n\n\nLoading…\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "forblog/posts/6_ai_in_a_nutshell.html",
    "href": "forblog/posts/6_ai_in_a_nutshell.html",
    "title": "AI in a Nutshell",
    "section": "",
    "text": "This blog post was updated on Saturday, 12 November 2022.\nArtificial Intelligence. Machine Learning. Neural Networks. Deep Learning. Fancy Words. Deceptively Simple. All really the same.\nThe basic workflow to create such a system is below.\nVery simple, eh? Of course, it’s a very high level abstraction, but this high level view will make this seemingly complex topic very simple.\nFirst, what’s the main thing modern AI methods try to do? They try to make predictions about certain things.\nSo a function of sorts is needed to achieve this. A function that can make these predictions. Think of a function as a machine. You put something into the machine and then, with whatever was input, the machine then produces an output.\nThe machine that we will be working with has two input slots: one slot is for training and the other slot is for predictions.\nTo create a function that produces predictions, we need to tell the function what sort of predictions it needs to make.\nTo do that, we can pour some data into the training slot. This data will tell the function what sort of predictions to output. This process is known as fitting the function to the data.\nTo fit the function onto data, you train the function."
  },
  {
    "objectID": "forblog/posts/6_ai_in_a_nutshell.html#simple-case-quadratic-function",
    "href": "forblog/posts/6_ai_in_a_nutshell.html#simple-case-quadratic-function",
    "title": "AI in a Nutshell",
    "section": "Simple Case: Quadratic Function",
    "text": "Simple Case: Quadratic Function\nGasp! A quadratic?? What’s this nonsense!\nA quadratic is a very simple equation. When shown on a graph, it looks like this.\n\n\n\n\n\n\n\n\n\nWe’ll be using this equation to demonstrate a very simple example.\nThe basic workflow for fitting a function to data is below.\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n    B[Calculate Loss] --&gt; C[Calculate Gradients] --&gt; D[Update Parameters] --&gt; B\n\n\n\n\n\n\n\n\n\n\n\nIt can seem like a lot at first glance; quite a few new terms too.\nWe’ll break this down by going over the very simple example.\nLet’s say we have the following data points that describe, say, the speed of an object with respect to time. We want to predict what the speed of an object would be outside these data points.\nThe horizontal axis is time and the vertical axis is the object’s speed.\n\n\n\n\n\n\n\n\n\nWe can see that the data looks like the quadratic function shown above! Therefore, we could use the quadratic to predict what the speed of the object would be after 2.0 s and before -2.0 s.\nA quadratic equation includes three numbers which we will call \\(a\\), \\(b\\), and \\(c\\). These three numbers affect or control how our quadratic function will end up looking. \\(a\\), \\(b\\), and \\(c\\) are our parameters.\nLet’s let \\(a\\), \\(b\\), and \\(c\\) all equal \\(1\\) to begin with.\n\n\n\n\n\n\n\n\n\nHmm, not a very good fit.\nLet’s try another set of values for the parameters: \\(2\\), \\(1\\), \\(1.5\\).\n\n\n\n\n\n\n\n\n\nLooking much better now!\nLet’s see what \\(2\\), \\(0\\), and \\(1.5\\) gives us.\n\n\n\n\n\n\n\n\n\nEyeballing this is difficult. A certain set of parameters we use may be good by looking at the resulting graph, but in reality, it may not be.\nWhat we need is something that can tell us how good our function is; something that tells us whether the changes we are making are actually good or not. To do this, we can calculate a number called the loss. The smaller the loss, the better the function is.\nThere are many different ways loss can be calculated. The way we will be doing it is known as mean absolute error (MAE). In simple terms, it tells us how far off each prediction is from the actual value. For example, if we have a MAE of 1, this means that, on average, each prediction we make is 1 unit off from the real value.\nIn our case, a MAE of 1 would mean that each prediction is on average 1 m/s off from the real value.\nLet’s repeat what we did above, but this time, we’ll also see what the MAE is.\n\n\n\n\n\n\n\n\n\nAgain, this means that on average, each prediction we will make is 2.61 m/s off from the real value.\n\n\n\n\n\n\n\n\n\nThat’s a big jump!\n\n\n\n\n\n\n\n\n\nHmm, things got worse.\nDoing this process by hand is very tedious. How do we know if the new set of parameters we are using would improve the function? There needs to be a way to automate this so we don’t have to sit down and do this by hand.\nWhat we can do is update the parameters based on the loss. This would in turn create new parameters that would decrease the loss.\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n    A[Loss] -- Updates ---&gt; B[Parameters] -- Updates ---&gt; A\n\n\n\n\n\n\n\n\n\n\n\nLet’s give \\(a\\), \\(b\\), and \\(c\\) an arbitrary set of parameters \\(1.1\\), \\(1.1\\), and \\(1.1\\).\nNow let’s create a quadratic with this set of parameters and calculate its mean absolute error.\n\n\n\n\n\n\n\n\nCode Output\n\n\n\nThe MAE is 2.42.\n\n\n\n\nNow comes the next step: how do we update the parameters based on this loss we have calculated?\nTo do this, we calculate a new set of quantities known as the gradients. Each parameter has its own gradient.\nLet’s say \\(a\\) has the value of \\(1\\). If \\(a\\) has a gradient of value \\(0.5\\), this would mean that if we increase \\(a\\) by \\(1\\), the loss would increase by \\(0.5\\). Therefore, if we decrease \\(a\\) by \\(1\\), this would mean the loss would decrease by \\(0.5\\), which is what we want!\nRead over this once more and it’ll make sense!\nLet’s quickly go over the inverse: if \\(a\\) has a gradient of value \\(-0.5\\), increasing \\(a\\) by \\(1\\) would decrease the loss by \\(0.5\\) — again, this is what we want! Similarly, decreasing \\(a\\) by \\(1\\) would increase the loss by \\(0.5\\).\nThe gradients are calculated from the loss. Then the gradients, the current parameters, and along with another value, the parameters are updated to new values. The “another value” is known as the learning rate. The learning rate controls how much the gradients update the parameters.\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n    A[Gradients]\n    B[Current Parameters]\n    C[Learning Rate]\n    D[Magical Box]\n    E[Updated Paramters]\n    A & B & C ---&gt; D ---&gt; E\n\n\n\n\n\n\n\n\n\n\n\nLets see this tangibly.\n\n\n\n\n\n\n\n\nCode Output\n\n\n\nThe gradients for each parameter respectively are [-1.35, -0.03, -0.5].\n\n\n\n\nOkay, let’s break this down. The gradient for the first parameter \\(a\\) is \\(-1.35\\). This tells us that if we increase the parameter \\(a\\) by \\(1\\), our loss will decrease by \\(-1.35\\). Similary, if we increase the parameter \\(b\\) by \\(1\\), this will result in the loss being decreased by \\(-0.03\\). The same logic holds for \\(c\\).\nLet’s now update the parameters. Remember, the current set of parameters, their gradients, and the learning rate all update the current set of parameters to new values.\n\n\n\n\n\n\n\n\nCode Output\n\n\n\nThe new parameters are [1.11, 1.1, 1.11].\n\n\n\n\nWe can now repeat the process as many times as desired. Let’s do it 4 times.\n\n\nPass: 0; Loss: 2.4010409560416095\nPass: 1; Loss: 1.9847692009423128\nPass: 2; Loss: 1.498316818239171\nPass: 3; Loss: 1.171195547258246\n\n\n\n\n\n\n\n\nCode Output\n\n\n\nThe MAE after 4 passes is 1.17.\n\n\n\n\n\n\n\n\n\n\n\nAnd there you go! An even better fitting quadratic!\nLet’s see what the object’s speed is at 1 second.\n\n\n\n\n\n\n\n\nCode Output\n\n\n\nThe object’s velocity at 1 seconds is 5.65 m/s.\n\n\n\n\nThat roughly seems right!\nLet’s see what the object’s speed would be at 3 seconds.\n\n\n\n\n\n\n\n\nCode Output\n\n\n\nThe object’s velocity at 1 seconds is 30.31 m/s.\n\n\n\n\nAnd now, the diagram below should make sense!\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n    B[Calculate Loss] --&gt; C[Calculate Gradients] --&gt; D[Update Parameters] --&gt; B"
  },
  {
    "objectID": "forblog/posts/6_ai_in_a_nutshell.html#the-cool-case-relus",
    "href": "forblog/posts/6_ai_in_a_nutshell.html#the-cool-case-relus",
    "title": "AI in a Nutshell",
    "section": "The Cool Case: ReLUs",
    "text": "The Cool Case: ReLUs\nThe quadratic example above is a nice, simple way to get a grasp of things. However, you may be wondering, “What if the data doesn’t follow a quadratic shape? What do we do then?”\nAnd that’s a good question! What if our data doesn’t follow any sort of mathematical shape? What if we don’t even know the shape the data will follow? How do we know what function to use in that case?\nThere is a solution to that! There is an easy way to create a function that bends and twists itself to fit the data; an “unbound” function of sorts, as I like to call it.\nThis can be achieved by using another equation known as the ReLU. Another fancy word that can make you sound like a professional, while also being really simple. ReLU is short for Rectified Linear Unit.\nThe ReLU takes any value that is less than 0, and converts to 0.\nLet’s see this.\nTake the following line. It has both positive and negative values on the vertical axis.\n\n\n\n\n\n\n\n\n\nWhen we use a ReLU, all negative values are converted to zero.\n\n\n\n\n\n\n\n\n\nLet’s return to our original data.\n\n\n\n\n\n\n\n\n\nNow a single ReLU won’t work as seen below.\n\n\n\n\n\n\n\n\n\nEven after we try to fit it.\n\n\n\n\n\n\n\n\n\nBut look at what happens when two ReLUs are, literally, added together!\n\n\n\n\n\n\n\n\n\n\nPretty neat, hey?\nLet’s add a third ReLU to the mix.\n\n\n\n\n\n\n\n\n\n\nYou can see here how the function is adapting to the shape of the data.\nWith some extra experimentation, I was able to get the loss down to 1.08!\n\n\n\n\n\n\n\n\n\nThat said, it’s not too much of a difference when compared to two ReLUs.\nWhat if we add 5 more to the mix, for a total of 8?\n\n\n\n\n\n\n\n\n\nNice! The MAE has gone below 1!\nIt’s even beat the quadratic function from before! With some expermimenting, I had managed to get the quadratic’s loss down to 1.03.\n\n\n\n\n\n\n\n\n\n\nLet’s use the model that has 8 ReLUs to predict what the object’s velocity would be at 1 second.\n\n\n\n\n\n\n\n\nCode Output\n\n\n\nThe object’s speed at 1 s is 4.9 m/s.\n\n\n\n\nHmm, yes, that is a bit off. But that is fine because overall, the function is a lot more accurate for all the datapoints."
  },
  {
    "objectID": "forblog/posts/6_ai_in_a_nutshell.html#conclusion",
    "href": "forblog/posts/6_ai_in_a_nutshell.html#conclusion",
    "title": "AI in a Nutshell",
    "section": "Conclusion",
    "text": "Conclusion\nSee how easy this stuff all is? All those fancy terms makes this feel complex when in reality, it’s all really simple.\nWhy not now go and venture off to learn more and implement your own models!\nBelow are two free courses I can recommend:\n\nElements of AI\nA great primer into AI. The course goes over the history, the implementations, and the implications of this field, all without needing the knowledge of programming or complex mathematics.\nPractical Deep Learning for Coders\nThis course is different from other AI courses you’ll find. How? Because instead of starting off with the nitty gritty basics, you begin by actually implementing your own simple image classifier (a model that can tell what thing is in an image). You’ll be surprised at how simple it is to implement models with minimal code, and how little you need to know to get started (hint: you only really need high-school maths).\n\nIf you have any questions, comments, suggestions, or feedback, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/6_ai_in_a_nutshell.html#acknowledgements",
    "href": "forblog/posts/6_ai_in_a_nutshell.html#acknowledgements",
    "title": "AI in a Nutshell",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis article was inspired by the How does a neural net really work Kaggle Notebook by Jeremy Howard, and lesson 3 of Practical Deep Learning for Coders."
  },
  {
    "objectID": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html",
    "href": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html",
    "title": "How to Convert Audio to Spectrogram Images",
    "section": "",
    "text": "You can find this notebook on Kaggle here.\nIn this to-the-point notebook, I go over how one can create images of spectrograms from audio files using the PyTorch torchaudio module.\nThe notebook also goes over how I created the spectrogram images for the BirdCLEF 2023 competition, and how one can create and push a dataset right on Kaggle (useful if your local machine doesn’t have enough storage).\nYou can view the dataset that was generated from this notebook here."
  },
  {
    "objectID": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#setup",
    "href": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#setup",
    "title": "How to Convert Audio to Spectrogram Images",
    "section": "Setup",
    "text": "Setup\n\ntry: from fastkaggle import *\nexcept ModuleNotFoundError:\n    ! pip install -Uqq fastkaggle\n    from fastkaggle import *\n\niskaggle\n\n'Batch'\n\n\n\ncomp = 'birdclef-2023'\nd_path = setup_comp(comp, install='nbdev')\n\n\nfrom fastai.imports import *\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#data",
    "href": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#data",
    "title": "How to Convert Audio to Spectrogram Images",
    "section": "Data",
    "text": "Data\n\nPaths\nLet’s see all the files and directories we have.\n\nd_path.ls()\n\n(#5) [Path('../input/birdclef-2023/sample_submission.csv'),Path('../input/birdclef-2023/train_audio'),Path('../input/birdclef-2023/eBird_Taxonomy_v2021.csv'),Path('../input/birdclef-2023/train_metadata.csv'),Path('../input/birdclef-2023/test_soundscapes')]\n\n\nLet’s get the path to the audio files.\n\naud_files = d_path/'train_audio'\n\nAnd create a directory to store the spectrogram images.\n\nmkdir('/kaggle/train_images', exist_ok=True); Path('/kaggle/train_images').exists()\n\nTrue"
  },
  {
    "objectID": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#single-image",
    "href": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#single-image",
    "title": "How to Convert Audio to Spectrogram Images",
    "section": "Single Image",
    "text": "Single Image\nIt’s always a good idea to try things out on a smaller scale; so let’s begin by converting only a single audio file.\nLet’s get the first audio file.\n\naud_files.ls()\n\n(#264) [Path('../input/birdclef-2023/train_audio/yetgre1'),Path('../input/birdclef-2023/train_audio/moccha1'),Path('../input/birdclef-2023/train_audio/rostur1'),Path('../input/birdclef-2023/train_audio/walsta1'),Path('../input/birdclef-2023/train_audio/ratcis1'),Path('../input/birdclef-2023/train_audio/norfis1'),Path('../input/birdclef-2023/train_audio/macshr1'),Path('../input/birdclef-2023/train_audio/brrwhe3'),Path('../input/birdclef-2023/train_audio/crefra2'),Path('../input/birdclef-2023/train_audio/pabspa1')...]\n\n\n\naud_files.ls()[0].ls()\n\n(#27) [Path('../input/birdclef-2023/train_audio/yetgre1/XC247367.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC574558.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC403259.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC498854.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC289493.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC716763.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC498853.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC338717.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC349660.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC403543.ogg')...]\n\n\n\naud = aud_files.ls()[0].ls()[0]; aud\n\nPath('../input/birdclef-2023/train_audio/yetgre1/XC247367.ogg')\n\n\nNow it’s time to load it in. What we get in return is the waveform and the sample rate.\n\nimport torchaudio\nwvfrm, sr = torchaudio.load(aud); wvfrm\n\ntensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.0518e-05, 0.0000e+00,\n         0.0000e+00]])\n\n\n\n\nBirdCLEF 2023 — Clipping the Audio Files\nThis competition requires predictions to be submitted of all 5 second intervals in each audio clip. This means the audio files need to be clipped.\nBelow is an easy way this can be done. We clip the first 5 seconds of the audio file.\n\nstart_sec = 0\nend_sec = 5\nwvfrm = wvfrm[:, start_sec*sr:end_sec*sr]\nwvfrm.shape[1] / sr\n\n5.0\n\n\nSample rate is simply the number of frames recorded per second. The waveform that torchaudio returns is a tensor of frames. Therefore, we can easily select the desired range of frames by multiplying the sample rate with the desired start and end seconds.\n\nNow let’s create the spectrogram.\n\nimport torchaudio.transforms as T\nspec = T.Spectrogram()(wvfrm); spec\n\ntensor([[[4.3970e-08, 8.2461e-09, 4.7306e-11,  ..., 7.8266e-08,\n          1.7642e-08, 1.5016e-03],\n         [6.3310e-09, 6.5514e-10, 1.6958e-08,  ..., 4.3492e-09,\n          3.6019e-08, 1.5231e-03],\n         [1.1548e-08, 1.7308e-08, 6.5956e-08,  ..., 2.9340e-06,\n          1.2277e-06, 1.4124e-03],\n         ...,\n         [2.4446e-07, 6.1277e-09, 1.4932e-09,  ..., 1.4665e-08,\n          1.0110e-08, 1.8980e-05],\n         [3.1582e-07, 1.4777e-09, 1.2275e-08,  ..., 1.3213e-08,\n          1.9035e-09, 2.0009e-05],\n         [3.1673e-07, 1.1897e-10, 2.7457e-09,  ..., 1.0001e-08,\n          6.0452e-14, 1.9979e-05]]])\n\n\nLet’s scale it logarithmically. This allows for better viewing.\n\nspec = T.AmplitudeToDB()(spec); spec\n\ntensor([[[ -73.5684,  -80.8375, -100.0000,  ...,  -71.0643,  -77.5346,\n           -28.2346],\n         [ -81.9853,  -91.8367,  -77.7063,  ...,  -83.6159,  -74.4347,\n           -28.1726],\n         [ -79.3751,  -77.6177,  -71.8074,  ...,  -55.3254,  -59.1092,\n           -28.5005],\n         ...,\n         [ -66.1180,  -82.1270,  -88.2588,  ...,  -78.3373,  -79.9524,\n           -47.2170],\n         [ -65.0056,  -88.3043,  -79.1097,  ...,  -78.7899,  -87.2045,\n           -46.9877],\n         [ -64.9931,  -99.2458,  -85.6135,  ...,  -79.9997, -100.0000,\n           -46.9943]]])\n\n\nThe PyTorch tensor needs to be converted into a NumPy array so it can then further be converted to an image. I’m using the squeeze method to remove the uneeded axis of length 1, as seen below.\n\nspec.shape\n\ntorch.Size([1, 201, 801])\n\n\n\nspec = spec.squeeze().numpy(); spec\n\narray([[ -73.56842 ,  -80.83749 , -100.      , ...,  -71.064285,\n         -77.53463 ,  -28.234562],\n       [ -81.98525 ,  -91.83666 ,  -77.706314, ...,  -83.615875,\n         -74.43467 ,  -28.172626],\n       [ -79.37505 ,  -77.61765 ,  -71.80743 , ...,  -55.32541 ,\n         -59.109177,  -28.500452],\n       ...,\n       [ -66.118   ,  -82.127014,  -88.25883 , ...,  -78.33732 ,\n         -79.95244 ,  -47.21705 ],\n       [ -65.00563 ,  -88.30426 ,  -79.10974 , ...,  -78.78989 ,\n         -87.20445 ,  -46.987743],\n       [ -64.99306 ,  -99.24575 ,  -85.61355 , ...,  -79.99969 ,\n        -100.      ,  -46.994343]], dtype=float32)\n\n\n\nspec.shape\n\n(201, 801)\n\n\nThe array now needs to be normalized so it contains integers between 0 and 255: the values needed for images.\n\nspec = (spec - spec.min()) / (spec.max() - spec.min()) * 255; spec\n\narray([[ 53.000538 ,  38.424625 ,   0.       , ...,  58.021824 ,\n         45.047504 , 143.90388  ],\n       [ 36.123127 ,  16.369104 ,  44.703243 , ...,  32.853405 ,\n         51.263535 , 144.02808  ],\n       [ 41.35709  ,  44.881027 ,  56.53168  , ...,  89.581375 ,\n         81.99417  , 143.37071  ],\n       ...,\n       [ 67.94011  ,  35.838867 ,  23.543371 , ...,  43.437954 ,\n         40.199318 , 105.84024  ],\n       [ 70.17062  ,  23.452267 ,  41.889095 , ...,  42.530468 ,\n         25.6576   , 106.30005  ],\n       [ 70.19584  ,   1.5124193,  28.847677 , ...,  40.104576 ,\n          0.       , 106.28681  ]], dtype=float32)\n\n\n\nspec = spec.astype('uint8'); spec\n\narray([[ 53,  38,   0, ...,  58,  45, 143],\n       [ 36,  16,  44, ...,  32,  51, 144],\n       [ 41,  44,  56, ...,  89,  81, 143],\n       ...,\n       [ 67,  35,  23, ...,  43,  40, 105],\n       [ 70,  23,  41, ...,  42,  25, 106],\n       [ 70,   1,  28, ...,  40,   0, 106]], dtype=uint8)\n\n\nNow we can finally convert the array to an image!\n\nimg = Image.fromarray(spec)\nprint(img.shape)\nimg\n\n(201, 801)\n\n\n\n\n\n\n\n\n\nCool, hey? We’ve just visualized audio!\n\n\n\nBirdCLEF 2023 — Resizing the Images\nTo allow the images to easily be used by various models, I resized the spectrograms to be 512 by 512 pixels as shown below.\n\nimg_size = (512, 512)\nimg = img.resize(img_size)\nprint(img.shape); img\n\n(512, 512)\n\n\n\n\n\n\n\n\n\n\nTo save the image, we can simply use the save method.\n\nimg.save('img.png')"
  },
  {
    "objectID": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#all-the-images",
    "href": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#all-the-images",
    "title": "How to Convert Audio to Spectrogram Images",
    "section": "All the Images",
    "text": "All the Images\nNow that we have verified that our algorithm works fine, we can extend it to convert all audio files.\n\ndef create_imgs(duration, f):\n    for step in range(0, duration, 5):\n        wvfrm, sr = torchaudio.load(f)\n        wvfrm = cut_wvfrm(wvfrm, sr, step)\n        spec = create_spec(wvfrm)\n        img = spec2img(spec)\n        end_sec = step + 5\n        img.save(f'/kaggle/train_images/{bird.stem}/{f.stem}_{end_sec}.png')\n\ndef cut_wvfrm(wvfrm, sr, step):\n    start_sec, end_sec = step, step + 5\n    return wvfrm[:, start_sec * sr: end_sec * sr]\n            \ndef create_spec(wvfrm):\n    spec = T.Spectrogram()(wvfrm)\n    return T.AmplitudeToDB()(spec)\n        \ndef spec2img(spec, img_size=(512, 512)):\n    spec = np.real(spec.squeeze().numpy())\n    spec = ((spec - spec.min()) / (spec.max() - spec.min()) * 255).astype('uint8')\n    return Image.fromarray(spec).resize(img_size)\n\n\nif not iskaggle:\n    for bird in aud_files.ls().sorted():\n        mkdir(f'/kaggle/train_images/{bird.stem}', exist_ok=True)\n        for f in bird.ls().sorted():\n            info = torchaudio.info(f)\n            duration = info.num_frames / info.sample_rate\n            if duration &gt;= 5:\n                create_imgs(round(duration/5)*5, f)\n            else: continue\n\n\nNote: Ignore the if not iskaggle statement when replicating. I added it since I edited this notebook and needed to save changes without reproducing the entire dataset.\n\nIn the first for loop below, we loop through all the bird folders. For each folder, a folder with the same name is created in the directory where we want to store the images.\nIn the second for loop, we loop through all audio files within the folder and then convert them to spectrogram images through the create_images function I defined.\n\n\nBirdCLEF 2023 — Clipping the Audio Files\nSome audio files in the training set are of different durations. Therefore, we obtain the duration of the audio file so it can correctly be clipped into 5 second intervals.\ninfo = torchaudio.info(f)\nduration = info.num_frames / info.sample_rate\nif duration &gt;= 5:\n    create_images(round(duration/5)*5, f)\nelse: continue\nAgain, since sample rate is the number of frames recorded per second, we can divide the total number of frames by the sample rate to obtain the duration in seconds of a clip.\nduration = info.num_frames / info.sample_rate\nThen we round the duration to the nearest 5 for easy clipping.\nround(duration/5)*5\n\nThe images now created! The rest of this notebook covers how one can generate a dataset in a Kaggle Notebook and push it directly to Kaggle within it."
  },
  {
    "objectID": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#api-setup",
    "href": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#api-setup",
    "title": "How to Convert Audio to Spectrogram Images",
    "section": "API Setup",
    "text": "API Setup\nWe need to configure the user keys so we can push to the correct account.\nTo do this, first obtain your Kaggle API key. Then, while in the notebook editor, click Add-ons -&gt; Secrets -&gt; Add a New Secret…\n\n\n\n…input your key and give it a name…\n\n…and click save. Then click the checkbox next to the secret to activate it for your notebook.\n\nRepeat for your Kaggle username.\nNow we can set the keys for the notebook as shown below (input the name of your key into get_secret).\n\nimport os\nfrom kaggle_secrets import UserSecretsClient\n\n\nsecrets = UserSecretsClient()\nos.environ['KAGGLE_USERNAME'] = secrets.get_secret('KAGGLE_USERNAME')\nos.environ['KAGGLE_KEY'] = secrets.get_secret('KAGGLE_KEY')"
  },
  {
    "objectID": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#push-dataset",
    "href": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#push-dataset",
    "title": "How to Convert Audio to Spectrogram Images",
    "section": "Push Dataset",
    "text": "Push Dataset\nThe fastkaggle library offers a convenient way to easily create and push a dataset to Kaggle.\n\ndoc(mk_dataset)\n\n\nmk_dataset\nmk_dataset(dataset_path, title, force=False, upload=True)Creates minimal dataset metadata needed to push new dataset to kaggle\n\n\n\nNote: Ignore the if not iskaggle statement when replicating. I added it since I edited this notebook and needed to save changes without reproducing the entire dataset.\n\n\nif not iskaggle:\n    mk_dataset('/kaggle/train_images', 'spectrograms-birdclef-2023', force=True, upload=True)\n\nAnd we can verify our dataset has been created by having a look at the generated metadata file.\n\nif not iskaggle:\n    ! cat /kaggle/train_images/dataset-metadata.json\n\nFrom here, we can go directly to the dataset page on Kaggle and fill out the rest of the details."
  },
  {
    "objectID": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#and-there-you-have-it",
    "href": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#and-there-you-have-it",
    "title": "How to Convert Audio to Spectrogram Images",
    "section": "And there you have it!",
    "text": "And there you have it!\nIn summary, you saw how to: * Generate spectrogram images from audio files using torchaudio and fastai * How to cut audio tracks * And how to create and push a dataset directly on Kaggle\nYou can view the dataset that was generated from this notebook here.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/11_musings_through_stable_diffusion.html",
    "href": "forblog/posts/11_musings_through_stable_diffusion.html",
    "title": "My Musings Through Stable Diffusion",
    "section": "",
    "text": "Quick tip: Click or tap the images to view them up close.\nI recently began fastai Course Part 2: a course where one dives into the deeper workings of deep learning by fully implementing stable diffusion.\nIn the first lesson, we play around with diffusers using the Hugging Face Diffusers library. Below are things I have noticed; my musings."
  },
  {
    "objectID": "forblog/posts/11_musings_through_stable_diffusion.html#steps",
    "href": "forblog/posts/11_musings_through_stable_diffusion.html#steps",
    "title": "My Musings Through Stable Diffusion",
    "section": "Steps",
    "text": "Steps\nDiffusion is simply a process whereby noise is progressively removed from a noisy image. A single step can be thought of a single portion of noise being removed.\n\n\n\nA depiction of a ring comprised of interwined serpents, topped with a single jewel of emerald.\n\n\nBelow is the evolution of the image above in 48 steps. Each new image has less and less noise (what the diffuser thinks is noise).\n\n\n\nThe gif itself has artefacts due to compression…\n\n\n\nIt still managed to generate a pretty good image despite the misspelling of “intertwined”!"
  },
  {
    "objectID": "forblog/posts/11_musings_through_stable_diffusion.html#when-it-doesnt-work-well",
    "href": "forblog/posts/11_musings_through_stable_diffusion.html#when-it-doesnt-work-well",
    "title": "My Musings Through Stable Diffusion",
    "section": "When It Doesn’t Work Well",
    "text": "When It Doesn’t Work Well\nI’ve found that a diffuser doesn’t work well when one prompts it for things, which I assume, it hasn’t “seen” or hasn’t been trained on before. It sounds obvious, but it’s really interesting when you see the result of it.\n\n\n\nA grasshopper riding a bunny.\n\n\n\nA quick Google search also doesn’t return any images matching the prompt in the top results."
  },
  {
    "objectID": "forblog/posts/11_musings_through_stable_diffusion.html#cfg-classifier-free-guidance",
    "href": "forblog/posts/11_musings_through_stable_diffusion.html#cfg-classifier-free-guidance",
    "title": "My Musings Through Stable Diffusion",
    "section": "CFG (Classifier Free Guidance)",
    "text": "CFG (Classifier Free Guidance)\nOr simply known as guidance, CFG is a value which tells the diffuser how much it should stick to the prompt.\nA lower guidance leads to more varied and random images that are loosely related to the prompt. A higher guidance produces more relevant images.\nI’ve found that too high of a guidenace leads to images having too much contrast.\n\n\n\nAn antique 18th century painting of a gorilla eating a plate of chips.\n\n\nThe image above shows rows with increasing levels of guidance (1, 2.5, 5, 7.5, 10, 25, 50). 7.5 is the sweetspot."
  },
  {
    "objectID": "forblog/posts/11_musings_through_stable_diffusion.html#negative-prompts",
    "href": "forblog/posts/11_musings_through_stable_diffusion.html#negative-prompts",
    "title": "My Musings Through Stable Diffusion",
    "section": "Negative Prompts",
    "text": "Negative Prompts\nThe best way to think about negative prompts is that a negative prompt guides a diffuser away from generating a certain entity.\nTake the image below as an example.\n\n\n\nAn antique 18th century painting of a gorilla eating a plate of chips.\n\n\nI generated the image again using the exact same seed and prompt, but also used the following negative prompt, “yellow circle”.\n\n\n\nPrompt: An antique 18th century painting of a gorilla eating a plate of chips. | Negative Prompt: yellow circle"
  },
  {
    "objectID": "forblog/posts/11_musings_through_stable_diffusion.html#image-to-image",
    "href": "forblog/posts/11_musings_through_stable_diffusion.html#image-to-image",
    "title": "My Musings Through Stable Diffusion",
    "section": "Image to Image",
    "text": "Image to Image\nInstead of starting from noise, one can make a diffuser begin from an existing image. The diffuser follows the image as guide and doesn’t match it 1 to 1.\nI quickly mocked up the following image.\n\nI input it to a diffuser with a prompt, and it output the following.\n\n\n\nA bench under a tree in a park\n\n\nI then further generated another image from this one.\n\n\n\nA low poly 3D render of a bench under a tree in a park"
  },
  {
    "objectID": "forblog/posts/11_musings_through_stable_diffusion.html#further-adapting-a-diffuser",
    "href": "forblog/posts/11_musings_through_stable_diffusion.html#further-adapting-a-diffuser",
    "title": "My Musings Through Stable Diffusion",
    "section": "Further Adapting a Diffuser",
    "text": "Further Adapting a Diffuser\nThere are two ways one can further customize a diffuser to produce desired images: textual inversion and dreambooth.\n\nTextual Inversion\nA diffuser contains a text encoder. This encoder is responsible for parsing the prompt and giving it a mathematical representation.\nA text encoder can only parse according to its vocabulary. If it encounters words not in its vocabulary, the diffuser will be unable to produce an image relevant to the prompt.\nIn a nutshell, textual inversion adds new words to the vocabulary of the text encoder so it can parse prompts with those new words.\nI managed to generate the image below by adding the word “Mr Doodle” to the vocabulary of the diffuser’s text encoder.\n\n\n\nAn antique 18th century painting of a gorilla eating a plate of chips in the style of Mr Doodle\n\n\n\n\nDreambooth\nDreambooth is more akin to traditional fine-tuning methods. A diffuser is further trained on images one supplies to it."
  },
  {
    "objectID": "forblog/posts/11_musings_through_stable_diffusion.html#so-end-my-musings",
    "href": "forblog/posts/11_musings_through_stable_diffusion.html#so-end-my-musings",
    "title": "My Musings Through Stable Diffusion",
    "section": "So End my Musings",
    "text": "So End my Musings\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/23_basic_math_terms_in_chinese.html",
    "href": "forblog/posts/23_basic_math_terms_in_chinese.html",
    "title": "中文里的一些基本数学术语",
    "section": "",
    "text": "算术运算\n\n\\(n + m\\) 是 “n 加 m”.\n\\(n - m\\) 是 “n 减 m”.\n\n\n\n乘除法\n\n标准说法：\n\n\\(n \\cdot m\\) 是 “n 乘以 m”\n\\(n / m\\) 是 “n 除以 m”\n\n传统用法：\n\n“n 乘 m” 可能表示 \\(m \\cdot n\\)\n“n 除 m” 可能表示 \\(m / n\\)\n\n\n\n\n等式/相等\n\n\\(=\\) 是 “等于”\n\\(≈\\) 是 “约等于”\n\\(≠\\) 是 “不等于”\n\\(&lt;\\) 是 “小于”\n\\(&gt;\\) 是 “大于”\n\\(\\leq\\) 是 “小于等于”\n\\(\\geq\\) 是 “小于等于”\n\n\n\n负数\n\n\\(-4.87\\) 是 “负 4 点 87”\n\n\n\n几何\n\n\\(\\angle ABC\\) 是 “角 ABC”\n\\(\\triangle ABC\\) 是 “三角形 ABC”\n\n\n\n微积分\n\n\\(\\frac{dy}{dx}\\) 是 “dy 除以 dx” 或者 “y 对 x 的导数”\n\\(\\int f(x) dx\\) 是 “f(x) 的积分”\n\n\n\n集合论\n\n\\(\\in\\) 是 “属于”\n\\(\\subset\\) 是 “子集”\n\n\n\n组合数学\n\n\\(C^{n}_{m}\\) 是 “n 取 m” 或者 “从 n 个元素中取 m 个元素的组合数”\n\n\n\n阶乘与连乘\n\n\\(1 \\cdot 2 \\cdot 3 \\cdot \\cdots\\) 是 “1 乘 2 乘 3 乘 4 一直乘下去”\n\\(1 \\cdot 2 \\cdot 3 \\cdot 4 \\cdots \\cdot n = n!\\) 是 “1 乘 2 乘 3 一直乘到 n” 或者 “n 的阶乘”\n\n\n\n分数\n\n\\(\\frac{a}{b}\\) 是 “b 分之 a”\n\\(c\\frac{a}{b}\\) 是 “c 又 b 分之 a”\n\n\n\n指数与根式\n\n\\(n^{m}\\) 是 “n 次方”\n\\(\\sqrt{n}\\) 是 “根号 n”\n\\(\\sqrt[m]{n}\\) 是 “n 的 m 次方根”\n\n\n如果你有任何意见、问题、建议、反馈、批评、或更正，请在下方评论区留言！\n\n\n\n\n Back to top"
  },
  {
    "objectID": "forblog/posts/3_the_confusion_matrix.html",
    "href": "forblog/posts/3_the_confusion_matrix.html",
    "title": "A No Nonsense Guide to Reading a Confusion Matrix",
    "section": "",
    "text": "This article was updated on Thursday, 10 November 2022.\nConfusion matrices help model designers view what mistakes a model has made.\nIn this post, I’ll be telling you how to easily read such matrices.\nJump to Section 2 for an ultra concise rundown.\nReady? Here we go."
  },
  {
    "objectID": "forblog/posts/3_the_confusion_matrix.html#case-1-introduction",
    "href": "forblog/posts/3_the_confusion_matrix.html#case-1-introduction",
    "title": "A No Nonsense Guide to Reading a Confusion Matrix",
    "section": "Case 1: Introduction",
    "text": "Case 1: Introduction\n\nIgnore the “Actual” and “Predicted” labels for now.\nLet’s compare grizzly bears to black bears.\nAll comparisons begin at the bottom, with the columns.\nFirst, highlight the grizzly bear column.\n\nNext, highlight the black bear row.\n\nNow find the common entry in the highlighted column and row.\n\nThis common entry is our required information.\nAll entries in the diagonal going from the top left to the bottom right (blue) are correct classifications. All other entries are incorrect classifications.\nOur common entry does not lie in the main diagonal. Therefore, we are looking at incorrect classifications.\nWe have compared grizzly bears to black bears. Therefore, from this deduction, three grizzly bears have been incorrectly classified as black bears.\n\n\n\n\n\n\nNote\n\n\n\nThere is a difference between comparing grizzly bears to black bears and black bears to grizzly bears.\nComparing grizzly bears to black bears means, “How many grizzly bears were misclassified as black bears?”\nComparing black bears to grizzly bears means, “How many black bears were misclassified as grizzly bears?”"
  },
  {
    "objectID": "forblog/posts/3_the_confusion_matrix.html#sec-case2",
    "href": "forblog/posts/3_the_confusion_matrix.html#sec-case2",
    "title": "A No Nonsense Guide to Reading a Confusion Matrix",
    "section": "Case 2: Ultra Concise",
    "text": "Case 2: Ultra Concise\nLet’s compare black bears to grizzly bears.\nHighlight the black bear column.\n\nHighlight the grizzly bear row.\n\nHighlight the common entry.\n\nZero black bears were misclassified as grizzly bears."
  },
  {
    "objectID": "forblog/posts/3_the_confusion_matrix.html#case-3-correct-classifications",
    "href": "forblog/posts/3_the_confusion_matrix.html#case-3-correct-classifications",
    "title": "A No Nonsense Guide to Reading a Confusion Matrix",
    "section": "Case 3: Correct Classifications",
    "text": "Case 3: Correct Classifications\nLet’s see how many teddy bears were correctly classified. We are essentially comparing teddy bears to teddy bears.\nHighlight the teddy bear column.\n\nHighlight the teddy bear row.\n\nHighlight the common entry.\n\nFifty three teddy bears were correctly classified as teddy bears."
  },
  {
    "objectID": "forblog/posts/3_the_confusion_matrix.html#exercise-do-it-yourself",
    "href": "forblog/posts/3_the_confusion_matrix.html#exercise-do-it-yourself",
    "title": "A No Nonsense Guide to Reading a Confusion Matrix",
    "section": "Exercise: Do It Yourself",
    "text": "Exercise: Do It Yourself\nBelow is a confusion matrix of a car classifier that classifies cars into their brand.\n\nYou learn by doing!\n\nHow many Lamborghinis were correctly classified?\nHow many Jaguars were incorrectly classified?\nHow many Chevrolets were misclassified as Fords?\nHow many Fords were misclassified as Chevrolets?\nWhich two car brands did the model have the most trouble differentiating between?\n\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/19_tokenizers.html",
    "href": "forblog/posts/19_tokenizers.html",
    "title": "A Brief Token on Tokenizers",
    "section": "",
    "text": "This notebook follows the fastai style guide.\nTokenization is the process whereby text is given a numerical representation. Sentences are split into components known as tokens. These tokens represent numerical values that language models can work with.\nThere are various approaches to tokenization. Examples include:\nLanguage models require the use of their own tokenization technique to properly work. Let’s have a look at three approaches."
  },
  {
    "objectID": "forblog/posts/19_tokenizers.html#word-based",
    "href": "forblog/posts/19_tokenizers.html#word-based",
    "title": "A Brief Token on Tokenizers",
    "section": "Word-based",
    "text": "Word-based\nThe word-based approach, well, splits sentences into individual words. In some cases, it also splits on punctuation.\nIn the example below, the sentence is tokenized into its words using whitespace.\n\n\"I'm really excited doing this, you know?\".split()\n\n[\"I'm\", 'really', 'excited', 'doing', 'this,', 'you', 'know?']\n\n\nLet’s see it split based on its punctuation.\n\nimport re\n\nseq = \"I'm really excited doing this, you know?\"\ntoks = re.findall(r'\\w+|[^\\w\\s]+', seq); toks\n\n['I', \"'\", 'm', 'really', 'excited', 'doing', 'this', ',', 'you', 'know', '?']\n\n\nAfter tokenizing, an ID is assigned to each word, or token, so the model can identify them.\nThe issue with the word-based approach is wend up with huge vocabularies1, especially when splitting on punctuation. For instance, the English language has over 500,000 words, so we would also need more than 500,000 tokens.\n1 A vocabulary is a collection of tokens.2 Examples of such tokens include [UNK] or &lt;unk&gt;.To remedy this, we could use only the \\(x\\) most frequently used words However, the issue that arises here is when the tokenizer encounters a word not present in its vocabulary. In this situation, a token representing the concept of “unknown” would be assigned2. When there are many such tokens, the model has no way of “knowing” that these tokens in fact represent different words.\nAnother issue with this approach is that the tokenizer will assign words such as “car” and “cars” different tokens. The model will not know that these two words are actually similar and represent almost the same concept."
  },
  {
    "objectID": "forblog/posts/19_tokenizers.html#character-based",
    "href": "forblog/posts/19_tokenizers.html#character-based",
    "title": "A Brief Token on Tokenizers",
    "section": "Character-based",
    "text": "Character-based\nThis approach splits text into characters, resulting in a much, much smaller vocabulary – the English alphabet only has 26 letters, as opposed to hundreds of thousands of words. It also results in fewer unknown tokens as words are comprised from everything within the vocabulary.\n\nlist(\"Who doesn't love tokenization!\")\n\n['W',\n 'h',\n 'o',\n ' ',\n 'd',\n 'o',\n 'e',\n 's',\n 'n',\n \"'\",\n 't',\n ' ',\n 'l',\n 'o',\n 'v',\n 'e',\n ' ',\n 't',\n 'o',\n 'k',\n 'e',\n 'n',\n 'i',\n 'z',\n 'a',\n 't',\n 'i',\n 'o',\n 'n',\n '!']\n\n\nHowever, this approach also has its drawbacks. Individual characters hold less meaning than a whole word. For example, ‘t’ holds less meaning than ‘tokenization’.\nThat said, this issue is not as prevalent in other languages. In Chinese languages, each character is also a word. Therefore, characters in Chinese languages hold less meaning than characters in Latin languages.\nWhile there will be an overall smaller vocabulary, there will still be much processing to do – we end up with a large amount of individual tokens to process. ‘Hello!’ would need only a single token, where as ‘H’, ‘e’, ‘l’, ‘l’, ‘o’, and ‘!’ would require six tokens."
  },
  {
    "objectID": "forblog/posts/19_tokenizers.html#subword-based",
    "href": "forblog/posts/19_tokenizers.html#subword-based",
    "title": "A Brief Token on Tokenizers",
    "section": "Subword-based",
    "text": "Subword-based\nThis approach is a combination of the two approaches above, and is also the approach most state-of-the-art tokenizers use today.\nWith subword-based tokenizers, words fall into two categories: frequent words and rare words. Frequent words are not to be split, but rare words are to be split into meaningful subwords.\n\nFor example, ‘tokenization’ would be categorized as a rare word and would be tokenized into the tokens ‘token’ and ‘ization’. Though one word is now represented by two tokens, as opposed to a single token with the word-based approach, it is split into two components that much more frequently appear. We also don’t need eleven tokens, as would be with the character-based approach. On top of that, the model would learn the grammatical function of ‘ization’.\nThis is all while giving the model the ability to learn the meaning of ‘realization’ as the two tokens that comprise the word appear next to each other\nThis approach allows us to have relatively good covereage for a language while having relatively smaller vocabularies. It also results in minimal unknown tokens."
  },
  {
    "objectID": "forblog/posts/19_tokenizers.html#tokenizing-time",
    "href": "forblog/posts/19_tokenizers.html#tokenizing-time",
    "title": "A Brief Token on Tokenizers",
    "section": "Tokenizing Time",
    "text": "Tokenizing Time\nIf we draw parallels between a tokenizer and a model, the algorithm of a tokenizer is akin to the architecture of a model. On a similar note, the vocabulary of a tokenizer is akin to the weights of a model.\nLet’s load in the tokenizer used for the BERT base model (cased).\n\n! pip install -Uqq transformers\n\n\n1import logging; logging.disable(logging.WARNING)\nfrom transformers import AutoTokenizer\n\ntokz = AutoTokenizer.from_pretrained('bert-base-cased')\n\n\n1\n\nHuggingFace is verbose.\n\n\n\n\nWe can use the loaded tokenizer to directly tokenize our desired sequence.\n\nseq = \"The process of tokenization has lead me to the appalling conclusion: life isn't what it is.\"\ntokz(seq)\n\n{'input_ids': [101, 1109, 1965, 1104, 22559, 2734, 1144, 1730, 1143, 1106, 1103, 12647, 5727, 1158, 6593, 131, 1297, 2762, 112, 189, 1184, 1122, 1110, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\nHowever, let’s look behind the scenes to see what’s happening. We’ll only focus on how input_ids came to be."
  },
  {
    "objectID": "forblog/posts/19_tokenizers.html#behind-the-scenes",
    "href": "forblog/posts/19_tokenizers.html#behind-the-scenes",
    "title": "A Brief Token on Tokenizers",
    "section": "Behind the Scenes",
    "text": "Behind the Scenes\n\nEncoding\nEncoding is the name given to the process whereby text is mapped to numbers. Text is first tokenized, after which, the tokens are mapped to their respective IDs.\n\nTokenization\n\ntoks = tokz.tokenize(seq); toks\n\n['The',\n 'process',\n 'of',\n 'token',\n '##ization',\n 'has',\n 'lead',\n 'me',\n 'to',\n 'the',\n 'app',\n '##all',\n '##ing',\n 'conclusion',\n ':',\n 'life',\n 'isn',\n \"'\",\n 't',\n 'what',\n 'it',\n 'is',\n '.']\n\n\nAs we can see, the tokenizer used by the BERT base model (cased) is a subword-based tokenizer. This can be seen by ‘tokenization’ being split into ‘token’ and ‘##ization’, as well as ‘appalling’ being split into ‘app’, ‘##all’, and ‘##ing’.\n\n\nTokens to IDs\n\nids = tokz.convert_tokens_to_ids(toks); ids\n\n[1109,\n 1965,\n 1104,\n 22559,\n 2734,\n 1144,\n 1730,\n 1143,\n 1106,\n 1103,\n 12647,\n 5727,\n 1158,\n 6593,\n 131,\n 1297,\n 2762,\n 112,\n 189,\n 1184,\n 1122,\n 1110,\n 119]\n\n\nThe numbers that have been assigned are based on the vocabulary of the tokenizer. These IDs can now be used as input to a model."
  },
  {
    "objectID": "forblog/posts/19_tokenizers.html#decoding",
    "href": "forblog/posts/19_tokenizers.html#decoding",
    "title": "A Brief Token on Tokenizers",
    "section": "Decoding",
    "text": "Decoding\nDecoding is simply the opposite process: convert a sequence of IDs into their respective tokens, including putting together tokens that were part of the same word.\n\ndec_seq = tokz.decode(ids); dec_seq\n\n\"The process of tokenization has lead me to the appalling conclusion : life isn't what it is.\"\n\n\nThe decoding algorithm of our tokenzier has introduced a space before the colon. 🤔\nDecoding is used for models that generate text: the model outputs a sequence of IDs which are then decoded to their respective tokens."
  },
  {
    "objectID": "forblog/posts/19_tokenizers.html#conclusion",
    "href": "forblog/posts/19_tokenizers.html#conclusion",
    "title": "A Brief Token on Tokenizers",
    "section": "Conclusion",
    "text": "Conclusion\nTokenization is all about splitting text up and giving the split up text a numerical representation that computers can work with.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/7_blog_subscriptions.html",
    "href": "forblog/posts/7_blog_subscriptions.html",
    "title": "Adding Subscriptions to a Quarto Site",
    "section": "",
    "text": "The Quarto Documenation covers how to implement website subscriptions at a surface level. This guide goes into the details on how one could do so, with three different options. That said, this guide can also be helpful for sites that do not use Quarto.\nThe three ways this guide will cover:\nSwitch between the tabs below to view the steps for each option."
  },
  {
    "objectID": "forblog/posts/7_blog_subscriptions.html#option-3",
    "href": "forblog/posts/7_blog_subscriptions.html#option-3",
    "title": "Adding Subscriptions to a Quarto Site",
    "section": "Option 3",
    "text": "Option 3\nPerhaps you know some HTML and JS, or even only JS, and don’t have an alternative address. Instead of creating the frontend with HTML, try using the Quarto HTML Forms extension by Jonathan Graves.\nThis extension allows you to implement HTML forms through Quarto Shortcodes and YAML Options. However, you still will need to handle the backend with JavaScript and perhaps a few other technologies. If you’re interested in implementing it this way, you probably already know how to. If not, there are plenty of great guides online!."
  },
  {
    "objectID": "forblog/posts/7_blog_subscriptions.html#acknowledgements",
    "href": "forblog/posts/7_blog_subscriptions.html#acknowledgements",
    "title": "Adding Subscriptions to a Quarto Site",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Isaac Flath for collaborating with me on this guide! You can view his blog, works, and contact here."
  },
  {
    "objectID": "forblog/posts/18_backprop_from_scratch.html",
    "href": "forblog/posts/18_backprop_from_scratch.html",
    "title": "Backpropagation Explained using English Words*",
    "section": "",
    "text": "This post was edited on Wednesday, 9 August 2023\nBackpropagation sounds and looks daunting. It doesn’t need to be. In fact, backpropagation is really just a fancy word for the chain rule. Implementing a backpropagation algorithm is simply implementing one big fat chain rule equation.\nLet’s remind ourselves of the chain rule. The chain rule lets us figure out how much a given variable indirectly changes with respect to another variable. Take the example below.\n\\[\n\\begin{align}\n  y &= 3u \\\\\n  u &= 7 + x^2\n\\end{align}\n\\]\nWe want to figure out how much \\(y\\) changes with each increment in \\(x\\). The problem is that \\(x\\) doesn’t direcly change \\(y\\). Rather, \\(x\\) changes \\(u\\) which in turn changes \\(y\\).\nThe chain rule allows us to solve this problem. In this case, the chain rule tells us that we can figure out how much \\(x\\) indirecly changes \\(y\\) by multiplying the derivative of \\(y\\) with respect to \\(u\\), and the derivative of \\(u\\) with respect to \\(x\\).\n\\[\n\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}\n\\]\nAaand I’ve just described backpropagation in a nutshell. That’s all there really is to it. The only difference is that in a neural network there are many more intermediate variables and functions, and that we want to find out how the weights indirectly change the loss.\nLet’s see this tangibly in action.\nWe have the following neural network comprised of two layers: the first layer contains the affine function1 together with the ReLU, while the second layer contains only the affine function. The loss, which is MSE (Mean Squared Error), will then be calculated from the output of the second layer.\nflowchart LR\n  subgraph A [Layer 1]\n    direction LR\n    id1[Affine Function] --&gt; id2[ReLU]\n  end\n\n  subgraph B [Layer 2]\n    direction LR\n    id2 --&gt; id3[Affine Function]\n  end\n  \n  subgraph C [Loss Function]\n    direction LR\n    id3 --&gt; id4[MSE]\n  end\nMathematically speaking, the first layer with a single sample \\(x\\) looks like this.\n\\[\n\\text{max}(0, x \\cdot \\vec{\\rm{w}}_1 + b_1)\n\\]\nThe second layer looks like this.\n\\[\n\\text{max}(0, x \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2\n\\]\nAnd the loss function looks like this.\n\\[\n\\frac{(y - (\\text{max}(0, x \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2))^2}{2}\n\\]\nHowever, when working with multiple samples, the mean squared error comes out looking like this, where \\(N\\) represents the total number of samples.\n\\[\n\\frac{(\\vec{\\rm{y}}_1 - (\\text{max}(0, \\vec{\\rm{x}}_1 \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2))^2 + (\\vec{\\rm{y}}_2 - (\\text{max}(0, \\vec{\\rm{x}}_2 \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2))^2 + \\cdots + (\\vec{\\rm{y}}_N - (\\text{max}(0, \\vec{\\rm{x}}_N \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2))^2}{N}\n\\]\nOr more simply…2\n\\[\n\\frac{\\sum^N_{i=1} (\\vec{\\rm{y}}_i - (\\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2))^2}{N}\n\\]\n…or even more simply.\n\\[\n\\frac{1}{N} \\sum^N_{i=1} (\\vec{\\rm{y}}_i - (\\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2))^2\n\\]\nOur goal for the rest of this guide is to derive the gradients of \\(w_1\\).\nThe equation above looks quite the mouthful though. One might even say scary. How would you even apply the chain rule here? How would you use the chain rule to derive the gradients of the weights and biases?\nLet’s simplify things by introducing a bunch of intermediate variables. We’ll begin by substituting the innermost pieces of the equation, and then gradually make our way out.\n\\[\n\\begin{align}\n    u_1 &= \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 \\\\\n    u_2 &= \\text{max}(0, u_1) \\\\\n    u_3 &= u_2 \\cdot \\vec{\\rm{w}}_2 + b_2 \\\\\n    u_4 &= \\vec{\\rm{y}}_i - u_3 \\\\\n    u_5 &= u_4^2\n\\end{align}\n\\]\nThe menacing equation above now gradually simplifies into the cute equation below.\n\\[\n\\begin{align}\n    \\text{MSE} &= \\frac{1}{N} \\sum^N_{i=1} (\\vec{\\rm{y}}_i - (\\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2))^2 \\\\\n    &= \\frac{1}{N} \\sum^N_{i=1} (\\vec{\\rm{y}}_i - (\\text{max}(0, u_1) \\cdot \\vec{\\rm{w}}_2 + b_2))^2 \\\\\n    &= \\frac{1}{N} \\sum^N_{i=1} (\\vec{\\rm{y}}_i - (u_2 \\cdot \\vec{\\rm{w}}_2 + b_2))^2 \\\\\n    &= \\frac{1}{N} \\sum^N_{i=1} (\\vec{\\rm{y}}_i - u_3)^2 \\\\\n    &= \\frac{1}{N} \\sum^N_{i=1} (u_4)^2 \\\\\n    &= \\frac{1}{N} \\sum^N_{i=1} u_5\n\\end{align}\n\\]\nVery cute, hey?\nIn this cuter version of the equation, it is visible that incrementing \\(\\vec{\\rm{w}}_1\\) does not directly change the MSE. Rather, incrementing \\(\\vec{\\rm{w}}_1\\) changes \\(u_1\\), which changes \\(u_2\\), which changes \\(u_3\\), which changes \\(u_4\\), which in turn changes \\(u_5\\).\n\\[\n\\frac{\\partial}{\\partial \\vec{\\rm{w}}_1} \\text{MSE} = \\frac{\\partial}{\\partial \\vec{\\rm{w}}_1} \\frac{1}{N} \\sum^N_{i=1} u_5 = \\frac{1}{N} \\sum^N_{i=1} \\frac{\\partial u^5}{\\partial \\vec{\\rm{w}}_1} = \\frac{1}{N} \\sum^N_{i=1} \\frac{\\partial u_5}{\\partial u_4} \\cdot \\frac{\\partial u_4}{\\partial u_3} \\cdot \\frac{\\partial u_3}{\\partial u_2} \\cdot \\frac{\\partial u_2}{\\partial u_1} \\cdot \\frac{\\partial u_1}{\\partial \\vec{\\rm{w}}_1}\n\\]\nSee? Just a big, fat, and simple chain rule problem.\nNow we can tackle finding the gradients for \\(w_1\\). To do so, let’s find the gradients of each intermediate variable.3 4\n\\[\n\\begin{align*}\n\\text{gradient of } u_4 &= \\frac{\\partial u_5}{\\partial u_4} &&= \\frac{\\partial}{\\partial u_4} u_4^2 &&&= 2u_4 \\\\\n\\text{gradient of } u_3 &= \\frac{\\partial u_4}{\\partial u_3} &&= \\frac{\\partial}{\\partial u_3} \\vec{\\rm{y}}_i - u_3 &&&= -1 \\\\\n\\text{gradient of } u_2 &= \\frac{\\partial u_3}{\\partial u_2} &&= \\frac{\\partial}{\\partial u_2} u_2 \\cdot \\vec{\\rm{w}}_2 + b_2 &&&= \\vec{\\rm{w}}^T_2 \\\\\n\\text{gradient of } u_1 &= \\frac{\\partial u_2}{\\partial u_1} &&= \\frac{\\partial}{\\partial u_1} \\text{max}(0, u_1) &&&=\n\\begin{cases}\n0 & u_1 ≤ 0 \\\\\n1 & u_1 &gt; 0\n\\end{cases} \\\\\n\\text{gradient of } \\vec{\\rm{w}}_1 &= \\frac{\\partial u_1}{\\partial \\vec{\\rm{w}}_1} &&= \\frac{\\partial}{\\partial w_1} \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 &&&= \\vec{\\rm{x}}^T_i\n\\end{align*}\n\\]\nNow we multiply everything together.\n\\[\n\\frac{\\partial \\text{MSE}}{\\partial \\vec{\\rm{w}}_1} = \\frac{\\partial u_5}{\\partial \\vec{\\rm{w}}_1} = \\frac{1}{N} \\sum^N_{i=1} (2u_4) \\cdot (-1) \\cdot \\left(\\vec{\\rm{w}}^T_2\\right) \\cdot \\left(\\begin{cases} 0 & u_1 ≤ 0 \\\\ 1 & u_1 &gt; 0 \\end{cases}\\right) \\cdot \\left(\\vec{\\rm{x}}^T_i\\right)\n\\]\nAnd it all eventually expands out to the following.\n\\[\n\\frac{\\partial \\text{MSE}}{\\partial \\vec{\\rm{w}}_1} =\n\\begin{cases}\n  0 & \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 ≤ 0 \\\\\n  \\frac{1}{N} \\sum^N_{i=1} -2(\\vec{\\rm{y}}_i - \\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2) \\cdot \\vec{\\rm{w}}^T_2 \\cdot\\vec{\\rm{x}}_i^T & \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 &gt; 0\n\\end{cases}\n\\]\nWe can further simplify by taking \\(-1\\) and \\(2\\) common.\n\\[\n\\frac{\\partial \\text{MSE}}{\\partial \\vec{\\rm{w}}_1} =\n\\begin{cases}\n  0 & \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 ≤ 0 \\\\\n  \\frac{2}{N} \\sum^N_{i=1} (\\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2 - \\vec{\\rm{y}}_i) \\cdot \\vec{\\rm{w}}^T_2 \\cdot \\vec{\\rm{x}}_i^T & \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 &gt; 0\n\\end{cases}\n\\]\nWe can simplify even further, by letting \\(e_i = \\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2 - \\vec{\\rm{y}}_i\\). The \\(e\\) stands for “error”.\n\\[\n\\frac{\\partial \\text{MSE}}{\\partial \\vec{\\rm{w}}_1} =\n\\begin{cases}\n  0 & \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 ≤ 0 \\\\\n  \\frac{2}{N} \\sum^N_{i=1} e_i \\cdot \\vec{\\rm{w}}^T_2 \\cdot \\vec{\\rm{x}}_i^T & \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 &gt; 0\n\\end{cases}\n\\]\nAnd there you go! We’ve derived the formula that will allow us to calculate the gradients of \\(\\vec{\\rm{w}}_1\\).\nWhen implementing backpropagation in a program, it is often better to implement the entire equation in pieces, as opposed to a single line of code, through storing the result of each intermediate gradient. These intermediate gradients can be reused to calculate the gradients of another variable, such as the bias \\(b_1\\).\nInstead of implementing the following in a single line of code.\n\\[\n\\frac{\\partial u_5}{\\partial u_4} \\cdot \\frac{\\partial u_4}{\\partial u_3} \\cdot \\frac{\\partial u_3}{\\partial u_2} \\cdot \\frac{\\partial u_2}{\\partial u_1} \\cdot \\frac{\\partial u_1}{\\partial \\vec{w}_1}\n\\]\nWe can instead first calculate the gradients of \\(u_4\\).\n\\[\nu_{4_g} = \\frac{\\partial u_5}{\\partial u_4}\n\\]\nThen calculate the gradients of \\(u_3\\) and multiply it with it with the gradients of \\(u_4\\).\n\\[\nu_{3_g} = u_{4_g} \\cdot \\frac{\\partial u_4}{\\partial u_3} = \\left(\\frac{\\partial u_5}{\\partial u_4}\\right) \\cdot \\frac{\\partial u_4}{\\partial u_3}\n\\]\nThen multiply the product above with the gradients of \\(u_2\\).\n\\[\nu_{2_g} = u_{3_g} \\cdot \\frac{\\partial u_3}{\\partial u_2} = \\left(\\frac{\\partial u_5}{\\partial u_4} \\cdot \\frac{\\partial u_4}{\\partial u_3}\\right) \\cdot \\frac{\\partial u_3}{\\partial u_2}\n\\]\nThen multiply the product above with the gradients of \\(u_1\\).\n\\[\nu_{1_g} = u_{2_g} \\cdot \\frac{\\partial u_2}{\\partial u_1} = \\left(\\frac{\\partial u_5}{\\partial u_4} \\cdot \\frac{\\partial u_4}{\\partial u_3} \\cdot \\frac{\\partial u_3}{\\partial u_2}\\right) \\cdot \\frac{\\partial u_2}{\\partial u_1}\n\\]\nAnd finally multiply the product above with the gradients of \\(\\vec{\\rm{w}}_1\\)\n\\[\n\\vec{\\rm{w}}_{1_g} = u_{1_g} \\cdot \\frac{\\partial u_1}{\\partial \\vec{w}_1} = \\left(\\frac{\\partial u_5}{\\partial u_4} \\cdot \\frac{\\partial u_4}{\\partial u_3} \\cdot \\frac{\\partial u_3}{\\partial u_2} \\cdot \\frac{\\partial u_2}{\\partial u_1}\\right) \\cdot \\frac{\\partial u_1}{\\partial \\vec{w}_1}\n\\]\nLet’s see this using Python instead.\nThe following is our neural network.\nFirst we need to calculate the gradients of \\(u_4\\).\nNext are the gradients of \\(u_3\\)\nThen the gradients of \\(u_2\\)\nThen the gradients of \\(u_1\\)\nAnd finally the gradients of \\(\\vec{\\rm{w}}_1\\).\nThe equation for the gradient of \\(b_1\\) is almost the same as the equation for the gradients of \\(w_1\\), save for the last line where we do not have to matrix multiply with \\(\\vec{\\rm{x}}_i\\). Therefore, we can reuse all previous gradient calculations to find the gradient of \\(b_1\\)."
  },
  {
    "objectID": "forblog/posts/18_backprop_from_scratch.html#conclusion",
    "href": "forblog/posts/18_backprop_from_scratch.html#conclusion",
    "title": "Backpropagation Explained using English Words*",
    "section": "Conclusion",
    "text": "Conclusion\nAnd that’s all there really is to backpropagation; think of it a one big chain rule problem.\nTo make sure you’ve got it hammered down, get out a pen and paper and derivate the equations that would compute the gradients of \\(\\vec{\\rm{x}}_i\\), \\(b_1\\), \\(\\vec{\\rm{w}}_2\\), and \\(u_2\\) respectively with respect to the MSE.\nAnd if you really want to hammer down your understanding on what’s happening, then I highly recommend reading The Matrix Calculus You Need For Deep Learning. I’ve also compiled backpropagation practice questions from this paper!\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\\[\n\\begin{align}\n  \\frac{\\partial \\text{MSE}}{\\partial b_1} &=\n    \\begin{cases}\n      0 & \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 ≤ 0 \\\\\n      \\frac{2}{N} \\sum^N_{i=1} (\\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2 - \\vec{\\rm{y}}_i) \\cdot \\vec{\\rm{w}}_2^T & \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 &gt; 0\n    \\end{cases} \\\\\n  \\frac{\\partial \\text{MSE}}{\\partial \\vec{\\rm{x}}_i} &=\n    \\begin{cases}\n      0 & \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 ≤ 0 \\\\\n      \\frac{2}{N} \\sum^N_{i=1} (\\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2 - \\vec{\\rm{y}}_i) \\cdot \\vec{\\rm{w}}^T_2 \\cdot \\vec{\\rm{w}}_1^T & \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1 &gt; 0\n    \\end{cases} \\\\\n  \\frac{\\partial \\text{MSE}}{\\partial \\vec{\\rm{w}}_2} &= \\frac{2}{N} \\sum^N_{i=1} (\\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2 - \\vec{\\rm{y}}_i) \\cdot \\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\\\\n  \\frac{\\partial \\text{MSE}}{\\partial b_2} &= \\frac{2}{N} \\sum^N_{i=1} \\text{max}(0, \\vec{\\rm{x}}_i \\cdot \\vec{\\rm{w}}_1 + b_1) \\cdot \\vec{\\rm{w}}_2 + b_2 - \\vec{\\rm{y}}_i\n\\end{align}\n\\]\n\n\n\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/4_data_quality_is_important.html",
    "href": "forblog/posts/4_data_quality_is_important.html",
    "title": "Data Quality is Important | Car Classifier",
    "section": "",
    "text": "This article was updated on Thursday, 10 November 2022.\n\nI recently created a car classifier that classified cars into their respective brands.\nDespite having almost 5000 images in my training set, I ended up trying out over a hundred layers in my model, and twenty epochs. Even then, I had an error rate of 17.4%.\nThe culprit? My dataset.\nI scraped 5000 images of cars (500 for each company) from DuckDuckGo. Naturally, as expected, the data quality is not so good.\nWhy? Below are some potential reasons:\n\nNoncar images present in dataset\nCars of incorrect company present in dataset\nF1 cars present in dataset\nA large variety of cars from different time periods present in dataset\nDifferent companys’ cars look similar\nModded cars present in dataset\nConcept cars present in dataset\nMultiple cars present in a single image\nCertain angles of cars appear more than others\nCars appear in certain backgrounds more than others\nThe search term {car_brand} car could be skewing results\n\nI could have absolutely achieved better results with fewer layers and fewer epochs if I trained the model on better quality data — or manually combed through the 5000 images 💀. However, I did use fastai’s GUI for data cleaning. This GUI sorts images by their loss which helps to determine if certain images should be relabeled or deleted.\nBelow is the confusion matrix for this model.\n\nIt can be seen that this model “confuses” between quite a few different brands: Ford and Chevrolet, Chevrolet and Ford, Jaguar and Aston Martin, Renault and Ford.\nBut why is data quality important? Because without good data, the model will not be able to “see” things the way they actually are, and in turn end up making worse predictions and not generalize to other data.\nLet’s say you did not know how, say, a toaster looked like. So I taught you by showing you pictures of a kettle. Then to test you, I showed you a set of pictures depicting various kitchen appliances and told you to find the toaster. You would not be able to.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtending upon this example, say I showed you toasters only from the last two years and from two brands only. You would not be able to identify toasters older than two years, and toasters from other brands to much success.\nObviously, humans are smarter and can infer. AI methods can only infer to a certain degree, mainly based on what is in their dataset. This talk does start to become more philosophical.\nThe point of this post is to emphasize the importance of data quality and different aspects to consider as to why data quality may not be good. You can have the best architecture in the world, but it is useless if you do not have good data.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!\n\n\n\n Back to top"
  },
  {
    "objectID": "forblog/posts/22_how_deepseek_r1_was_trained.html#in-a-nutshell",
    "href": "forblog/posts/22_how_deepseek_r1_was_trained.html#in-a-nutshell",
    "title": "How Deepseek R1 was trained.",
    "section": "In a Nutshell",
    "text": "In a Nutshell\n\nR1-Zero is a model created purely through reinforcement learning (RL), with Group Relative Policy Optimization (GRPO) as the policy.\nR1 is a model created with a combination of multiple RL and supervised finetuning (SFT) stages."
  },
  {
    "objectID": "forblog/posts/22_how_deepseek_r1_was_trained.html#r1-zero-the-pure-rl-approach",
    "href": "forblog/posts/22_how_deepseek_r1_was_trained.html#r1-zero-the-pure-rl-approach",
    "title": "How Deepseek R1 was trained.",
    "section": "R1-Zero: The Pure RL Approach",
    "text": "R1-Zero: The Pure RL Approach\nR1-Zero was a model developed exclusively through RL using GRPO as the policy. A rule based reward system was utilized with the following two rewards:\n\nAccuracy rewards for factual correctness of responses.\nFormat rewards for pushing the model to put its thinking between &lt;think&gt; and &lt;/think&gt; tags.\n\nThe neural reward model1 was left out so as to eliminate reward hacking.\n1 A neural reward model is a neural network trained to predict human preferences or desirability of different actions or states, providing a numerical reward signal that guides RL agents to align with human values. It learns from human feedback, such as comparisons or rankings of different outcomes, to estimate rewards for the RL agent.In addition, a prompt template was used that required the model to first produce the reasoning process, after which, it could produce the answer:\nA conversation between User and Assistant. The user asks a question, and the Assistant solves it.\nThe assistant first thinks about the reasoning process in the mind and then provides the user\nwith the answer. The reasoning process and answer are enclosed within &lt;think&gt; &lt;/think&gt; and\n&lt;answer&gt; &lt;/answer&gt; tags, respectively, i.e., &lt;think&gt; reasoning process here &lt;/think&gt;\n&lt;answer&gt; answer here &lt;/answer&gt;. User: prompt. Assistant:\n\nIssues\nThere were certain issues that arose from a pure RL approach:\n\nThe output was not very readable.\nThere was language mixing."
  },
  {
    "objectID": "forblog/posts/22_how_deepseek_r1_was_trained.html#r1-the-hybrid-training-pipeline",
    "href": "forblog/posts/22_how_deepseek_r1_was_trained.html#r1-the-hybrid-training-pipeline",
    "title": "How Deepseek R1 was trained.",
    "section": "R1: The Hybrid Training Pipeline",
    "text": "R1: The Hybrid Training Pipeline\nIn order to remedy these issues, a multi-stage training pipeline was introduced.\n\n\n\n\n\nflowchart LR\n  A[SFT]--&gt;B[RL]--&gt;C[SFT]--&gt;D[RL]\n  style A fill:#b3e6ff,stroke:#333\n  style C fill:#b3e6ff,stroke:#333\n  style B fill:#ffb3b3,stroke:#333\n  style D fill:#ffb3b3,stroke:#333\n\n\n\n\n\n\nThis process arose from wondering whether performance could be improved, convergence accelerated, or a coherent chain-of-thought (CoT) be produced through a small amount of high quality data.\n\nStage 1: Cold-start SFT\nThis stage is performed to prevent the unstable cold start phase of RL. The tuning is done through a small amount of CoT data, which is collected by:\n\nFew-shot prompting with a long CoT example.\nDirect requests for detailed answers.\nGathering human refined R1-Zero outputs in a readable format.\n\nThis stage allows for reduced language mixing, improved markdown formatting, and enhanced reader friendly outputs.\n\n\n\n\n\n\nAs I was reading this paper, it reminded how I previously read about iterative training processes. Such processes typically result in a better model.\n\n\n\n\n\nStage 2: Reasoning-oriented RL\nThe goal of this RL stage is on enhancing reasoning capabilities.\nA prominent issue found during the training of R1-Zero was language mixing. To alleviate this, a language consistency reward is introduced that is calculated as a proportion of the target language words in the CoT. The introduction of this reward results in a small performance decrease, but results in a significant human readability gain.\nThe final reward in the RL pipeline was produced by summing the accuracy of the reasoning task with the language consistency reward.\n\n\nStage 3: Rejection Sampling SFT\nSFT is performed once more. In this stage, the checkpoint from the previous round is used to collect data for this stage. Rather than fully focusing on reasoning data like in the first stage, data from other domains is utilized to enhance capabilities in writing, role playing, and other general purpose tasks. In total, an additional 800k samples were collected.\n\n\n\n\n\n\n\n\n\nData Type\nSamples\nSource\nFilter Criteria\n\n\n\n\nReasoning\n600k\nCurated prompts + sampling\nRemove mixed-language/code CoT\n\n\nGeneral Purpose\n200k\nDeepSeek V3 + CoT generation\nSimple queries without CoT\n\n\n\n\n\nStage 4: RL Alignment\nIn this stage, RL was performed once more to futher align with human preferences, and to have better reasoning.\nTo improve helpfulness, the final answer quality was evaluated, with an emphasis on utility and relevance. Focus was placed exclusively on the resulting answer so as to minimize interference with the reasoning process. Harmlessness was improved by analyzing the entire response (including CoT) to identify and mitigate any risks, biases, and harmful content.\nThe RL pipeline in this stage integrated a multireward models, diverse prompt distributions, and was also based on the DeepSeek V3 pipeline."
  },
  {
    "objectID": "forblog/posts/22_how_deepseek_r1_was_trained.html#conclusion",
    "href": "forblog/posts/22_how_deepseek_r1_was_trained.html#conclusion",
    "title": "How Deepseek R1 was trained.",
    "section": "Conclusion",
    "text": "Conclusion\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!\nRead the Deepseek R1 paper here.."
  },
  {
    "objectID": "forblog/posts/14_iterators_and_generators.html",
    "href": "forblog/posts/14_iterators_and_generators.html",
    "title": "Iterators and Generators",
    "section": "",
    "text": "This notebook follows the fastai style guide."
  },
  {
    "objectID": "forblog/posts/14_iterators_and_generators.html#iter",
    "href": "forblog/posts/14_iterators_and_generators.html#iter",
    "title": "Iterators and Generators",
    "section": "iter",
    "text": "iter\niter creates what’s known as an iterator. It is a type of iterable.\nAn iterable is anything that can be looped through (e.g., a list or a string).\niter essentially allows you to loop through an iterable without using a for loop. It gives you finer and more granuler control over when you loop, and how how much you loop.\n\n\n\n\nDocstring:\n\niter(iterable) -&gt; iterator\n\niter(callable, sentinel) -&gt; iterator\n\n\n\nGet an iterator from an object.  In the first form, the argument must\n\nsupply its own iterator, or be a sequence.\n\nIn the second form, the callable is called until it returns the sentinel.\n\nType:      builtin_function_or_method\n\n\n\nl = list(range(10)); l\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\nit = iter(l); it\n\n&lt;list_iterator at 0x11e29a6e0&gt;\n\n\n\nnext(it)\n\n0\n\n\n\nnext(it)\n\n1\n\n\n\nnext(it)\n\n2"
  },
  {
    "objectID": "forblog/posts/14_iterators_and_generators.html#islice",
    "href": "forblog/posts/14_iterators_and_generators.html#islice",
    "title": "Iterators and Generators",
    "section": "islice",
    "text": "islice\nislice is a type of iterator that returns \\(x\\) items from an iterable at a time.\n\n\n\n\nInit signature: islice(self, /, *args, **kwargs)\n\nDocstring:     \n\nislice(iterable, stop) --&gt; islice object\n\nislice(iterable, start, stop[, step]) --&gt; islice object\n\n\n\nReturn an iterator whose next() method returns selected values from an\n\niterable.  If start is specified, will skip all preceding elements;\n\notherwise, start defaults to zero.  Step defaults to one.  If\n\nspecified as another value, step determines how many values are\n\nskipped between successive calls.  Works like a slice() on a list\n\nbut returns an iterator.\n\nType:           type\n\nSubclasses:     \n\n\n\nfrom itertools import islice\nit = iter(l)\nlist(islice(it, 5))\n\n[0, 1, 2, 3, 4]\n\n\n\nlist(islice(it, 5))\n\n[5, 6, 7, 8, 9]\n\n\n\nlist(islice(it, 5))\n\n[]"
  },
  {
    "objectID": "forblog/posts/14_iterators_and_generators.html#yield",
    "href": "forblog/posts/14_iterators_and_generators.html#yield",
    "title": "Iterators and Generators",
    "section": "yield",
    "text": "yield\nyield is a substitute for return in a function or method. When yield is used, the function is known as a generator.\nyield essentially allows you to perform multiple returns, and also allows you to treat a function as an iterator.\n\nMultiple Returns\nTo demonstrate multiple returns, let’s create a function that chops a list up into smaller lists.\n\ndef chunks(l, step):\n    for i in range(0, len(l), step): yield l[i:i+step]\n\n\nlist(chunks(l, 5))\n\n[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n\n\n\n\nFunction as an Iterator\n\nl_iter = chunks(l, 5); l_iter\n\n&lt;generator object chunks at 0x11e2a8cf0&gt;\n\n\n\nnext(l_iter)\n\n[0, 1, 2, 3, 4]\n\n\n\nnext(l_iter)\n\n[5, 6, 7, 8, 9]\n\n\n\nnext(l_iter)\n\n\n---------------------------------------------------------------------------\nStopIteration                             Traceback (most recent call last)\nCell In[17], line 1\n----&gt; 1 next(l_iter)\n\nStopIteration: \n\n\n\n\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/9_transformers_explained.html#at-a-high-view",
    "href": "forblog/posts/9_transformers_explained.html#at-a-high-view",
    "title": "Transformers, Simply Explained",
    "section": "At a High View",
    "text": "At a High View\nTransformers are all the rage right now. They’re what’s powering the current wave of chat bots. Here’s a high level view of how transformers work, so you know how these bots really work.\nSimply put, a transformer is a type of architecture used for Natural Language Processing (NLP) tasks that either fills-in-the-blanks or autocompletes.\nTransformers consist of either an encoder, decoder, or both. Encoders and decoders contain attention layers.\nLanguage models need numbers to work. Text is given a numerical representation after breaking it down into smaller pieces. To keep this explanation simple, these pieces are words.\nThe numerical representation given to a word describes the word itself and its relation to the surrounding words."
  },
  {
    "objectID": "forblog/posts/9_transformers_explained.html#encoders",
    "href": "forblog/posts/9_transformers_explained.html#encoders",
    "title": "Transformers, Simply Explained",
    "section": "Encoders",
    "text": "Encoders\nEncoder-only transformers are good for “understanding” text, such as classifying sentences by sentiment or figuring out what parts of a sentence refers, for example, to a person or location.\nWhen training encoders, words are given a numerical representation by the attention layers considering adjacent words. For example, let’s say we have the sentence, “I am really hungry.”. The attention layers consider the words ‘am’ and ‘hungry’ when giving the word ‘really’ a numerical representation.\nThe goal of training encoders is to predict words omitted from text (e.g., “I … really hungry.”). This is how encoders can “understand” text."
  },
  {
    "objectID": "forblog/posts/9_transformers_explained.html#decoders",
    "href": "forblog/posts/9_transformers_explained.html#decoders",
    "title": "Transformers, Simply Explained",
    "section": "Decoders",
    "text": "Decoders\nDecoder-only transformers are good for text generation. An example is the autocomplete feature on a smartphone’s keyboard.\nDecoders similary give text a numerical representation, except that the attention layers consider only the previous words. When giving ‘am’ a numerical representation from “I am hungry.”, the attention layers will only consider the word ‘I’. When giving ‘hungry’ a numerical representation, the attention layers will consider the words ‘I’ and ‘am’.\nThe goal of training decoders is to predict the most likely word to continue a piece of text (e.g., “I am ….”). All generated words are used in conjunction to generate the next word."
  },
  {
    "objectID": "forblog/posts/9_transformers_explained.html#encoders-and-decoders",
    "href": "forblog/posts/9_transformers_explained.html#encoders-and-decoders",
    "title": "Transformers, Simply Explained",
    "section": "Encoders and Decoders",
    "text": "Encoders and Decoders\nTransformers that use both encoders and decoders are known as encoder-decoder models or sequence-to-sequence models. Such models are good for translation and summarization.\nEncoder-decoder models are trained by first letting the encoder give the input text a numerical representation. Next, this representation is input to the decoder which generates text as described above. The encoder part of the model provides the “understanding”, while the decoder part of the model generates based off of this “understanding”."
  },
  {
    "objectID": "forblog/posts/9_transformers_explained.html#closing-words",
    "href": "forblog/posts/9_transformers_explained.html#closing-words",
    "title": "Transformers, Simply Explained",
    "section": "Closing Words",
    "text": "Closing Words\nAnd there you have it! It’s as simple as that!\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html",
    "href": "forblog/posts/2_bear_classifier_model.html",
    "title": "My first AI model",
    "section": "",
    "text": "This article was updated on Tuesday, 1 November 2022."
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#introduction",
    "href": "forblog/posts/2_bear_classifier_model.html#introduction",
    "title": "My first AI model",
    "section": "Introduction",
    "text": "Introduction\nThis is my first attempt at creating an AI model: an image classifier. This classifier can tell whether a grizzly bear, black bear, or teddy bear is in an image.\nYou can visit the classifier here to test it out for yourself!"
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#load-libraries",
    "href": "forblog/posts/2_bear_classifier_model.html#load-libraries",
    "title": "My first AI model",
    "section": "Load libraries",
    "text": "Load libraries\n\n# No need to fret! fastai is specifically designed to be used with import *.\nfrom fastbook import *\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#download-image-files",
    "href": "forblog/posts/2_bear_classifier_model.html#download-image-files",
    "title": "My first AI model",
    "section": "Download image files",
    "text": "Download image files\nSpecify the bear images we wish to download.\n\nbear_types = ('grizzly', 'black', 'teddy',)\npath = Path('bears')\n\nDownload 200 of each bear (search_images_ddg defaults to 200 URLs) and assign them to a specific directory.\n\nif not path.exists():\n    path.mkdir()\n    for bear_type in bear_types:\n        destination = (path / bear_type)\n        destination.mkdir(exist_ok=True)\n        urls = search_images_ddg(f\"{bear_type} bear\")\n        download_iamges(destination, urls=urls)\n\nCheck if our folder has the image files.\n\nfns = get_image_files(path)\nfns\n\n(#802) [Path('bears/grizzly/00000238.jpg'),Path('bears/grizzly/00000047.jpg'),Path('bears/grizzly/00000199.jpg'),Path('bears/grizzly/00000237.jpg'),Path('bears/grizzly/00000055.jpg'),Path('bears/grizzly/00000000.png'),Path('bears/grizzly/00000235.jpg'),Path('bears/grizzly/00000159.jpg'),Path('bears/grizzly/00000268.jpg'),Path('bears/grizzly/00000266.jpg')...]\n\n\nCheck for corrupt images.\n\ncorrupt_images = verify_images(fns)\ncorrupt_images\n\n(#0) []\n\n\nRemove corrupt images.\n\ncorrupt_images.map(pathlib.Path.unlink)\n\n(#0) []"
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#load-image-files",
    "href": "forblog/posts/2_bear_classifier_model.html#load-image-files",
    "title": "My first AI model",
    "section": "Load image files",
    "text": "Load image files\nThe DataBlock API for creates the necessary DataLoaders for us.\n\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128),\n)\n\nThe blocks parameter allows us to specify the independent and dependent variables.\nThe get_items parameter tells fastai how to obtain our data. We use the get_image_files function to obtain our images.\nThe splitter parameter allows us to tell fastai how to split our data into training and validation sets. Since our data is one big set, we use the RandomSplitter class and tell it to use 20% of our data as the validation set. We specify a seed so the same split occurs each time.\nThe get_y parameter obtains our labels. The parent_label function simply gets the name of the folder a file is in. Since we have organized our bear images into different folders, this will nicely handle our target labels.\nThe item_tfms parameter allows us to specify a transform to apply to our data. Since we want all our images to be of the same size, we use the Resize() class.\nWe now have a DataBlock object from which can load the data.\n\ndataloaders = bears.dataloaders(path)\n\nLet us view a few images in the validation set.\n\ndataloaders.valid.show_batch(max_n=4, nrows=1)"
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#data-augmentation",
    "href": "forblog/posts/2_bear_classifier_model.html#data-augmentation",
    "title": "My first AI model",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nData augmentation refers to creating random variations to our input data. This produces new data points based on the existing data points. This allows each data point to look different, without changing their meaning.\nTypical examples of image augmentation include rotation, flipping, perspective warping, brightness changing, and contrast changing.\n\nCropping\nThe validation set images shown above are cropped. We achieved this by specifying the Resize argument when defining the DataBlock. Resize crops images to the size specified.\nCropping results in detail being lost.\nAlternatively, we can squish or stretch images, or pad them to a desired size.\n\n\nSquishing/Stretching\nThe problem with squishing or stretching images is that the model will learn to “see” images the way they are not supposed to be.\n\nbears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish))\ndataloaders = bears.dataloaders(path)\ndataloaders.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n\n\n\n\n\nPadding\nBy padding, the image is surrounded typically by black, meaningless pixels. This results in extra, wasted computation.\n\nbears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeros'))\ndataloaders = bears.dataloaders(path)\ndataloaders.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n\n\n\nThe best approach is to take random crops of different parts of the same image. This makes sure that the model does not miss out on any details whilst letting it “know” how an object fully looks like.\nBelow, we have unique=True so that the same image is repeated with different variations.\n\nbears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3))\ndataloaders = bears.dataloaders(path)\ndataloaders.train.show_batch(max_n=4, nrows=1, unique=True)\n\n\n\n\n\n\n\n\nfastai comes with a function that applies a variety of augmentations to images. This can allow a model to “see” and recognize images in a variety of scenarios.\n\nbears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2))\ndataloaders  = bears.dataloaders(path)\ndataloaders.train.show_batch(max_n=8, nrows=2, unique=True)\n\n\n\n\n\n\n\n\nI have not used RandomResizedCrop here so that the different augmentations can be seen more clearly. RandomResizedCrop will be used when the model is trained.\nbatch_tfms tells fastai that we want to use these transforms on a batch."
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#training-the-model",
    "href": "forblog/posts/2_bear_classifier_model.html#training-the-model",
    "title": "My first AI model",
    "section": "Training the model",
    "text": "Training the model\nWe do not have a lot of data. Only 200 images of each bear at most. Therefore, we will augment our images not only to get more data, but so that the model can recognize data in a variety of situations.\n\nbears = bears.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms(),\n)\ndataloaders = bears.dataloaders(path)\n\nWe will now create our learner and fine-tune it.\nWe will be using the ResNet18 architecture (which is a convolutional neural network, or CNN for short). Error rate will be the metric.\n\nlearn = cnn_learner(dataloaders, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.985666\n0.104632\n0.025000\n00:20\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.132230\n0.073527\n0.012500\n00:22\n\n\n1\n0.106222\n0.054833\n0.018750\n00:22\n\n\n2\n0.087129\n0.058497\n0.012500\n00:20\n\n\n3\n0.069890\n0.058845\n0.018750\n00:19\n\n\n\n\n\nOur model only has a 1.9% error rate! Not bad! Though it seems if I had done an extra epoch, the error rate may have gone down to 1.3%, judging by the previous epochs’ error rates."
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#visualizing-mistakes",
    "href": "forblog/posts/2_bear_classifier_model.html#visualizing-mistakes",
    "title": "My first AI model",
    "section": "Visualizing mistakes",
    "text": "Visualizing mistakes\nWe can visualize the mistakes the model is making by a confusion matrix.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n3 grizzly bears were misclassified as black bears.\nLet us see where the errors are occurring, so we can determine if they are due to a dataset problem or a model problem.\nTo do this, we will sort images by their loss.\n\ninterp.plot_top_losses(5, nrows=1)"
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#data-cleaning",
    "href": "forblog/posts/2_bear_classifier_model.html#data-cleaning",
    "title": "My first AI model",
    "section": "Data cleaning",
    "text": "Data cleaning\nThe intuitive approach to data cleaning is to do it before training the model. However, a trained model can help us clean the data. For example, we can see some mislabaled bears in the above cases.\nfastai includes a GUI for data cleaning. This GUI allows you to choose a category/label and its associated training and validation sets. It then shows you images in order of highest-loss first, from which you can select images for removal or relabeling.\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\nImageClassifierCleaner does not actually delete or relabel. It just returns the indices that are to be deleted or relabeled.\n\n# Delete images selected for deletion.\nfor index in cleaner.delete():\n    cleaner.fns[index].unlink()\n\n# Relabel images selected for relabeling.\nfor index, category in cleaner.change():\n    shutil.move(str(cleaner.fns[index]), path/category)\n\nWe can now retrain and better performance should be expected."
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#saving-the-model",
    "href": "forblog/posts/2_bear_classifier_model.html#saving-the-model",
    "title": "My first AI model",
    "section": "Saving the model",
    "text": "Saving the model\nA model consists of two parts: the architecture and the parameters.\nWhen we use the export() method, both of these are saved.\nThis method also saves the definition of our DataLoaders. This is done so that we do not have to redefine how to transform our data when the model is used in production.\nfastai uses our validation set DataLoader by default, so the data augmentation will not be applied, which is generally what is wanted.\nThe export() method creates a file named “export.pkl”.\n\nlearn.export()\n\nLet us check that the file exists.\n\npath = Path()\npath.ls(file_exts='.pkl')\n\n(#1) [Path('export.pkl')]\n\n\nIf you wish to deploy an app, this is the file you will need."
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#loading-the-model-for-inference",
    "href": "forblog/posts/2_bear_classifier_model.html#loading-the-model-for-inference",
    "title": "My first AI model",
    "section": "Loading the model for inference",
    "text": "Loading the model for inference\nNow obviously we do not need to load the model as we already have the learner variable. But I shall do so anyways.\n\nlearn_inf = load_learner(path/'export.pkl')\n\nWe generally do inference for a single image at a time.\n\nlearn_inf.predict('images/grizzly.jpg')\n\n\n\n\n('grizzly', TensorBase(1), TensorBase([1.4230e-06, 1.0000e+00, 3.9502e-08]))\n\n\nThree things have been returned: the predicted category, the index of the predicted category, and the probabilities of each category.\nThe order of each category is based on the order of the vocabulary of the DataLoaders; that is, the stored tuple of all possible categories.\nThe DataLoaders can be accessed as an attribute of the Learner.\n\nlearn_inf.dataloaders.vocab\n\n['black', 'grizzly', 'teddy']"
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#why-cnns-work-so-well",
    "href": "forblog/posts/2_bear_classifier_model.html#why-cnns-work-so-well",
    "title": "My first AI model",
    "section": "Why CNNs work so well",
    "text": "Why CNNs work so well\nThe ResNet18 architecture is a sort of CNN. Below is my understanding as to why CNNs work so well.\nA neural network is comprised of many layers. Each layer is comprised of many neurons. In a CNN, each neuron in the same layer is given the exact same weights, while being given different input data. This allows all neurons in a layer to fire upon detecting the same pattern.\nBecause of this, CNNs can become really good at detecting objects in various patterns, orientations, shapes, positions, and so on."
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#conclusion",
    "href": "forblog/posts/2_bear_classifier_model.html#conclusion",
    "title": "My first AI model",
    "section": "Conclusion",
    "text": "Conclusion\nWell then, that wraps up my first deep learning model! I have to say, it is much easier than I thought it would be to implement a model. You do not need to go into the nitty gritty details of artificial intelligence. A high level understanding can suffice in the beginning. It is like playing a sport: you do not need to understand the physics to be able to play it.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "feedback.html",
    "href": "feedback.html",
    "title": "Site Feedback",
    "section": "",
    "text": "Loading…"
  },
  {
    "objectID": "patch_notes.html",
    "href": "patch_notes.html",
    "title": "Site Patch Notes",
    "section": "",
    "text": "Detailed patch notes are unavailable prior to site version 2.0.0.0."
  },
  {
    "objectID": "patch_notes.html#version-2.2.2.0-28-december-2024",
    "href": "patch_notes.html#version-2.2.2.0-28-december-2024",
    "title": "Site Patch Notes",
    "section": "Version 2.2.2.0 | 28 December 2024",
    "text": "Version 2.2.2.0 | 28 December 2024\n\nLaunched Bits and Bobs.\nUnlinked App Playground.\nReplaced App Playground with Bits and Bobs on landing page."
  },
  {
    "objectID": "patch_notes.html#version-2.2.1.6-27-december-2024",
    "href": "patch_notes.html#version-2.2.1.6-27-december-2024",
    "title": "Site Patch Notes",
    "section": "Version 2.2.1.6 | 27 December 2024",
    "text": "Version 2.2.1.6 | 27 December 2024\n\nUpdated copyright notices for 2025 and beyond."
  },
  {
    "objectID": "patch_notes.html#version-2.2.2.0-26-may-2024",
    "href": "patch_notes.html#version-2.2.2.0-26-may-2024",
    "title": "Site Patch Notes",
    "section": "Version 2.2.2.0 | 26 May 2024",
    "text": "Version 2.2.2.0 | 26 May 2024\n\nRemoved buttons.\nTweaked footer text."
  },
  {
    "objectID": "patch_notes.html#version-2.2.1.4-23-may-2024",
    "href": "patch_notes.html#version-2.2.1.4-23-may-2024",
    "title": "Site Patch Notes",
    "section": "Version 2.2.1.4 | 23 May 2024",
    "text": "Version 2.2.1.4 | 23 May 2024\n\nFixed Mermaid diagram rendering issue."
  },
  {
    "objectID": "patch_notes.html#version-2.2.1.3-1-january-2024",
    "href": "patch_notes.html#version-2.2.1.3-1-january-2024",
    "title": "Site Patch Notes",
    "section": "Version 2.2.1.3 | 1 January 2024",
    "text": "Version 2.2.1.3 | 1 January 2024\n\nUpdated copyright notices for 2024."
  },
  {
    "objectID": "patch_notes.html#version-2.2.1.2-3-august-2023",
    "href": "patch_notes.html#version-2.2.1.2-3-august-2023",
    "title": "Site Patch Notes",
    "section": "Version 2.2.1.2 | 3 August 2023",
    "text": "Version 2.2.1.2 | 3 August 2023\n\nRemoved featured posts section in the ForBlog."
  },
  {
    "objectID": "patch_notes.html#version-2.2.1.1-1-august-2023",
    "href": "patch_notes.html#version-2.2.1.1-1-august-2023",
    "title": "Site Patch Notes",
    "section": "Version 2.2.1.1 | 1 August 2023",
    "text": "Version 2.2.1.1 | 1 August 2023\n\nSplit website footer into 3 sections."
  },
  {
    "objectID": "patch_notes.html#version-2.2.1.0-31-july-2023",
    "href": "patch_notes.html#version-2.2.1.0-31-july-2023",
    "title": "Site Patch Notes",
    "section": "Version 2.2.1.0 | 31 July 2023",
    "text": "Version 2.2.1.0 | 31 July 2023\n\nAdded a featured posts section in the ForBlog.\nIncreased thumbnail size in the App Playground.\nFixed Ship of Theseus link in patch notes."
  },
  {
    "objectID": "patch_notes.html#version-2.2.0.2-15-july-2023",
    "href": "patch_notes.html#version-2.2.0.2-15-july-2023",
    "title": "Site Patch Notes",
    "section": "Version 2.2.0.2 | 15 July 2023",
    "text": "Version 2.2.0.2 | 15 July 2023\n\nRe-enabled App Playground."
  },
  {
    "objectID": "patch_notes.html#version-2.2.0.1-13-may-2023",
    "href": "patch_notes.html#version-2.2.0.1-13-may-2023",
    "title": "Site Patch Notes",
    "section": "Version 2.2.0.1 | 13 May 2023",
    "text": "Version 2.2.0.1 | 13 May 2023\n\nFixed animations for the App Playground.\nFixed back-to-top button covering content on the About Me page."
  },
  {
    "objectID": "patch_notes.html#version-2.2.0.0-13-may-2023",
    "href": "patch_notes.html#version-2.2.0.0-13-may-2023",
    "title": "Site Patch Notes",
    "section": "Version 2.2.0.0 | 13 May 2023",
    "text": "Version 2.2.0.0 | 13 May 2023\n\nImplemented site-wide animations!\nConsolidated unsubscribe form and all subscribe forms into a single form.\nAdded a back-to-top button.\nRSS and source code navbar icons are now positioned more cleanly when accessed from the drop down menu on smaller screens.\nIncreased number of displayed posts on the ForBlog from 5 per page to 7 per page.\nChanged Home navbar icon.\nTemporarily disabled App Playground."
  },
  {
    "objectID": "patch_notes.html#version-2.1.0.1-25-march-2023",
    "href": "patch_notes.html#version-2.1.0.1-25-march-2023",
    "title": "Site Patch Notes",
    "section": "Version 2.1.0.1 | 25 March 2023",
    "text": "Version 2.1.0.1 | 25 March 2023\n\nTweaked the various subscription forms’ positioning."
  },
  {
    "objectID": "patch_notes.html#version-2.1.0.0-23-february-2023",
    "href": "patch_notes.html#version-2.1.0.0-23-february-2023",
    "title": "Site Patch Notes",
    "section": "Version 2.1.0.0 | 23 February 2023",
    "text": "Version 2.1.0.0 | 23 February 2023\n\nLaunched the AI dictionary.\nShortened navbar text."
  },
  {
    "objectID": "patch_notes.html#version-2.0.3.2-22-february-2023",
    "href": "patch_notes.html#version-2.0.3.2-22-february-2023",
    "title": "Site Patch Notes",
    "section": "Version 2.0.3.2 | 22 February 2023",
    "text": "Version 2.0.3.2 | 22 February 2023\n\nUpdated copyright notices for 2023."
  },
  {
    "objectID": "patch_notes.html#version-2.0.3.1-28-january-2023",
    "href": "patch_notes.html#version-2.0.3.1-28-january-2023",
    "title": "Site Patch Notes",
    "section": "Version 2.0.3.1 | 28 January 2023",
    "text": "Version 2.0.3.1 | 28 January 2023\n\nChanged comment section theme."
  },
  {
    "objectID": "patch_notes.html#version-2.0.3.0-27-november-2022",
    "href": "patch_notes.html#version-2.0.3.0-27-november-2022",
    "title": "Site Patch Notes",
    "section": "Version 2.0.3.0 | 27 November 2022",
    "text": "Version 2.0.3.0 | 27 November 2022\n\nFully implemented Twitter Cards."
  },
  {
    "objectID": "patch_notes.html#version-2.0.2.0-26-november-2022",
    "href": "patch_notes.html#version-2.0.2.0-26-november-2022",
    "title": "Site Patch Notes",
    "section": "Version 2.0.2.0 | 26 November 2022",
    "text": "Version 2.0.2.0 | 26 November 2022\n\nFully implemented Open Graph.\nAdded button for direct link to site’s source code.\nTweaked landing page description."
  },
  {
    "objectID": "patch_notes.html#version-2.0.1.2-17-november-2022",
    "href": "patch_notes.html#version-2.0.1.2-17-november-2022",
    "title": "Site Patch Notes",
    "section": "Version 2.0.1.2 | 17 November 2022",
    "text": "Version 2.0.1.2 | 17 November 2022\n\nFixed broken license link."
  },
  {
    "objectID": "patch_notes.html#version-2.0.1.1-17-november-2022",
    "href": "patch_notes.html#version-2.0.1.1-17-november-2022",
    "title": "Site Patch Notes",
    "section": "Version 2.0.1.1 | 17 November 2022",
    "text": "Version 2.0.1.1 | 17 November 2022\n\nFixed broken site feedback link.\nUpdated site version references."
  },
  {
    "objectID": "patch_notes.html#version-2.0.1.0-17-november-2022",
    "href": "patch_notes.html#version-2.0.1.0-17-november-2022",
    "title": "Site Patch Notes",
    "section": "Version 2.0.1.0 | 17 November 2022",
    "text": "Version 2.0.1.0 | 17 November 2022\n\nFixed a bunch of broken links.\nFixed RSS buttons.\nShifted links on the landing page."
  },
  {
    "objectID": "patch_notes.html#version-2.0.0.0-16-november-2022",
    "href": "patch_notes.html#version-2.0.0.0-16-november-2022",
    "title": "Site Patch Notes",
    "section": "Version 2.0.0.0 | 16 November 2022",
    "text": "Version 2.0.0.0 | 16 November 2022\n\nCreated, erm, site patch notes.\nSite is now entirely remade in Quarto.\nUI overhaul.\nForBlog is no longer the main landing page.\nApp playground has been added; a place where I can host my various creations.\n\nNew…\n\nfavicon.\nabout me page.\nlanding page.\nForBlog home page.\nForBlog post layout.\nglobal search bar.\n\nAdded…\n\na ForBlog only search bar.\nForBlog post filters.\nForBlog and App Playground subscriptions.\na form for site feedback.\ncopyright licences.\nnew fancy buttons."
  },
  {
    "objectID": "patch_notes.html#version-1.0.0.0-15-may-2022",
    "href": "patch_notes.html#version-1.0.0.0-15-may-2022",
    "title": "Site Patch Notes",
    "section": "Version 1.0.0.0 | 15 May 2022",
    "text": "Version 1.0.0.0 | 15 May 2022\n\nInitial release.\nSite is built on fastpages, by fastai."
  },
  {
    "objectID": "web_apps/apps/coming_soon.html",
    "href": "web_apps/apps/coming_soon.html",
    "title": "More apps coming soon…",
    "section": "",
    "text": "I never told you how soon…\n\n\n\nThis image was generated by Dall-E 2!\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "web_apps/apps/flood_detector.html",
    "href": "web_apps/apps/flood_detector.html",
    "title": "Flood Classifier",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/optimizer.html",
    "href": "dictionary/terms/optimizer.html",
    "title": "Optimizer",
    "section": "",
    "text": "An optimizer in its most basic form looks like this.\nwith torch.no_grad(): \n  for p in model.parameters(): p -= p.grad * lr \n  model.zero_grad() \n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/forward_pass.html",
    "href": "dictionary/terms/forward_pass.html",
    "title": "Forward Pass",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/gradient_boosting_machine_gbm.html",
    "href": "dictionary/terms/gradient_boosting_machine_gbm.html",
    "title": "Gradient Boosting Machine (GBM)",
    "section": "",
    "text": "The first model produces a prediction.\nThe difference between this prediction and the actual value is obtained.\nThe difference is now set as the target.\nThe next model now attempts to predict this difference.\nRepeat steps 2-4 for as many models as desired.\nSum all obtained differences.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nWhile this technique tends to produce better results, is more likely to overfit. This is because the machine is trying to minimize the difference between the predictions and actual values in the training set.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/latent.html",
    "href": "dictionary/terms/latent.html",
    "title": "Latent (Diffusion)",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/dataset.html",
    "href": "dictionary/terms/dataset.html",
    "title": "Dataset",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/activation_function.html",
    "href": "dictionary/terms/activation_function.html",
    "title": "Activation Function",
    "section": "",
    "text": "If there existed no activation function, there would be no point in having individual neurons, as the entire network would become a single, big linear equation.\n\n\n\n\n\n\nWith Activation Function\n\n\n\n\\(w_2 \\cdot \\text{activation}(w_1 x + b_1) + b_2\\)\n\n\n\n\n\n\n\n\nWithout Activation Function\n\n\n\n\\(w_2 \\cdot (w_1 x + b_1) + b_2 = w_2 w_1 x + w_2 b_1 + b_2\\)\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/one_hot_encoding.html",
    "href": "dictionary/terms/one_hot_encoding.html",
    "title": "One Hot Encoding",
    "section": "",
    "text": "Let’s say we have a categorical feature, speed, that can be either slow or fast. Instead of assigning a value of 0 to slow and a value of 1 to fast, a slow column can be created and a fast column can be created. If the speed is slow, slow is true and fast is false. If the speed is fast, slow is false and fast is true.\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/cross_entropy_loss.html",
    "href": "dictionary/terms/cross_entropy_loss.html",
    "title": "Cross Entropy Loss",
    "section": "",
    "text": "Let’s say that we have a model that tells us what sort of vehicle is in a picture. It outputs the following predictions.\n\n\n\n\n\n\n\n\nVehicle\nActuals\nPrediction\n\n\n\n\ncar\n0\n\\(-4.89\\)\n\n\nbus\n1\n\\(2.60\\)\n\n\ntruck\n0\n\\(0.59\\)\n\n\nmotorbike\n0\n\\(-2.07\\)\n\n\nbicycle\n0\n\\(-4.57\\)\n\n\n\nActuals is a one hot encoded column that tells us what is the correct vehicle in the picture.\nTo convert these predictions into loss, first take the softmax of each prediction.\n\n\n\n\n\n\n\n\n\nVehicle\nActuals\nPrediction\nSoftmax\n\n\n\n\ncar\n0\n\\(-4.89\\)\n\\(4.88 \\cdot 10^{-4}\\)\n\n\nbus\n1\n\\(2.60\\)\n\\(0.874\\)\n\n\ntruck\n0\n\\(0.59\\)\n\\(0.117\\)\n\n\nmotorbike\n0\n\\(-2.07\\)\n\\(8.19 \\cdot 10^{-3}\\)\n\n\nbicycle\n0\n\\(-4.57\\)\n\\(6.72 \\cdot 10^{-4}\\)\n\n\n\nNext take the logarithm of each softmax value.\n\n\n\n\n\n\n\n\n\n\nVehicle\nActuals\nPrediction\nSoftmax\n\\(\\ln(\\text{Softmax})\\)\n\n\n\n\ncar\n0\n\\(-4.89\\)\n\\(4.88 \\cdot 10^{-4}\\)\n\\(-7.63\\)\n\n\nbus\n1\n\\(2.60\\)\n\\(0.874\\)\n\\(-1.35\\)\n\n\ntruck\n0\n\\(0.59\\)\n\\(0.117\\)\n\\(-2.14\\)\n\n\nmotorbike\n0\n\\(-2.07\\)\n\\(8.19 \\cdot 10^{-3}\\)\n\\(-4.81\\)\n\n\nbicycle\n0\n\\(-4.57\\)\n\\(6.72 \\cdot 10^{-4}\\)\n\\(-7.31\\)\n\n\n\nMultiply the actuals with the computed logarithms.\n\n\n\n\n\n\n\n\n\n\n\nVehicle\nActuals\nPrediction\nSoftmax\n\\(\\ln(\\text{Softmax})\\)\n\\(\\text{Actuals} \\cdot \\ln(\\text{Softmax})\\)\n\n\n\n\ncar\n0\n\\(-4.89\\)\n\\(4.88 \\cdot 10^{-4}\\)\n\\(-7.63\\)\n\\(0\\)\n\n\nbus\n1\n\\(2.60\\)\n\\(0.874\\)\n\\(-1.35\\)\n\\(-1.35\\)\n\n\ntruck\n0\n\\(0.59\\)\n\\(0.117\\)\n\\(-2.14\\)\n\\(0\\)\n\n\nmotorbike\n0\n\\(-2.07\\)\n\\(8.19 \\cdot 10^{-3}\\)\n\\(-4.81\\)\n\\(0\\)\n\n\nbicycle\n0\n\\(-4.57\\)\n\\(6.72 \\cdot 10^{-4}\\)\n\\(-7.31\\)\n\\(0\\)\n\n\n\nSum the the results of the multiplications.\n\\[\n0 + -1.35 + 0 + 0 + 0 = -1.35\n\\]\nAnd there you have your loss!\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/sample.html",
    "href": "dictionary/terms/sample.html",
    "title": "Sample",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/random_forest.html",
    "href": "dictionary/terms/random_forest.html",
    "title": "Random Forest",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/inference.html",
    "href": "dictionary/terms/inference.html",
    "title": "Inference",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/neuron.html",
    "href": "dictionary/terms/neuron.html",
    "title": "Neuron",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/metric.html",
    "href": "dictionary/terms/metric.html",
    "title": "Metric",
    "section": "",
    "text": "Examples of metrics are, but not limited to, accuracy, error rate, MAE, and MSE\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/backward_pass.html",
    "href": "dictionary/terms/backward_pass.html",
    "title": "Backward Pass",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/sampler.html",
    "href": "dictionary/terms/sampler.html",
    "title": "Sampler",
    "section": "",
    "text": "Metaphorically speaking, if we let a dataset be a warehouse, a dataloader be a human, a batch be a crate, and the sampler be the manager, then the manager is responsible for informing the human what items to gather from the warehouse, who then puts them into crates.\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/k_fold_cross_validation.html",
    "href": "dictionary/terms/k_fold_cross_validation.html",
    "title": "K-Fold Cross Validation",
    "section": "",
    "text": "Another way to think of it is that the dataset is split into \\(K\\) pieces. Then each model is trained on a different set of \\(K-1\\) pieces.\nFor example, let’s say that the dataset is split into 5 pieces. Then each model is trained on a different set of 4 pieces.\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/softmax.html",
    "href": "dictionary/terms/softmax.html",
    "title": "Softmax",
    "section": "",
    "text": "Let’s say that we have a model that tells us what sort of vehicle is in a picture. It outputs the following predictions.\n\n\n\n\n\n\n\nVehicle\nPrediction\n\n\n\n\ncar\n\\(-4.89\\)\n\n\nbus\n\\(2.60\\)\n\n\ntruck\n\\(0.59\\)\n\n\nmotorbike\n\\(-2.07\\)\n\n\nbicycle\n\\(-4.57\\)\n\n\n\nThese predictions aren’t very meaningful to us as humans. So what we can do is convert these predictions into probabilities. The steps to do this are below.\n1. Take the exponent of each prediction to base \\(e\\). So for the car category, \\(e^{-4.89} \\approx 7.52 \\cdot 10^{-3}\\).\nThe results of the calculations below are displayed with 3 significant figures.\n\n\n\n\n\n\n\n\nVehicle\nPrediction\n\\(e^{\\text{prediction}}\\)\n\n\n\n\ncar\n\\(-4.89\\)\n\\(7.52 \\cdot 10^{-3}\\)\n\n\nbus\n\\(2.60\\)\n\\(13.4\\)\n\n\ntruck\n\\(0.59\\)\n\\(1.80\\)\n\n\nmotorbike\n\\(-2.07\\)\n\\(0.126\\)\n\n\nbicycle\n\\(-4.57\\)\n\\(0.010\\)\n\n\n\n2. Sum all the calculated values.\n\n\n\n\n\n\n\n\n\nVehicle\nPrediction\n\\(e^{\\text{prediction}}\\)\n\\(\\text{sum of} e^{\\text{prediction}}\\)\n\n\n\n\ncar\n\\(-4.89\\)\n\\(7.52 \\cdot 10^{-3}\\)\n\\(15.4\\)\n\n\nbus\n\\(2.60\\)\n\\(13.4\\)\n\\(15.4\\)\n\n\ntruck\n\\(0.59\\)\n\\(1.80\\)\n\\(15.4\\)\n\n\nmotorbike\n\\(-2.07\\)\n\\(0.126\\)\n\\(15.4\\)\n\n\nbicycle\n\\(-4.57\\)\n\\(0.010\\)\n\\(15.4\\)\n\n\n\n3. For each respective category, divide \\(e^{\\text{prediction}}\\) by \\(\\text{sum of} e^{\\text{prediction}}\\). This is your probability. So the probability of the vehicle in the picture being a car is\n\\[\n\\frac{7.52 \\cdot 10^{-3}}{15.4} \\approx 4.88 \\cdot 10^{-4} = 0.000488 = 0.0488 \\%\n\\]\n\n\n\n\n\n\n\n\n\n\nVehicle\nPrediction\n\\(e^{\\text{prediction}}\\)\n\\(\\text{sum of} e^{\\text{prediction}}\\)\n\\(\\frac{e^{\\text{prediction}}}{\\text{sum of}e^{\\text{prediction}}}\\)\n\n\n\n\ncar\n\\(-4.89\\)\n\\(7.52 \\cdot 10^{-3}\\)\n\\(15.4\\)\n\\(4.88 \\cdot 10^{-4}\\)\n\n\nbus\n\\(2.60\\)\n\\(13.4\\)\n\\(15.4\\)\n\\(0.874\\)\n\n\ntruck\n\\(0.59\\)\n\\(1.80\\)\n\\(15.4\\)\n\\(0.117\\)\n\n\nmotorbike\n\\(-2.07\\)\n\\(0.126\\)\n\\(15.4\\)\n\\(8.19 \\cdot 10^{-3}\\)\n\n\nbicycle\n\\(-4.57\\)\n\\(0.010\\)\n\\(15.4\\)\n\\(6.72 \\cdot 10^{-4}\\)\n\n\n\nFrom the table above, it can be seen that the vehicle in the picture is most likely a bus with probability \\(87.4\\%\\).\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/decision_tree.html",
    "href": "dictionary/terms/decision_tree.html",
    "title": "Decision Tree",
    "section": "",
    "text": "A split is made for each feature in the data. If the feature of a certain data sample is larger than or less than the split for that respective feature, the next appropriate split is made.\nBelow is an example determining whether a car is fast or slow.\n\n\n\n\n\nflowchart TB\n  A([Weight &lt; 2000kg])\n  B([Is Engine Powerful])\n  C([Is Windy Day])\n  D1([Car Is Fast])\n  E1([Car Is Slow])\n  D2([Car Is Fast])\n  E2([Car Is Slow])\n\n\n  A -- Yes --&gt; B\n  A -- No --&gt; C\n  B -- Yes --&gt; D1\n  B -- No --&gt; E1\n  C -- Yes --&gt; E2\n  C -- No --&gt; D2\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/cognitive_map.html",
    "href": "dictionary/terms/cognitive_map.html",
    "title": "Cognitive Map",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/bagging.html",
    "href": "dictionary/terms/bagging.html",
    "title": "Bagging",
    "section": "",
    "text": "“with replacement” means that if a model, for example, randomly chooses row number 5, another model can also randomly choose row number 5.\n\n\n\n\n\n\nNote\n\n\n\n\n\nThrough this technique, each model ends up training on roughly 63% of the entire dataset.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/accuracy.html",
    "href": "dictionary/terms/accuracy.html",
    "title": "Accuracy",
    "section": "",
    "text": "It can be calculated by dividing the number of correct predictions by the number of total predictions. Optionally multiply the result by 100 to obtain a percentage.\n\\[\n\\frac{\\text{number of correct predictions}}{\\text{number of total predictions}}\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nAccuracy is also 1 - error rate.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/policy.html",
    "href": "dictionary/terms/policy.html",
    "title": "Policy (Reinforcement Learning)",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/tabular_model.html",
    "href": "dictionary/terms/tabular_model.html",
    "title": "Tabular Model",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/learning_rate.html",
    "href": "dictionary/terms/learning_rate.html",
    "title": "Learning Rate",
    "section": "",
    "text": "The learning rate controls how much the gradients adjust the parameters by multiplying the learning rate and gradients together.\n\n\n\n\n\n\nNote\n\n\n\n\n\nA learning rate that is too high can cause the training system to either get stuck in a loop or diverge from the optimal weights.\nA learning rate that is too low can cause the training system to take a very long time to reach the optimal weights.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/root_mean_squared_error_rmse.html",
    "href": "dictionary/terms/root_mean_squared_error_rmse.html",
    "title": "Root Mean Squared Error (RMSE)",
    "section": "",
    "text": "It is calculated by:\n\nFirst taking the difference between each respective predicted and actual value.\nThen the squaring all obtained values.\nTaking the average.\nAnd finally taking the square root.\n\nLet’s say we have a set of predicted values \\(1, 2, 3, 4\\). The set of actual values is \\(1, 4, 3, 3\\)\n\n\\(1-1, 2-4, 3-3, 4-3, = 0, -2, 0, 1\\)\n\\((0)^2, (-2)^2, (0)^2, (1)^2 = 0, 4, 0, 1\\)\n\\(\\frac{0 + 4 + 0 + 1}{4} = \\frac{5}{4} = 1.25\\)\n\\(\\sqrt{1.25} \\approx 1.12\\)\n\nThis tells us, that on average, our set of predicted values is \\(1.12\\) units off from the actual values.\nIn a nutshell, you take the root of the mean of the square of the differences between the predicted and actual values.\n\nThe main difference between MSE and RMSE is that RMSE undoes the squaring step by taking the square root.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe reason the square value is taken is due to the averaging step. Let’s say the first predicted value is off from the first actual value by \\(-3\\) units. And let’s say that the second predicted value is off from the second actual value by \\(3\\) units.\nIf we didn’t take the square, the average would be zero \\(\\left( \\frac{-3 + 3}{2} = \\frac{0}{2} = 0 \\right)\\). This is incorrect as both values are off from the actual value.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/mean_absolute_error_mae.html",
    "href": "dictionary/terms/mean_absolute_error_mae.html",
    "title": "Mean Absolute Error (MAE)",
    "section": "",
    "text": "It is calculated by: 1. First taking the difference between each respective predicted and actual value. 1. Then removing all negative signs — this is known as taking the absolute value. 1. And finally taking the average.\nLet’s say we have a set of predicted values \\(1, 2, 3, 4\\). The set of actual values is \\(1, 4, 3, 3\\)\n\n\\(1-1, 2-4, 3-3, 4-3, = 0, -2, 0, 1\\)\n\\(|0|, |-2|, |0|, |1| = 0, 2, 0, 1\\)\n\\(\\frac{0 + 2 + 0 + 1}{4} = \\frac{2}{4} = 0.5\\)\n\nThis tells us, that on average, our set of predicted values is \\(0.5\\) units off from the actual values.\nIn a nutshell, you take the mean of the absolute differences between the predicted and actual values.\n\nThe main difference between MAE and MSE is that MSE penalizes smaller differences more heavily.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe reason the absolute value is taken is due to the averaging step. Let’s say the first predicted value is off from the first actual value by \\(-3\\) units. And let’s say that the second predicted value is off from the second actual value by \\(3\\) units.\nIf we didn’t take the absolute value, the average would be zero \\(\\left( \\frac{-3 + 3}{2} = \\frac{0}{2} = 0 \\right)\\). This is incorrect as both values are off from the actual value.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/named_entity_recognition.html",
    "href": "dictionary/terms/named_entity_recognition.html",
    "title": "Named Entity Recognition (NER)",
    "section": "",
    "text": "Take the sentence “Tim went to the Moon.” as an example. The sentence would first be broken into ‘Tim’, ‘went’, ‘to’, the’, ‘Moon’. The model could then give ‘Tim’ the label of ‘person’, and ‘Moon’ the label of ‘location’.\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/weight_decay.html",
    "href": "dictionary/terms/weight_decay.html",
    "title": "Weight Decay",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/batch.html",
    "href": "dictionary/terms/batch.html",
    "title": "Batch",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "testing_page.html",
    "href": "testing_page.html",
    "title": "Testing Page",
    "section": "",
    "text": "Trying to change Quarto Version.\n\n\nForBlog\n\n\n\n\n\n\n\n\n\n\nPulling Back the Curtain on VLM Attention\n\n\nUnderstanding LLM Attention and Vision Encoder Attention\n\n\nA LLM narrates our conversation.\n\n\n\n\n\nAug 4, 2025\n\n\nSalman Naqvi\n\n\n\n\n\n\n\n\n\n\n\n\nGetting General Purpose Robots by Decomposing Problems and Working with Data\n\n\nroboOS®, powered by roboBrain™\n\n\nWhy these Robo series of papers are cold drink in a desert of hobbled pipelines\n\n\n\n\n\nJul 24, 2025\n\n\nSalman Naqvi\n\n\n\n\n\n\n\n\n\n\n\n\nCurrent Ideas in Spatial Understanding\n\n\nWhat is good, what is not?\n\n\nA collated set of current ideas.\n\n\n\n\n\nJul 14, 2025\n\n\nSalman Naqvi\n\n\n\n\n\nNo matching items\n\n\n\nPlayground\n\n\n\n\n\n\n\n\n\n\nMore apps coming soon…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlood Classifier\n\n\nHow well can you classify floods?\n\n\n\nSalman Naqvi\n\n\nSep 20, 2022\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n Back to top"
  }
]