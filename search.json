[
  {
    "objectID": "patch_notes.html",
    "href": "patch_notes.html",
    "title": "Site Patch Notes",
    "section": "",
    "text": "Detailed patchnotes are unavailable prior to site version 2.0.0.0."
  },
  {
    "objectID": "patch_notes.html#version-2.2.0.1-13-may-2023",
    "href": "patch_notes.html#version-2.2.0.1-13-may-2023",
    "title": "Site Patch Notes",
    "section": "Version 2.2.0.1 | 13 May 2023",
    "text": "Version 2.2.0.1 | 13 May 2023\n\nFixed animations for the App Playground.\nFixed back-to-top button covering content on the About Me page."
  },
  {
    "objectID": "patch_notes.html#version-2.2.0.0-13-may-2023",
    "href": "patch_notes.html#version-2.2.0.0-13-may-2023",
    "title": "Site Patch Notes",
    "section": "Version 2.2.0.0 | 13 May 2023",
    "text": "Version 2.2.0.0 | 13 May 2023\n\nImplemented site-wide animations!\nConsolidated unsubscribe form and all subscribe forms into a single form.\nAdded a back-to-top button.\nRSS and source code navbar icons are now positioned more cleanly when accessed from the drop down menu on smaller screens.\nIncreased number of displayed posts on the ForBlog from 5 per page to 7 per page.\nChanged Home navbar icon.\nTemporarily disabled App Playground."
  },
  {
    "objectID": "patch_notes.html#version-2.1.0.1-25-march-2023",
    "href": "patch_notes.html#version-2.1.0.1-25-march-2023",
    "title": "Site Patch Notes",
    "section": "Version 2.1.0.1 | 25 March 2023",
    "text": "Version 2.1.0.1 | 25 March 2023\n\nTweaked the various subscription forms’ positioning."
  },
  {
    "objectID": "patch_notes.html#version-2.1.0.0-23-february-2023",
    "href": "patch_notes.html#version-2.1.0.0-23-february-2023",
    "title": "Site Patch Notes",
    "section": "Version 2.1.0.0 | 23 February 2023",
    "text": "Version 2.1.0.0 | 23 February 2023\n\nLaunched the AI dictionary.\nShortened navbar text."
  },
  {
    "objectID": "patch_notes.html#version-2.0.3.2-22-february-2023",
    "href": "patch_notes.html#version-2.0.3.2-22-february-2023",
    "title": "Site Patch Notes",
    "section": "Version 2.0.3.2 | 22 February 2023",
    "text": "Version 2.0.3.2 | 22 February 2023\n\nUpdated copyright notices for 2023."
  },
  {
    "objectID": "patch_notes.html#version-2.0.3.1-28-january-2023",
    "href": "patch_notes.html#version-2.0.3.1-28-january-2023",
    "title": "Site Patch Notes",
    "section": "Version 2.0.3.1 | 28 January 2023",
    "text": "Version 2.0.3.1 | 28 January 2023\n\nChanged comment section theme."
  },
  {
    "objectID": "patch_notes.html#version-2.0.3.0-27-november-2022",
    "href": "patch_notes.html#version-2.0.3.0-27-november-2022",
    "title": "Site Patch Notes",
    "section": "Version 2.0.3.0 | 27 November 2022",
    "text": "Version 2.0.3.0 | 27 November 2022\n\nFully implemented Twitter Cards."
  },
  {
    "objectID": "patch_notes.html#version-2.0.2.0-26-november-2022",
    "href": "patch_notes.html#version-2.0.2.0-26-november-2022",
    "title": "Site Patch Notes",
    "section": "Version 2.0.2.0 | 26 November 2022",
    "text": "Version 2.0.2.0 | 26 November 2022\n\nFully implemented Open Graph.\nAdded button for direct link to site’s source code.\nTweaked landing page description."
  },
  {
    "objectID": "patch_notes.html#version-2.0.1.2-17-november-2022",
    "href": "patch_notes.html#version-2.0.1.2-17-november-2022",
    "title": "Site Patch Notes",
    "section": "Version 2.0.1.2 | 17 November 2022",
    "text": "Version 2.0.1.2 | 17 November 2022\n\nFixed broken license link."
  },
  {
    "objectID": "patch_notes.html#version-2.0.1.1-17-november-2022",
    "href": "patch_notes.html#version-2.0.1.1-17-november-2022",
    "title": "Site Patch Notes",
    "section": "Version 2.0.1.1 | 17 November 2022",
    "text": "Version 2.0.1.1 | 17 November 2022\n\nFixed broken site feedback link.\nUpdated site version references."
  },
  {
    "objectID": "patch_notes.html#version-2.0.1.0-17-november-2022",
    "href": "patch_notes.html#version-2.0.1.0-17-november-2022",
    "title": "Site Patch Notes",
    "section": "Version 2.0.1.0 | 17 November 2022",
    "text": "Version 2.0.1.0 | 17 November 2022\n\nFixed a bunch of broken links.\nFixed RSS buttons.\nShifted links on the landing page."
  },
  {
    "objectID": "patch_notes.html#version-2.0.0.0-16-november-2022",
    "href": "patch_notes.html#version-2.0.0.0-16-november-2022",
    "title": "Site Patch Notes",
    "section": "Version 2.0.0.0 | 16 November 2022",
    "text": "Version 2.0.0.0 | 16 November 2022\n\nCreated, erm, site patch notes.\nSite is now entirely remade in Quarto.\nUI overhaul.\nForBlog is no longer the main landing page.\nApp playground has been added; a place where I can host my various creations.\n\nNew…\n\nfavicon.\nabout me page.\nlanding page.\nForBlog home page.\nForBlog post layout.\nglobal search bar.\n\nAdded…\n\na ForBlog only search bar.\nForBlog post filters.\nForBlog and App Playground subscriptions.\na form for site feedback.\ncopyright licences.\nnew fancy buttons."
  },
  {
    "objectID": "patch_notes.html#version-1.0.0.0-15-may-2022",
    "href": "patch_notes.html#version-1.0.0.0-15-may-2022",
    "title": "Site Patch Notes",
    "section": "Version 1.0.0.0 | 15 May 2022",
    "text": "Version 1.0.0.0 | 15 May 2022\n\nInitial release.\nSite is built on fastpages, by fastai."
  },
  {
    "objectID": "forblog/posts/12_stable_diffusion_summarized.html",
    "href": "forblog/posts/12_stable_diffusion_summarized.html",
    "title": "Stable Diffusion, Summarized",
    "section": "",
    "text": "This post was edited on Sunday, 30 April 2023\nHere, I explain the workings of stable diffusion at a high level."
  },
  {
    "objectID": "forblog/posts/12_stable_diffusion_summarized.html#components",
    "href": "forblog/posts/12_stable_diffusion_summarized.html#components",
    "title": "Stable Diffusion, Summarized",
    "section": "Components",
    "text": "Components\nA diffuser contains four main components\n\nThe text encoder\nThe image encoder\nThe autoencoder (VAE autoencoder)\nThe neural network (U-net)\n\n\n\n\n\nflowchart TB\n    A{{Diffuser}}\n    B([U-net])\n    C([VAE Autoencoder])\n    D([Text Encoder])\n    E([Image Encoder])\n\n    A --&gt; D & E & C & B"
  },
  {
    "objectID": "forblog/posts/12_stable_diffusion_summarized.html#training",
    "href": "forblog/posts/12_stable_diffusion_summarized.html#training",
    "title": "Stable Diffusion, Summarized",
    "section": "Training",
    "text": "Training\nI’ll explain the training process in terms of a single image.\nWhen all components shown above are put into their respective places, the overall training process looks like this.\n\n\n\n\n\nflowchart LR\n    subgraph A [Feature Vector Creation]\n        id1([Text Encoder])\n        id2([Image Encoder])\n    end\n\n    subgraph B [Image Compression]\n        id3([VAE Autoencoder])\n    end\n\n    subgraph C [Noise Removal]\n        id4([U-net])\n    end\n\n    subgraph D [Image Decompression]\n        id5([VAE Autoencoder])\n    end\n\n    id7[Input Image Description] & id6[Input Image] --&gt; A --&gt; id9[Feature Vector]\n    id6 --&gt; B --noise added to image--&gt; id10[Noisy Latent]\n    id9 & id10 --&gt; C --&gt; id11[Less Noisy Latent] --&gt;  C\n    id11 --&gt; D --&gt; id8[Generated Image]\n\n\n\n\n\n\nLet’s break it down.\n\nFeature Vector Creation\n\n\n\n\n\nflowchart TB\n    subgraph B [ ]\n        direction LR\n        id1[Input Image]\n        id2[Input Image Description]\n        subgraph A [Feature Vector Creation]\n            id3([Text Encoder])\n            id4([Image Encoder])\n        end\n        id2 & id1 --&gt; A --&gt; id11[Feature Vector]\n    end\n    style B fill:#FFF, stroke:#333,stroke-width:3px\n\n    subgraph C [ ]\n        direction LR\n        id5[Input Image]\n        id7[Input Image Descripton]\n        id5 --&gt; id6\n        id7 --&gt; id8\n        subgraph D [Feature Vector Creation]\n            id6([Image Encoder])\n            id8([Text Encoder])\n            id6 & id8 --&gt; id9[CLIP Embedding]\n        end\n        id9 --&gt; id10[Feature Vector]\n    end\n    style C fill:#FFF, stroke:#333,stroke-width:3px\n\n    B --&gt; C\n    B --&gt; C\n    B --&gt; C\n\n\n\n\n\n\nWe start with an image and its description. The image encoder takes the image and produces a feature vector — a vector with numerical values that describe the image in some way. The text encoder takes the image’s description and similarly produces a feature vector.\nThese two feature vectors are then stored in what’s known as a CLIP embedding. An embedding is simply a table where each row is an item and each column describes the items in some way. In this case, the rows represent feature vectors, and the columns are each feature in the vector.\nBoth encoders keep producing feature vectors until they are as similar as possible.\n\n\nImage Compression\n\n\n\n\n\nflowchart TB\n    subgraph A [ ]\n        id2[Input Image] --&gt; id1\n        subgraph B [Image Compression]\n            direction LR\n            id1([VAE Autoencoder])\n        end\n        id1 --&gt; id7[Latent]\n    end\n    style A fill:#FFF, stroke:#333,stroke-width:3px\n\n    subgraph C[ ]\n        direction LR\n        id3[Input Image]\n        subgraph D [Image Compression]\n            id4([VAE Encoder])\n            id5([VAE Decoder])\n        end\n        id3 --&gt; id4 --&gt; id6[Latent]\n    end\n    style C fill:#FFF, stroke:#333,stroke-width:3px\n\n    A & A & A --&gt; C\n\n\n\n\n\n\nOnce the feature vectors have been produced, the image is compressed by the VAE autoencoder. Some noise is then tossed onto the image.\nThe VAE autoencoder contains an encoder and a decoder. The encoder handles compression whereas the decoder handles decompression.\nThe compressed noisy image is now known as the latent. The image is compressed for faster computation, as there would be fewer pixels to compute on.\n\n\nNoise Removal\n\n\n\n\n\nflowchart TB\n    subgraph A [ ]\n        id1[Feature Vector] & id2[Noisy Latent] --&gt; id3\n        subgraph B [Noise Removal]\n            direction LR\n            id3([U-net]) --&gt; id4[Noise]\n        end\n        id4 --with learning rate--&gt; id5[Less Noisy Latent] --&gt; id3\n    end\n    style A fill:#FFF, stroke:#333,stroke-width:3px \n\n    subgraph C [ ]\n        id6[Feature Vector] & id7[Noisy Latent] --&gt; id8\n        subgraph Noise Removal\n            direction LR\n            id8([U-net])\n        end\n        id8 --&gt; id9([Less Noisy Latent]) --&gt; id8\n    end\n    C & C & C --&gt; A\n    style C fill:#FFF, stroke:#333,stroke-width:3px\n\n\n\n\n\n\nThe latent, together with its feature vector, is now input to the U-net. Instead of predicting what the original, un-noisy image was, the U-net predicts the noise that was tossed onto the image.\nOnce it outputs the predicted noise, that noise is subtracted from the latent in conjunction with the learning rate. This new, less noisy latent is now input again and the process repeats until desired.\n\n\nImage Decompression\n\n\n\n\n\nflowchart TB\n    subgraph A [ ]\n        direction LR\n        id2[Input Image] --&gt; id1\n        subgraph B [Image Decompression]\n            id1([VAE Autoencoder])\n        end\n        id1 --&gt; id7[Latent]\n    end\n    style A fill:#FFF, stroke:#333,stroke-width:3px\n\n    subgraph C [ ]\n        direction LR\n        id3[Less Noisy Latent] --&gt; id5\n        subgraph D [Image Decompression]\n            id4([VAE Encoder])\n            id5([VAE Decoder])\n        end\n        id5 --&gt; id6[Generated Image]\n    end\n    style C fill:#FFF, stroke:#333,stroke-width:3px\n\n    A & A & A --&gt; C\n\n\n\n\n\n\nThe latent is now decompressed through the VAE autoencoder’s decoder.\nWe now have a generated image!"
  },
  {
    "objectID": "forblog/posts/12_stable_diffusion_summarized.html#inference",
    "href": "forblog/posts/12_stable_diffusion_summarized.html#inference",
    "title": "Stable Diffusion, Summarized",
    "section": "Inference",
    "text": "Inference\nWhen using a diffuser for inference, the diffuser typically begins with a purely noisey latent. The diffuser uses the input prompt to guide the removal of noise from the latent, until the latent resembles what is desired."
  },
  {
    "objectID": "forblog/posts/12_stable_diffusion_summarized.html#conclusion",
    "href": "forblog/posts/12_stable_diffusion_summarized.html#conclusion",
    "title": "Stable Diffusion, Summarized",
    "section": "Conclusion",
    "text": "Conclusion\nAnd that’s all there is to it!\nWe take an image and its prompt, and create a feature vector out of them. The image is compressed and noise is then added to it. The latent and the feature vector are input to a U-net which then predicts the noise in the latent. The predicted noise is subtracted from the latent, which is then input back to the U-net. After the desired number of steps has lapsed, the latent is decompressed and the generated image is ready!\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/3_the_confusion_matrix.html",
    "href": "forblog/posts/3_the_confusion_matrix.html",
    "title": "A No Nonsense Guide to Reading a Confusion Matrix",
    "section": "",
    "text": "This article was updated on Thursday, 10 November 2022.\nConfusion matrices help model designers view what mistakes a model has made.\nIn this post, I’ll be telling you how to easily read such matrices.\nJump to Section 2 for an ultra concise rundown.\nReady? Here we go."
  },
  {
    "objectID": "forblog/posts/3_the_confusion_matrix.html#case-1-introduction",
    "href": "forblog/posts/3_the_confusion_matrix.html#case-1-introduction",
    "title": "A No Nonsense Guide to Reading a Confusion Matrix",
    "section": "Case 1: Introduction",
    "text": "Case 1: Introduction\n\nIgnore the “Actual” and “Predicted” labels for now.\nLet’s compare grizzly bears to black bears.\nAll comparisons begin at the bottom, with the columns.\nFirst, highlight the grizzly bear column.\n\nNext, highlight the black bear row.\n\nNow find the common entry in the highlighted column and row.\n\nThis common entry is our required information.\nAll entries in the diagonal going from the top left to the bottom right (blue) are correct classifications. All other entries are incorrect classifications.\nOur common entry does not lie in the main diagonal. Therefore, we are looking at incorrect classifications.\nWe have compared grizzly bears to black bears. Therefore, from this deduction, three grizzly bears have been incorrectly classified as black bears.\n\n\n\n\n\n\nNote\n\n\n\nThere is a difference between comparing grizzly bears to black bears and black bears to grizzly bears.\nComparing grizzly bears to black bears means, “How many grizzly bears were misclassified as black bears?”\nComparing black bears to grizzly bears means, “How many black bears were misclassified as grizzly bears?”"
  },
  {
    "objectID": "forblog/posts/3_the_confusion_matrix.html#sec-case2",
    "href": "forblog/posts/3_the_confusion_matrix.html#sec-case2",
    "title": "A No Nonsense Guide to Reading a Confusion Matrix",
    "section": "Case 2: Ultra Concise",
    "text": "Case 2: Ultra Concise\nLet’s compare black bears to grizzly bears.\nHighlight the black bear column.\n\nHighlight the grizzly bear row.\n\nHighlight the common entry.\n\nZero black bears were misclassified as grizzly bears."
  },
  {
    "objectID": "forblog/posts/3_the_confusion_matrix.html#case-3-correct-classifications",
    "href": "forblog/posts/3_the_confusion_matrix.html#case-3-correct-classifications",
    "title": "A No Nonsense Guide to Reading a Confusion Matrix",
    "section": "Case 3: Correct Classifications",
    "text": "Case 3: Correct Classifications\nLet’s see how many teddy bears were correctly classified. We are essentially comparing teddy bears to teddy bears.\nHighlight the teddy bear column.\n\nHighlight the teddy bear row.\n\nHighlight the common entry.\n\nFifty three teddy bears were correctly classified as teddy bears."
  },
  {
    "objectID": "forblog/posts/3_the_confusion_matrix.html#exercise-do-it-yourself",
    "href": "forblog/posts/3_the_confusion_matrix.html#exercise-do-it-yourself",
    "title": "A No Nonsense Guide to Reading a Confusion Matrix",
    "section": "Exercise: Do It Yourself",
    "text": "Exercise: Do It Yourself\nBelow is a confusion matrix of a car classifier that classifies cars into their brand.\n\nYou learn by doing!\n\nHow many Lamborghinis were correctly classified?\nHow many Jaguars were incorrectly classified?\nHow many Chevrolets were misclassified as Fords?\nHow many Fords were misclassified as Chevrolets?\nWhich two car brands did the model have the most trouble differentiating between?\n\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/7_blog_subscriptions.html",
    "href": "forblog/posts/7_blog_subscriptions.html",
    "title": "Adding Subscriptions to a Quarto Site",
    "section": "",
    "text": "The Quarto Documenation covers how to implement website subscriptions at a surface level. This guide goes into the details on how one could do so, with three different options. That said, this guide can also be helpful for sites that do not use Quarto.\nThe three ways this guide will cover:\nSwitch between the tabs below to view the steps for each option."
  },
  {
    "objectID": "forblog/posts/7_blog_subscriptions.html#option-3",
    "href": "forblog/posts/7_blog_subscriptions.html#option-3",
    "title": "Adding Subscriptions to a Quarto Site",
    "section": "Option 3",
    "text": "Option 3\nPerhaps you know some HTML and JS, or even only JS, and don’t have an alternative address. Instead of creating the frontend with HTML, try using the Quarto HTML Forms extension by Jonathan Graves.\nThis extension allows you to implement HTML forms through Quarto Shortcodes and YAML Options. However, you still will need to handle the backend with JavaScript and perhaps a few other technologies. If you’re interested in implementing it this way, you probably already know how to. If not, there are plenty of great guides online!."
  },
  {
    "objectID": "forblog/posts/7_blog_subscriptions.html#acknowledgements",
    "href": "forblog/posts/7_blog_subscriptions.html#acknowledgements",
    "title": "Adding Subscriptions to a Quarto Site",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Isaac Flath for collaborating with me on this guide! You can view his blog, works, and contact here."
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html",
    "title": "Detecting Floods for Disaster Relief",
    "section": "",
    "text": "You can find this notebook on Kaggle here.\nThis article was updated on Friday, 11 November 2022.\nThe model that will be created in this notebook can detect whether an area shown in an image is flooded or not. The idea for creating this model has been spurred from the recent floodings in Pakistan.\nSuch models can prove useful in flood relief, helping to detect which areas need immediate focus.\nThe dataset used to train this model is Louisiana flood 2016, uploaded by Kaggle user Rahul T P, which you can view here.\nThe fastai library, a high level PyTorch library, has been used.\nOne of the points of this notebook is to showcase how simple it is to create powerful models. That said, this notebook is not a tutorial or guide.\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html#sort-data.",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html#sort-data.",
    "title": "Detecting Floods for Disaster Relief",
    "section": "Sort data.",
    "text": "Sort data.\nThe data in the dataset needs to be organized into train and valid folders. Each will contain the same subfolders, 0 and 1, which will be used to label the data. A label of 0 indicates the area shown in the image is not flooded, while a label of 1 indicates the area shown in the image is flooded.\nThe images in the dataset itself has been organized as follows:\n    If no underscore is in the file name, the image shows the area before or after the flood.\n    If an underscore is in the file name, the image shows the area during the flood:\n\nIf a zero follows the underscore, the area was not flooded.\nIf a one follows the underscore, the area was flooded.\n\nCreating the necessary paths.\n\nworking_path = Path.cwd(); print(working_path)\nfolders = ('train', 'valid')\nlabels = ('0', '1')\n\n/kaggle/working\n\n\n\ninput_path = Path('/kaggle/input')\ntrain_image_paths = sorted(input_path.rglob('train/*.png'))\nvalid_image_paths = sorted(input_path.rglob('test/*.png'))\nlen(train_image_paths), len(valid_image_paths)\n\n(270, 52)\n\n\nCreating the necessary directories.\n\nfor folder in folders:\n    if not (working_path/folder).exists():\n        (working_path/folder).mkdir()\n    for label in labels:\n        if not (working_path/folder/label).exists():\n            (working_path/folder/label).mkdir()\n\nMove images to new directories.\n\ntry:\n    for image_path in train_image_paths:\n        if '_1' in image_path.stem:\n            with (working_path/'train'/'1'/image_path.name).open(mode='xb') as f:\n                f.write(image_path.read_bytes())\n        else:\n            with (working_path/'train'/'0'/image_path.name).open(mode='xb') as f:\n                f.write(image_path.read_bytes())\nexcept FileExistsError:\n    print(\"Training images have already been moved.\")\nelse:\n    print(\"Training images moved.\")\n\nTraining images moved.\n\n\n\ntry:\n    for image_path in valid_image_paths:\n        if '_1' in image_path.stem:\n            with (working_path/'valid'/'1'/image_path.name).open(mode='xb') as f:\n                f.write(image_path.read_bytes())\n        else:\n            with (working_path/'valid'/'0'/image_path.name).open(mode='xb') as f:\n                f.write(image_path.read_bytes())\nexcept FileExistsError:\n    print(\"Testing images have already been moved.\")\nelse:\n    print(\"Testing images moved.\")\n\nTesting images moved.\n\n\nCheck that images have been moved.\n\ntraining_images = get_image_files(working_path/'train'); print(len(training_images))\n\n270\n\n\n\nImage.open(training_images[0])\n\n\n\n\n\nvalidation_images = get_image_files(working_path/'valid'); print(len(validation_images))\n\n52\n\n\n\nImage.open(validation_images[-1])"
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html#load-data",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html#load-data",
    "title": "Detecting Floods for Disaster Relief",
    "section": "Load data",
    "text": "Load data\nCreate the training and validation dataloaders through fastai’s quick and easy DataBlock class.\n\ndataloaders = DataBlock(\n    blocks = (ImageBlock, CategoryBlock),\n    get_items = get_image_files,\n    splitter = GrandparentSplitter(),\n    get_y = parent_label,\n    item_tfms = [Resize(192, method='squish')]\n).dataloaders(working_path, bs=32)\n\nCheck that data has been loaded correctly.\n\ndataloaders.show_batch(max_n=8)"
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html#instantiate-and-train-model",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html#instantiate-and-train-model",
    "title": "Detecting Floods for Disaster Relief",
    "section": "Instantiate and Train Model",
    "text": "Instantiate and Train Model\n\nlearner = vision_learner(dataloaders, resnet18, metrics=error_rate)\nlearner.fine_tune(9)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.919323\n1.118264\n0.365385\n00:09\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.490039\n0.628054\n0.250000\n00:02\n\n\n1\n0.367996\n0.411558\n0.192308\n00:02\n\n\n2\n0.266664\n0.472146\n0.192308\n00:02\n\n\n3\n0.203069\n0.256436\n0.115385\n00:03\n\n\n4\n0.158453\n0.127106\n0.076923\n00:03\n\n\n5\n0.124499\n0.095927\n0.038462\n00:02\n\n\n6\n0.098409\n0.089279\n0.038462\n00:03\n\n\n7\n0.079600\n0.093277\n0.038462\n00:02\n\n\n8\n0.064886\n0.090372\n0.038462\n00:02\n\n\n\n\n\nNice! A relatively low error rate for no tweaking."
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html#visualizing-mistakes",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html#visualizing-mistakes",
    "title": "Detecting Floods for Disaster Relief",
    "section": "Visualizing Mistakes",
    "text": "Visualizing Mistakes\nWe have to see how the model is getting confuzzled.\n\ninterp = ClassificationInterpretation.from_learner(learner)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnly a couple of mistakes. Let’s see what they are.\n\ninterp.plot_top_losses(5, nrows=1)\n\n\n\n\n\n\n\n\n\n\n\nNothing has been mislabeled, but the first one is especially tricky to determine, even for human eyes."
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html#model-inference",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html#model-inference",
    "title": "Detecting Floods for Disaster Relief",
    "section": "Model Inference",
    "text": "Model Inference\nLet’s test the model on some images of the recent flooding in Pakistan.\n\ndef infer_image(image_path):\n    display(Image.open(image_path))\n    label, _, probabilities = learner.predict(PILImage(PILImage.create(image_path)))\n    if label == '0':\n        print(f\"The area shown in the image is not flooded with probability {probabilities[0]*100:.2f}%.\")\n    elif label == '1':\n        print(f\"The area shown in the image is flooded with probability {probabilities[1]*100:.2f}%.\")\n    else:\n        print(\"Unknown label assigned to image.\")\n\n\ninfer_image(input_path/'floodclassifiertestset'/'1'/'1.jpeg')\n\n\n\n\n\n\n\n\n\n\n\nThe area shown in the image is not flooded with probability 65.65%.\n\n\nNot bad!\nLet’s try it on another image.\n\ninfer_image(input_path/'floodclassifiertestset'/'1'/'2.jpg')\n\n\n\n\n\n\n\n\n\n\n\nThe area shown in the image is flooded with probability 99.90%.\n\n\nThe label for this image is kind of meaningless. This is an image of a vast area of land, so certain areas could be flooded, while others are not. That said, it could be used to determine whether there is flooding in the image.\n\ninfer_image(input_path/'floodclassifiertestset'/'1'/'3.jpg')\n\n\n\n\n\n\n\n\n\n\n\nThe area shown in the image is flooded with probability 99.99%.\n\n\nThe model performed really well in this case: the input image is shown at a different angle. The images in the training set only show areas from a top-down view.\n\ninfer_image(input_path/'floodclassifiertestset'/'1'/'4.jpg')\n\n\n\n\n\n\n\n\n\n\n\nThe area shown in the image is not flooded with probability 64.56%.\n\n\nOver here, the limitations of the current state of the model can be seen. The model is not performing well on images where the view is more parallel to the ground, since the images in the training set are all top-down.\nLet’s do two more images.\n\ninfer_image(input_path/'floodclassifiertestset'/'1'/'5.jpg')\n\n\n\n\n\n\n\n\n\n\n\nThe area shown in the image is flooded with probability 99.94%.\n\n\n\ninfer_image(input_path/'floodclassifiertestset'/'1'/'6.jpg')\n\n\n\n\n\n\n\n\n\n\n\nThe area shown in the image is flooded with probability 100.00%.\n\n\nThe model is working well with images of different sizes too, and has given this image a very high, correct confidence."
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html#improving-the-model.",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html#improving-the-model.",
    "title": "Detecting Floods for Disaster Relief",
    "section": "Improving the model.",
    "text": "Improving the model.\nLet’s see if we can get the model’s performance to improve on the following image through augmenting the training set.\n\nImage.open(input_path/'floodclassifiertestset'/'1'/'4.jpg')\n\n\n\n\n\naugmented_dataloaders = DataBlock(\n    blocks = (ImageBlock, CategoryBlock),\n    get_items = get_image_files,\n    splitter = GrandparentSplitter(),\n    get_y = parent_label,\n    item_tfms = RandomResizedCrop(192, min_scale=0.5),\n    batch_tfms=aug_transforms()\n).dataloaders(working_path, bs=32)\n\n\naugmented_dataloaders.show_batch(max_n=8)\n\n\n\n\n\naugmented_learner = vision_learner(augmented_dataloaders, resnet18, metrics=error_rate)\naugmented_learner.fine_tune(9)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.161182\n0.835870\n0.365385\n00:02\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.442552\n0.686252\n0.288462\n00:03\n\n\n1\n0.417739\n0.411907\n0.153846\n00:02\n\n\n2\n0.346400\n0.316388\n0.057692\n00:03\n\n\n3\n0.306782\n0.213407\n0.076923\n00:02\n\n\n4\n0.251947\n0.199586\n0.076923\n00:02\n\n\n5\n0.209951\n0.141818\n0.057692\n00:02\n\n\n6\n0.188433\n0.116713\n0.057692\n00:03\n\n\n7\n0.169689\n0.125078\n0.057692\n00:02\n\n\n8\n0.151843\n0.131188\n0.057692\n00:02\n\n\n\n\n\nLet’s try the new model out.\n\ndisplay(Image.open(input_path/'floodclassifiertestset'/'1'/'4.jpg'))\nlabel, _, probabilities = augmented_learner.predict(PILImage(PILImage.create(input_path/'floodclassifiertestset'/'1'/'4.jpg')))\nif label == '0':\n    print(f\"The area shown in the image is not flooded with probability {probabilities[0]*100:.2f}%.\")\nelif label == '1':\n    print(f\"The area shown in the image is flooded with probability {probabilities[1]*100:.2f}%.\")\nelse:\n    print(\"Unknown label assigned to image.\")\n\n\n\n\n\n\n\n\n\n\n\nThe area shown in the image is flooded with probability 99.91%.\n\n\nDang, impressive! The correct label and with excellent confidence!\nBefore we get too excited though, we should check the performance on the model with the previous images.\n\ntest_dataloader = learner.dls.test_dl([image_path for image_path in sorted((input_path/'floodclassifiertestset').rglob('*.*'))])\n\n\nprobabilities, _, labels = augmented_learner.get_preds(dl=test_dataloader, with_decoded=True)\n\n\n\n\n\n\n\n\n\nprint(\"Images are numbered horizontally.\")\ntest_dataloader.show_batch()\nfor probability, label, image_number in zip(probabilities, labels, range(1, 7)):\n    if label == 1:\n        print(f\"Image {image_number} is flooded with a probability of {probability[1]*100:.2f}%.\")\n    elif label == 0:\n        print(f\"Image {image_number} is not flooded with a probability of {probability[0]*100:.2f}%.\")\n    else:\n        print(f\"Image {image_number} has been assigned an unknown label.\")\n\nImages are numbered horizontally.\nImage 1 is flooded with a probability of 95.94%.\nImage 2 is flooded with a probability of 99.92%.\nImage 3 is flooded with a probability of 91.34%.\nImage 4 is flooded with a probability of 99.71%.\nImage 5 is flooded with a probability of 100.00%.\nImage 6 is flooded with a probability of 100.00%.\n\n\n\n\n\nDrastically improved probabilities! A little augmentation can go a long way."
  },
  {
    "objectID": "forblog/posts/5_detecting_floods_for_disaster_relief.html#takeaways",
    "href": "forblog/posts/5_detecting_floods_for_disaster_relief.html#takeaways",
    "title": "Detecting Floods for Disaster Relief",
    "section": "Takeaways",
    "text": "Takeaways\nThis model was trained on only 270 images and minimal code. Accessbility and abstraction to the field of machine learning has come a long, long way. Given the right data and the right pretrained model, a powerful model can be produced in less than an hour, if not half.\nThis is important: in disasters such as floods, the time taken to produce the logistics required for relief can be drastically reduced. It is also important because the barrier of entry to this field is dramatically lowered; more people can create powerful models, in turn producing better solutions.\nHowever, there could be some improvements and additions made to the model:\n\nInclude a third class to the model. Images that are not flooded, but show signs of having been flooded would be assigned this class. The dataset used for this model includes such images.\nTrain the model on images that include a variety of geographic locations and dwellings. The current dataset only contains images taken in a lush, green area with plenty of trees; infrastructure looks a certain way; the color of the floodwater is also dependent on the surroundings. All this makes the model good a prediciting whether an image is flooded for images with certain features.\n\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/1_how_to_approach_creating_ai_models.html",
    "href": "forblog/posts/1_how_to_approach_creating_ai_models.html",
    "title": "How to Approach Creating AI Models",
    "section": "",
    "text": "This article was rewritten on Monday, 31 October 2022."
  },
  {
    "objectID": "forblog/posts/1_how_to_approach_creating_ai_models.html#introduction",
    "href": "forblog/posts/1_how_to_approach_creating_ai_models.html#introduction",
    "title": "How to Approach Creating AI Models",
    "section": "Introduction",
    "text": "Introduction\nHow you approach making models is crucial. The way AI methods are used in today’s landscape is very different. AI methods are created to solve small, atomic problems. And we’ve got most of the methods to handle these small tasks hammered down. Therefore, applied AI is not about creating models; it’s only a small part of it. It’s 80% problem solving and 20% implementing (I would not be surprised if it actually followed the 80-20 rule1).1 The 80/20 Rule, also known as the Pareto Principle\nThink of AI methods as a tool; think of it as a pencil. You can use pencils to draw, take notes, poke holes, and much more. There are also dozens of pencils out there. But what point is there in using any of those pencils if you don’t even know how to properly use a pencil in the first place? The art of creating pencils has already been perfected too.\nOne highly successful approach is the Drivetrain Approach, created by Jeremy Howard — who’s widely known for his fastai course and library —, Margit Zwemer, and Mike Loukides.\nThe goal of the Drivetrain Approach is to not just use data to generate more data — data that is in the form of predictions. But rather to use data to also generate actionable outcomes.\nThe official blogpost goes into much more depth here.\nIn this post, I’ll be providing a short overview of my understanding of this approach by applying it to the Elements of AI course’s final project (this online course was created by the University of Helsinki and Reaktor)."
  },
  {
    "objectID": "forblog/posts/1_how_to_approach_creating_ai_models.html#overview-of-the-drivetrain-approach",
    "href": "forblog/posts/1_how_to_approach_creating_ai_models.html#overview-of-the-drivetrain-approach",
    "title": "How to Approach Creating AI Models",
    "section": "Overview of the Drivetrain Approach",
    "text": "Overview of the Drivetrain Approach\nThere are four main steps to this approach:\n\nDefine the objective\nConsider your possible actions\nConsider your data\nCreate the models\n\n\n\n\nImage Source\n\n\n\nDefine the objective\nWrite out what you are really trying to achieve. What is your goal? Writing it out puts it in a tangible manner.\n\n\nConsider your actions\nThink about what actions you can take to achieve your objective.\nAlso think about what would happen if you did those actions.\nWhat would happen if I did x? Would y really be a good idea? What if z worked out too well? Will x lead to y? What would happen if x turned out poorly?\n\n\nConsider your data\nThink about the data you already have and how it could be used.\nThink about any further data that is needed and how it could be collected.\n\n\nCreate the models\nCreate models. But create models that produce actions. Actions that produce the best results for your objective."
  },
  {
    "objectID": "forblog/posts/1_how_to_approach_creating_ai_models.html#endangered-language-chatbot",
    "href": "forblog/posts/1_how_to_approach_creating_ai_models.html#endangered-language-chatbot",
    "title": "How to Approach Creating AI Models",
    "section": "Endangered Language Chatbot",
    "text": "Endangered Language Chatbot\nThe final project of the Elements of AI course asked me to come up with my own AI method that would solve a problem, and how it would do so.\nThe problem I tackled was the endangerment of languages. The solution I came up with was to create a chatbot that could converse in these endangered languages. I created an overview of how this could be done.\nThe overview can be read here.\nLet’s tackle this problem through the Drivetrain Approach.\n\nDefine the objective\nThe objective is to preserve languages that are in danger of going extinct. Through preserving languages, histories and cultures can be preserved.\n\n\nConsider your actions\nOne way this could be done is to create a chatbot that could converse in endangered languages. However, this would be a monumental task considering the amount of data needed to achieve this.\nAnother action that could be taken is to create an information retrieval (IR) system of sorts. A corpus of written text of the language could be provided, from which insights about the language’s history, culture, and way of conversing could be gained. In turn the language is preserved.\nThe latter action may be easier to achieve.\n\n\nConsider your data\nThe obvious source of data would be a corpora of text.\nHowever, a major problem arises for those languages which are only spoken. Audio recordings of conversations would have to be made which would take a lot of time and effort. This would be especially difficult for those languages where very few speakers remain.\nEven if a language does have written text, gathering enough text for the language can also be a problem: the language may not have much written text. This may especially be the case for endangered languages. Again, one solution is to manually create texts — using an NLP method to create these texts is not viable.\nIn short, for some languages, there may be no choice other than to manually create the data that would be fed into the system — this manual creation also has the chance to skew the performance of the model.\n\n\n\nKuş dili, a whistled language spoken in Turkey. How would such a language be preserved? Image Source\n\n\n\n\nCreate the model\nEither a chatbot needs to be created that speaks as accurately as a native speaker, or an IR system needs to be created that gives meaningful, correct insights into a language and its associated culture.\nThis step may either be easy or hard, depending on the language. Most NLP or IR systems have been built on a few, select languages. Perhaps this step may be easy for those languages that are similar to languages on which NLP or IR systems have already been built on. It will most likely be harder for those languages which are not."
  },
  {
    "objectID": "forblog/posts/1_how_to_approach_creating_ai_models.html#conclusion",
    "href": "forblog/posts/1_how_to_approach_creating_ai_models.html#conclusion",
    "title": "How to Approach Creating AI Models",
    "section": "Conclusion",
    "text": "Conclusion\nThis concludes my understanding of the Drivetrain Approach, through an example.\nApproaches are crucial: you can have state-of-the-art tools, but they are useless if not correctly applied. The approach you take can either make it or break it. Putting it into a concrete, organized, tangible manner goes a long way.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "",
    "text": "This notebook follows the fastai style guide.\nIn this notebook, we’ll implement stable diffusion from its various components through the Hugging Face Diffusers library.\nAt the end, we’ll have our own custom stable diffusion class, from which we can generate images as simply as diffuser.diffuse().\nIf you would like a refresher, I’ve summarized at a high level how a diffuser is trained in this post. Though this notebook focuses on inference and not the training aspect, the linked summary may be helpful.\nLet’s begin."
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#overview",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#overview",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Overview",
    "text": "Overview\nBefore we get hands on with the code, let’s refresh how inference works for a diffuser.\n\nWe input a prompt to the diffuser.\nThis prompt is given a mathematical representation (an embedding) through the text encoder.\nA latent comprised of noise is produced.\nThe U-Net predicts the noise in the latent in conjunction with the prompt.\nThe predicted noise is subtracted from the latent in conjunction with the scheduler.\nAfter many iterations, the denoised latent is decompressed to produce our final generated image.\n\nThe main components in use are:\n\na text encoder,\na U-Net,\nand a VAE decoder."
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#setup",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#setup",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Setup",
    "text": "Setup\n\n! pip install -Uqq fastcore transformers diffusers\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 40.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 934.9/934.9 kB 57.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 23.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 29.1 MB/s eta 0:00:00\n\n\n\n1import logging; logging.disable(logging.WARNING)\nfrom fastcore.all import *\nfrom fastai.imports import *\nfrom fastai.vision.all import *\n\n\n1\n\nHugging Face can be very verbose."
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#get-components",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#get-components",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Get Components",
    "text": "Get Components\n\nCLIP Components\nTo process the prompt, we need to download a tokenizer and a text encoder. The tokenizer will split the prompt into tokens while the text encoder will convert the tokens into a numerical representation (an embedding).\n\nfrom transformers import CLIPTokenizer, CLIPTextModel\n\ntokz = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14', torch_dtype=torch.float16)\ntxt_enc = CLIPTextModel.from_pretrained('openai/clip-vit-large-patch14', torch_dtype=torch.float16).to('cuda')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfloat16 is used for faster performance.\n\n\nU-Net and VAE\nThe U-Net will predict the noise in the image, while the VAE will decompress the generated image.\n\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\n\nvae = AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema', torch_dtype=torch.float16).to('cuda')\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScheduler\nThe scheduler will control how much noise is intially added to the image, and will also control how much of the noise predicted from the U-Net will be subtracted from the image.\n\nfrom diffusers import LMSDiscreteScheduler\n\nsched = LMSDiscreteScheduler(\n    beta_start = 0.00085,\n    beta_end = 0.012,\n    beta_schedule = 'scaled_linear',\n    num_train_timesteps = 1000\n); sched\n\nLMSDiscreteScheduler {\n  \"_class_name\": \"LMSDiscreteScheduler\",\n  \"_diffusers_version\": \"0.16.1\",\n  \"beta_end\": 0.012,\n  \"beta_schedule\": \"scaled_linear\",\n  \"beta_start\": 0.00085,\n  \"num_train_timesteps\": 1000,\n  \"prediction_type\": \"epsilon\",\n  \"trained_betas\": null\n}"
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#define-generation-parameters",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#define-generation-parameters",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Define Generation Parameters",
    "text": "Define Generation Parameters\nThe six main parameters needed for generation are:\n\nThe prompt\nThe width and height of the image\nA number describing how noisy the output image is to be (the number of inference steps)\nA number describing how much the diffuser should stick to the prompt (the guidance scale)\nThe batch size\nThe seed\n\n\nprompt = ['a photograph of an astronaut riding a horse']\nw, h = 512, 512\nn_inf_steps = 70\ng_scale = 7.5\nbs = 1\nseed = 77"
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#encode-prompt",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#encode-prompt",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Encode Prompt",
    "text": "Encode Prompt\nNow we need to parse the prompt. To do so, we’ll first tokenize it, and then encode the tokens to produce an embedding.\nFirst, let’s tokenize.\n\ntxt_inp = tokz(\n    prompt,\n    padding = 'max_length',\n    max_length = tokz.model_max_length,\n    truncation = True,\n    return_tensors = 'pt'\n); txt_inp\n\n{'input_ids': tensor([[49406,   320,  8853,   539,   550, 18376,  6765,   320,  4558, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0]])}\n\n\nThe token 49407 is a padding token and represents '&lt;|endoftext|&gt;'. These tokens have been given an attention mask of 0.\n\ntokz.decode(49407)\n\n'&lt;|endoftext|&gt;'\n\n\nNow using the text encoder, we’ll create an embedding out of these tokens.\n\ntxt_emb = txt_enc(txt_inp['input_ids'].to('cuda'))[0].half(); txt_emb\n\ntensor([[[-0.3884,  0.0229, -0.0523,  ..., -0.4902, -0.3066,  0.0674],\n         [ 0.0292, -1.3242,  0.3076,  ..., -0.5254,  0.9766,  0.6655],\n         [ 0.4609,  0.5610,  1.6689,  ..., -1.9502, -1.2266,  0.0093],\n         ...,\n         [-3.0410, -0.0674, -0.1777,  ...,  0.3950, -0.0174,  0.7671],\n         [-3.0566, -0.1058, -0.1936,  ...,  0.4258, -0.0184,  0.7588],\n         [-2.9844, -0.0850, -0.1726,  ...,  0.4373,  0.0092,  0.7490]]],\n       device='cuda:0', dtype=torch.float16, grad_fn=&lt;NativeLayerNormBackward0&gt;)\n\n\n\ntxt_emb.shape\n\ntorch.Size([1, 77, 768])"
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#embeddings-for-cfg",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#embeddings-for-cfg",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Embeddings for CFG",
    "text": "Embeddings for CFG\nWe also need to create an embedding for an empty prompt, also known as the uncondtional prompt. This embedding is what is used to control the guidance.\n\ntxt_inp['input_ids'].shape\n\ntorch.Size([1, 77])\n\n\n\n1max_len = txt_inp['input_ids'].shape[-1]\nuncond_inp = tokz(\n2    [''] * bs,\n    padding = 'max_length',\n    max_length = max_len,\n    return_tensors = 'pt',\n); uncond_inp\n\n\n1\n\nWe use the maximum length of the prompt so the unconditional prompt embedding matches the size of the text prmpt embedding.\n\n2\n\nWe also multiply the list containing the empty prompt with the batch size so we have an empty prompt for each text prompt.\n\n\n\n\n{'input_ids': tensor([[49406, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0]])}\n\n\n\nuncond_inp['input_ids'].shape\n\ntorch.Size([1, 77])\n\n\n\nuncond_emb = txt_enc(uncond_inp['input_ids'].to('cuda'))[0].half()\nuncond_emb.shape\n\ntorch.Size([1, 77, 768])\n\n\nWe can then concatenate both the unconditonal embedding and the text embedding together. This allows images to be generated from each prompt without having to go through the U-Net twice.\n\nembs = torch.cat([uncond_emb, txt_emb])"
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#create-noisy-image",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#create-noisy-image",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Create Noisy Image",
    "text": "Create Noisy Image\nIt’s now time to create our noisy image, which will be the starting point for generation.\nWe’ll create a single latent that is 64 by 64 pixels, and that also has 4 channels. After the latent is denoised, we’ll decompress it to a 512 by 512 pixel image with 3 channels.\n\nbs, unet.config.in_channels, h//8, w//8\n\n(1, 4, 64, 64)\n\n\n\nprint(torch.randn((2, 3, 4)))\nprint(torch.randn((2, 3, 4)).shape)\n\ntensor([[[ 0.0800, -1.3597, -0.2033, -0.5647],\n         [-1.6066,  0.8178,  1.0832,  0.0638],\n         [ 0.3133,  1.8516,  0.4320, -0.9295]],\n\n        [[-1.0798,  3.2928,  0.7443,  1.2190],\n         [-0.4984,  0.3551, -0.6012, -0.5856],\n         [-0.3988, -1.2950, -1.6061, -0.0207]]])\ntorch.Size([2, 3, 4])\n\n\n\ntorch.manual_seed(seed)\nlats = torch.randn((bs, unet.config.in_channels, h//8, w//8)); lats.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\nThe latent is a rank 4 tensor. 1 refers to the batch size, which is the number of images being generated. 4 is the number of channels, and 64 is the number of pixel with regard to both height and width.\n\nlats = lats.to('cuda').half(); lats\n\ntensor([[[[-0.5044, -0.4163, -0.1365,  ..., -1.6104,  0.1381,  1.7676],\n          [ 0.7017,  1.5947, -1.4434,  ..., -1.5859, -0.4089, -2.8164],\n          [ 1.0664, -0.0923,  0.3462,  ..., -0.2390, -1.0947,  0.7554],\n          ...,\n          [-1.0283,  0.2433,  0.3337,  ...,  0.6641,  0.4219,  0.7065],\n          [ 0.4280, -1.5439,  0.1409,  ...,  0.8989, -1.0049,  0.0482],\n          [-1.8682,  0.4988,  0.4668,  ..., -0.5874, -0.4019, -0.2856]],\n\n         [[ 0.5688, -1.2715, -1.4980,  ...,  0.2230,  1.4785, -0.6821],\n          [ 1.8418, -0.5117,  1.1934,  ..., -0.7222, -0.7417,  1.0479],\n          [-0.6558,  0.1201,  1.4971,  ...,  0.1454,  0.4714,  0.2441],\n          ...,\n          [ 0.9492,  0.1953, -2.4141,  ..., -0.5176,  1.1191,  0.5879],\n          [ 0.2129,  1.8643, -1.8506,  ...,  0.8096, -1.5264,  0.3191],\n          [-0.3640, -0.9189,  0.8931,  ..., -0.4944,  0.3916, -0.1406]],\n\n         [[-0.5259,  1.5059, -0.3413,  ...,  1.2539,  0.3669, -0.1593],\n          [-0.2957, -0.1169, -2.0078,  ...,  1.9268,  0.3833, -0.0992],\n          [ 0.5020,  1.0068, -0.9907,  ..., -0.3008,  0.7324, -1.1963],\n          ...,\n          [-0.7437, -1.1250,  0.1349,  ..., -0.6714, -0.6753, -0.7920],\n          [ 0.5415, -0.5269, -1.0166,  ...,  1.1270, -1.7637, -1.5156],\n          [-0.2319,  0.9165,  1.6318,  ...,  0.6602, -1.2871,  1.7568]],\n\n         [[ 0.7100,  0.4133,  0.5513,  ...,  0.0326,  0.9175,  1.4922],\n          [ 0.8862,  1.3760,  0.8599,  ..., -2.1172, -1.6533,  0.8955],\n          [-0.7783, -0.0246,  1.4717,  ...,  0.0328,  0.4316, -0.6416],\n          ...,\n          [ 0.0855, -0.1279, -0.0319,  ..., -0.2817,  1.2744, -0.5854],\n          [ 0.2402,  1.3945, -2.4062,  ...,  0.3435, -0.5254,  1.2441],\n          [ 1.6377,  1.2539,  0.6099,  ...,  1.5391, -0.6304,  0.9092]]]],\n       device='cuda:0', dtype=torch.float16)\n\n\nOur latent has random values which represent noise. This noise needs to be scaled so it can work with the scheduler.\n\nsched.set_timesteps(n_inf_steps); sched\n\nLMSDiscreteScheduler {\n  \"_class_name\": \"LMSDiscreteScheduler\",\n  \"_diffusers_version\": \"0.16.1\",\n  \"beta_end\": 0.012,\n  \"beta_schedule\": \"scaled_linear\",\n  \"beta_start\": 0.00085,\n  \"num_train_timesteps\": 1000,\n  \"prediction_type\": \"epsilon\",\n  \"trained_betas\": null\n}\n\n\n\nlats *= sched.init_noise_sigma; sched.init_noise_sigma\n\ntensor(14.6146)\n\n\n\nsched.sigmas\n\ntensor([14.6146, 13.3974, 12.3033, 11.3184, 10.4301,  9.6279,  8.9020,  8.2443,\n         7.6472,  7.1044,  6.6102,  6.1594,  5.7477,  5.3709,  5.0258,  4.7090,\n         4.4178,  4.1497,  3.9026,  3.6744,  3.4634,  3.2680,  3.0867,  2.9183,\n         2.7616,  2.6157,  2.4794,  2.3521,  2.2330,  2.1213,  2.0165,  1.9180,\n         1.8252,  1.7378,  1.6552,  1.5771,  1.5031,  1.4330,  1.3664,  1.3030,\n         1.2427,  1.1852,  1.1302,  1.0776,  1.0272,  0.9788,  0.9324,  0.8876,\n         0.8445,  0.8029,  0.7626,  0.7236,  0.6858,  0.6490,  0.6131,  0.5781,\n         0.5438,  0.5102,  0.4770,  0.4443,  0.4118,  0.3795,  0.3470,  0.3141,\n         0.2805,  0.2455,  0.2084,  0.1672,  0.1174,  0.0292,  0.0000])\n\n\n\nsched.timesteps\n\ntensor([999.0000, 984.5217, 970.0435, 955.5652, 941.0870, 926.6087, 912.1304,\n        897.6522, 883.1739, 868.6957, 854.2174, 839.7391, 825.2609, 810.7826,\n        796.3043, 781.8261, 767.3478, 752.8696, 738.3913, 723.9130, 709.4348,\n        694.9565, 680.4783, 666.0000, 651.5217, 637.0435, 622.5652, 608.0870,\n        593.6087, 579.1304, 564.6522, 550.1739, 535.6957, 521.2174, 506.7391,\n        492.2609, 477.7826, 463.3043, 448.8261, 434.3478, 419.8696, 405.3913,\n        390.9130, 376.4348, 361.9565, 347.4783, 333.0000, 318.5217, 304.0435,\n        289.5652, 275.0870, 260.6087, 246.1304, 231.6522, 217.1739, 202.6957,\n        188.2174, 173.7391, 159.2609, 144.7826, 130.3043, 115.8261, 101.3478,\n         86.8696,  72.3913,  57.9130,  43.4348,  28.9565,  14.4783,   0.0000],\n       dtype=torch.float64)\n\n\n\nplt.plot(sched.timesteps, sched.sigmas[:-1])"
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#denoise",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#denoise",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Denoise",
    "text": "Denoise\nThe denoising process can now begin!\n\nfrom tqdm.auto import tqdm\n\nfor i, ts in enumerate(tqdm(sched.timesteps)):\n1  inp = torch.cat([lats] * 2)\n2  inp = sched.scale_model_input(inp, ts)\n\n3  with torch.no_grad(): preds = unet(inp, ts, encoder_hidden_states=embs).sample\n\n4  pred_uncond, pred_txt = preds.chunk(2)\n  pred = pred_uncond + g_scale * (pred_txt - pred_uncond)\n\n5  lats = sched.step(pred, ts, lats).prev_sample\n\n\n1\n\nWe first create two latents: one for the text prompt, and one for the unconditional prompt.\n\n2\n\nWe then further scale the noise on the latents.\n\n3\n\nWe then predict noise.\n\n4\n\nWe then perform guidance.\n\n5\n\nWe then subtract the predicted, guided noise from the image."
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#decompress",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#decompress",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Decompress",
    "text": "Decompress\nWe can now decompress the latent and display it.\n\nwith torch.no_grad(): img = vae.decode(1/0.18215*lats).sample\n\n\nimg = (img / 2 + 0.5).clamp(0, 1)\nimg = img[0].detach().cpu().permute(1, 2, 0).numpy()\nimg = (img * 255).round().astype('uint8')\nImage.fromarray(img)\n\n\n\n\nAnd there you have it! We implemented stable diffusion using a text encoder, VAE, and U-Net!\nLet’s encapsulate everything so it looks simpler."
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#encapsulation",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#encapsulation",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Encapsulation",
    "text": "Encapsulation\nFirst we’ll encapsulate everything into functions, then we’ll encapsulate into a class.\n\nFunctions\nThe main steps that are happening are:\n\nWe create embeddings.\nWe create latents.\nWe denoise the latents.\nWe decompress the latents.\n\n\nCreate Embeddings\n\ndef set_embs():\n  txt_inp = tok_seq(prompt)\n  uncond_inp = tok_seq(['']*len(prompt), max_len=txt_inp['input_ids'].shape[-1])\n\n  txt_emb = make_emb(txt_inp['input_ids'])\n  uncond_emb = make_emb(uncond_inp['input_ids'])\n  return torch.cat([uncond_emb, txt_emb])\n\ndef tok_seq(prompt, max_len=None):\n  if max_len is None: max_len = tokz.model_max_length\n  return tokz(\n      prompt,\n      padding = 'max_length',\n      max_length = max_len,\n      truncation = True,\n      return_tensors = 'pt'\n  )\n\ndef make_emb(input_ids):\n  return txt_enc(input_ids.to('cuda'))[0].half()\n\n\n\nCreate Latents\n\ndef set_lat():\n  torch.manual_seed(seed)\n  lat = torch.randn((bs, unet.config.in_channels, h//8, w//8))\n  sched.set_timesteps(n_inf_steps)\n  return lat.to('cuda').half() * sched.init_noise_sigma\n\n\n\nDenoise Latents\n\ndef denoise(latent, embeddings, timestep):\n  inp = sched.scale_model_input(torch.cat([latent]*2), timestep)\n  with torch.no_grad():\n    pred_uncond, pred_txt = unet(inp, timestep, encoder_hidden_states=embeddings).sample.chunk(2)\n  pred = pred_uncond + g_scale * (pred_txt - pred_uncond)\n  return sched.step(pred, timestep, latent).prev_sample\n\n\n\nDecompress Latents\n\ndef decompress_lat(latent):\n  with torch.no_grad(): img = vae.decode(1/0.18215*latent).sample\n  img = (img / 2 + 0.5).clamp(0, 1)\n  img = img[0].detach().cpu().permute(1, 2, 0).numpy()\n  return (img * 255).round().astype('uint8')\n\n\n\nPutting it All Together\n\nprompt = ['An antique 18th century painting of a gorilla eating a plate of chips.']\nembs = set_embs()\nlat = set_lat()\nfor i, ts in enumerate(tqdm(sched.timesteps)): lat = denoise(lat, embs, ts)\nimg = decompress_lat(lat)\nImage.fromarray(img)\n\n\n\n\n\n\n\n\n\nNegative Prompts.\nTo implement negative prompts, we can simply pass in a list containing the negative prompt. This will be used in place of the empty list used for the unconditional prompt.\n\ndef set_embs():\n  txt_inp = tok_seq(prompt)\n  uncond_inp = tok_seq(neg_prompt*len(prompt), max_len=txt_inp['input_ids'].shape[-1])\n\n  txt_emb = make_emb(txt_inp['input_ids'])\n  uncond_emb = make_emb(uncond_inp['input_ids'])\n  return torch.cat([uncond_emb, txt_emb])\n\ndef tok_seq(prompt, max_len=None):\n  if max_len is None: max_len = tokz.model_max_length\n  return tokz(\n      prompt,\n      padding = 'max_length',\n      max_length = max_len,\n      truncation = True,\n      return_tensors = 'pt'\n  )\n\n\nprompt = ['An antique 18th century painting of a gorilla eating a plate of chips.']\nneg_prompt = ['plate']\nembs = set_embs()\nlat = set_lat()\nfor i, ts in enumerate(tqdm(sched.timesteps)): lat = denoise(lat, embs, ts)\nimg = decompress_lat(lat)\nImage.fromarray(img)\n\n\n\n\n\n\n\nLet’s now encapsulate everything into a class, so we can much more easily further iterate.\n\n\n\nClass\nI’ll be tweaking the code above so that multiple prompts can be input.\nThis is as simple as using the length of the list of prompts as the batch size (an image is generated for each prompt).\n\nclass Diffuser:\n  def __init__(self, prompts, neg_prompt=[''], guidance=7.5, seed=100, steps=70, width=512, height=512):\n    self.prompts = prompts\n    self.bs = len(prompts)\n    self.neg_prompt = neg_prompt\n    self.g = guidance\n    self.seed = seed\n    self.steps = steps\n    self.w = width\n    self.h = height\n  \n  def diffuse(self, progress=0):\n    embs = self.set_embs()\n    lats = self.set_lats()\n    for i, ts in enumerate(tqdm(sched.timesteps)): lats = self.denoise(lats, embs, ts)\n    return self.decompress_lats(lats)\n  \n  def set_embs(self):\n    txt_inp = self.tok_seq(self.prompts)\n    neg_inp = self.tok_seq(self.neg_prompt * len(self.prompts))\n\n    txt_embs = self.make_embs(txt_inp['input_ids'])\n    neg_embs = self.make_embs(neg_inp['input_ids'])\n    return torch.cat([neg_embs, txt_embs])\n  \n  def tok_seq(self, prompts, max_len=None):\n    if max_len is None: max_len = tokz.model_max_length\n    return tokz(prompts, padding='max_length', max_length=max_len, truncation=True, return_tensors='pt')    \n  \n  def make_embs(self, input_ids):\n    return txt_enc(input_ids.to('cuda'))[0].half()\n\n  def set_lats(self):\n    torch.manual_seed(self.seed)\n    lats = torch.randn((self.bs, unet.config.in_channels, self.h//8, self.w//8))\n    sched.set_timesteps(self.steps)\n    return lats.to('cuda').half() * sched.init_noise_sigma\n\n  def denoise(self, latents, embeddings, timestep):\n    inp = sched.scale_model_input(torch.cat([latents]*2), timestep)\n    with torch.no_grad(): pred_neg, pred_txt = unet(inp, timestep, encoder_hidden_states=embeddings).sample.chunk(2)\n    pred = pred_neg + self.g * (pred_txt - pred_neg)\n    return sched.step(pred, timestep, latents).prev_sample\n\n  def decompress_lats(self, latents):\n    with torch.no_grad(): imgs = vae.decode(1/0.18215*latents).sample\n    imgs = (imgs / 2 + 0.5).clamp(0, 1)\n    imgs = [img.detach().cpu().permute(1, 2, 0).numpy() for img in imgs]\n    return [(img*255).round().astype('uint8') for img in imgs]\n\n  def update_params(self, **kwargs):\n    allowed_params = ['prompts', 'neg_prompt', 'guidance', 'seed', 'steps', 'width', 'height']\n    for k, v in kwargs.items():\n      if k not in allowed_params:\n        raise ValueError(f\"Invalid parameter name: {k}\")\n      if k == 'prompts':\n        self.prompts = v\n        self.bs = len(v)\n      else:\n        setattr(self, k, v)\n\nNow creating a diffuser is as simple as this!\n\nprompts = [\n    'A lightning bolt striking a jumbo jet; 4k; photorealistic',\n    'A toaster in the style of Jony Ive; modern; different; apple; form over function'\n]\ndiffuser = Diffuser(prompts, seed=42)\nimgs = diffuser.diffuse()\n\n\n\n\n\nImage.fromarray(imgs[0])\n\n\n\n\n\nImage.fromarray(imgs[1])\n\n\n\n\nLet’s remove the wooden background from the second image.\n\nprompt = [prompts[1]]\ndiffuser.update_params(prompts=prompt, neg_prompt='wood')\nImage.fromarray(diffuser.diffuse()[0])\n\n\n\n\n\n\n\nNow that we have a class, we can easily add more functionality to our diffuser."
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#extra-functionality",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#extra-functionality",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Extra Functionality",
    "text": "Extra Functionality\n\nCallbacks\nLet’s make the diffuser output how the generated image looks like at each step interval (e.g., every 5 steps), if specified so.\nTo do so, we can simply tweak the diffuser.diffuse() method by make it output the latent at each desired interval.\ndef diffuse(self, interval=0)\n  embs = self.set_embs()\n  lats = self.set_lats()\n1  if interval &gt; 0:\n2    row = []\n    for i, ts in enumerate(tqdm(sched.timesteps)):\n      lats = self.denoise(lats, embs, ts)\n3      if (i % progress) == 0:\n        row.append(self.decompress_lats(lats)[0])\n4    row = np.concatenate(row, axis=1)\n5    display(Image.fromarray(row))\n  else:\n    for i, ts in enumerate(tqdm(sched.timesteps)): lats = self.denoise(lats, embs, ts)\n  return self.decompress_lats(lats)\n\n1\n\nWe first check if callbacks are desired (we can’t save how the latents looked like every 0 intervals).\n\n2\n\nAn empty list is created to store the images.\n\n3\n\nWe check if we have reached our desired interval. If the current loop number matches the interval, it should divide the interval cleanly.\n\n4\n\nWe smoosh all images into one long line.\n\n5\n\nThe image is displayed.\n\n\n\n\nDiffuser Class Redefined\nclass Diffuser:\n  def __init__(self, prompts, neg_prompt=[''], guidance=7.5, seed=100, steps=70, width=512, height=512):\n    self.prompts = prompts\n    self.bs = len(prompts)\n    self.neg_prompt = neg_prompt\n    self.g = guidance\n    self.seed = seed\n    self.steps = steps\n    self.w = width\n    self.h = height\n  \n  def diffuse(self, interval=0):\n    embs = self.set_embs()\n    lats = self.set_lats()\n    if interval &gt; 0:\n      row = []\n      for i, ts in enumerate(tqdm(sched.timesteps)):\n        lats = self.denoise(lats, embs, ts)\n        if (i % interval) == 0: row.append(self.decompress_lats(lats)[0])\n      row = np.concatenate(row, axis=1)\n      display(Image.fromarray(row))\n    else: \n      for i, ts in enumerate(tqdm(sched.timesteps)): lats = self.denoise(lats, embs, ts)\n    return self.decompress_lats(lats)\n  \n  def set_embs(self):\n    txt_inp = self.tok_seq(self.prompts)\n    neg_inp = self.tok_seq(self.neg_prompt * len(self.prompts))\n\n    txt_embs = self.make_embs(txt_inp['input_ids'])\n    neg_embs = self.make_embs(neg_inp['input_ids'])\n    return torch.cat([neg_embs, txt_embs])\n  \n  def tok_seq(self, prompts, max_len=None):\n    if max_len is None: max_len = tokz.model_max_length\n    return tokz(prompts, padding='max_length', max_length=max_len, truncation=True, return_tensors='pt')    \n  \n  def make_embs(self, input_ids):\n    return txt_enc(input_ids.to('cuda'))[0].half()\n\n  def set_lats(self):\n    torch.manual_seed(self.seed)\n    lats = torch.randn((self.bs, unet.config.in_channels, self.h//8, self.w//8))\n    sched.set_timesteps(self.steps)\n    return lats.to('cuda').half() * sched.init_noise_sigma\n\n  def denoise(self, latents, embeddings, timestep):\n    inp = sched.scale_model_input(torch.cat([latents]*2), timestep)\n    with torch.no_grad(): pred_neg, pred_txt = unet(inp, timestep, encoder_hidden_states=embeddings).sample.chunk(2)\n    pred = pred_neg + self.g * (pred_txt - pred_neg)\n    return sched.step(pred, timestep, latents).prev_sample\n\n  def decompress_lats(self, latents):\n    with torch.no_grad(): imgs = vae.decode(1/0.18215*latents).sample\n    imgs = (imgs / 2 + 0.5).clamp(0, 1)\n    imgs = [img.detach().cpu().permute(1, 2, 0).numpy() for img in imgs]\n    return [(img*255).round().astype('uint8') for img in imgs]\n\n  def update_params(self, **kwargs):\n    allowed_params = ['prompts', 'neg_prompt', 'guidance', 'seed', 'steps', 'width', 'height']\n    for key, value in kwargs.items():\n      if key not in allowed_params:\n        raise ValueError(f\"Invalid parameter name: {key}\")\n      if key == 'prompts':\n        self.prompts = value\n        self.bs = len(value)\n      else:\n        setattr(self, key, value)\n\n\n\nprompt = ['A toaster in the style of Jony Ive; modern; realistic; different; apple; form over function']\ndiffuser = Diffuser(prompts=prompt, neg_prompt=['wood'], seed=42)\nImage.fromarray(diffuser.diffuse(interval=7)[0])"
  },
  {
    "objectID": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#conclusion",
    "href": "forblog/posts/13_implementing_stable_diffusion_from_its_components.html#conclusion",
    "title": "Implementing Stable Diffusion From Its Components",
    "section": "Conclusion",
    "text": "Conclusion\nAnd there you have it! All that’s happening is:\n\nA compressed, noisy image is generated.\nThe noise in the image is predicted.\nThe predicted noise is subtracted.\nThis is repeated until desired.\nThe final image is decompressed.\n\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/4_data_quality_is_important.html",
    "href": "forblog/posts/4_data_quality_is_important.html",
    "title": "Data Quality is Important | Car Classifier",
    "section": "",
    "text": "This article was updated on Thursday, 10 November 2022.\n\nI recently created a car classifier that classified cars into their respective brands.\nDespite having almost 5000 images in my training set, I ended up trying out over a hundred layers in my model, and twenty epochs. Even then, I had an error rate of 17.4%.\nThe culprit? My dataset.\nI scraped 5000 images of cars (500 for each company) from DuckDuckGo. Naturally, as expected, the data quality is not so good.\nWhy? Below are some potential reasons:\n\nNoncar images present in dataset\nCars of incorrect company present in dataset\nF1 cars present in dataset\nA large variety of cars from different time periods present in dataset\nDifferent companys’ cars look similar\nModded cars present in dataset\nConcept cars present in dataset\nMultiple cars present in a single image\nCertain angles of cars appear more than others\nCars appear in certain backgrounds more than others\nThe search term {car_brand} car could be skewing results\n\nI could have absolutely achieved better results with fewer layers and fewer epochs if I trained the model on better quality data — or manually combed through the 5000 images 💀. However, I did use fastai’s GUI for data cleaning. This GUI sorts images by their loss which helps to determine if certain images should be relabeled or deleted.\nBelow is the confusion matrix for this model.\n\nIt can be seen that this model “confuses” between quite a few different brands: Ford and Chevrolet, Chevrolet and Ford, Jaguar and Aston Martin, Renault and Ford.\nBut why is data quality important? Because without good data, the model will not be able to “see” things the way they actually are, and in turn end up making worse predictions and not generalize to other data.\nLet’s say you did not know how, say, a toaster looked like. So I taught you by showing you pictures of a kettle. Then to test you, I showed you a set of pictures depicting various kitchen appliances and told you to find the toaster. You would not be able to.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtending upon this example, say I showed you toasters only from the last two years and from two brands only. You would not be able to identify toasters older than two years, and toasters from other brands to much success.\nObviously, humans are smarter and can infer. AI methods can only infer to a certain degree, mainly based on what is in their dataset. This talk does start to become more philosophical.\nThe point of this post is to emphasize the importance of data quality and different aspects to consider as to why data quality may not be good. You can have the best architecture in the world, but it is useless if you do not have good data.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!\n\n\n\n Back to top"
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html",
    "href": "forblog/posts/2_bear_classifier_model.html",
    "title": "My first AI model",
    "section": "",
    "text": "This article was updated on Tuesday, 1 November 2022."
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#introduction",
    "href": "forblog/posts/2_bear_classifier_model.html#introduction",
    "title": "My first AI model",
    "section": "Introduction",
    "text": "Introduction\nThis is my first attempt at creating an AI model: an image classifier. This classifier can tell whether a grizzly bear, black bear, or teddy bear is in an image.\nYou can visit the classifier here to test it out for yourself!"
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#load-libraries",
    "href": "forblog/posts/2_bear_classifier_model.html#load-libraries",
    "title": "My first AI model",
    "section": "Load libraries",
    "text": "Load libraries\n\n# No need to fret! fastai is specifically designed to be used with import *.\nfrom fastbook import *\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#download-image-files",
    "href": "forblog/posts/2_bear_classifier_model.html#download-image-files",
    "title": "My first AI model",
    "section": "Download image files",
    "text": "Download image files\nSpecify the bear images we wish to download.\n\nbear_types = ('grizzly', 'black', 'teddy',)\npath = Path('bears')\n\nDownload 200 of each bear (search_images_ddg defaults to 200 URLs) and assign them to a specific directory.\n\nif not path.exists():\n    path.mkdir()\n    for bear_type in bear_types:\n        destination = (path / bear_type)\n        destination.mkdir(exist_ok=True)\n        urls = search_images_ddg(f\"{bear_type} bear\")\n        download_iamges(destination, urls=urls)\n\nCheck if our folder has the image files.\n\nfns = get_image_files(path)\nfns\n\n(#802) [Path('bears/grizzly/00000238.jpg'),Path('bears/grizzly/00000047.jpg'),Path('bears/grizzly/00000199.jpg'),Path('bears/grizzly/00000237.jpg'),Path('bears/grizzly/00000055.jpg'),Path('bears/grizzly/00000000.png'),Path('bears/grizzly/00000235.jpg'),Path('bears/grizzly/00000159.jpg'),Path('bears/grizzly/00000268.jpg'),Path('bears/grizzly/00000266.jpg')...]\n\n\nCheck for corrupt images.\n\ncorrupt_images = verify_images(fns)\ncorrupt_images\n\n(#0) []\n\n\nRemove corrupt images.\n\ncorrupt_images.map(pathlib.Path.unlink)\n\n(#0) []"
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#load-image-files",
    "href": "forblog/posts/2_bear_classifier_model.html#load-image-files",
    "title": "My first AI model",
    "section": "Load image files",
    "text": "Load image files\nThe DataBlock API for creates the necessary DataLoaders for us.\n\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128),\n)\n\nThe blocks parameter allows us to specify the independent and dependent variables.\nThe get_items parameter tells fastai how to obtain our data. We use the get_image_files function to obtain our images.\nThe splitter parameter allows us to tell fastai how to split our data into training and validation sets. Since our data is one big set, we use the RandomSplitter class and tell it to use 20% of our data as the validation set. We specify a seed so the same split occurs each time.\nThe get_y parameter obtains our labels. The parent_label function simply gets the name of the folder a file is in. Since we have organized our bear images into different folders, this will nicely handle our target labels.\nThe item_tfms parameter allows us to specify a transform to apply to our data. Since we want all our images to be of the same size, we use the Resize() class.\nWe now have a DataBlock object from which can load the data.\n\ndataloaders = bears.dataloaders(path)\n\nLet us view a few images in the validation set.\n\ndataloaders.valid.show_batch(max_n=4, nrows=1)"
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#data-augmentation",
    "href": "forblog/posts/2_bear_classifier_model.html#data-augmentation",
    "title": "My first AI model",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nData augmentation refers to creating random variations to our input data. This produces new data points based on the existing data points. This allows each data point to look different, without changing their meaning.\nTypical examples of image augmentation include rotation, flipping, perspective warping, brightness changing, and contrast changing.\n\nCropping\nThe validation set images shown above are cropped. We achieved this by specifying the Resize argument when defining the DataBlock. Resize crops images to the size specified.\nCropping results in detail being lost.\nAlternatively, we can squish or stretch images, or pad them to a desired size.\n\n\nSquishing/Stretching\nThe problem with squishing or stretching images is that the model will learn to “see” images the way they are not supposed to be.\n\nbears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish))\ndataloaders = bears.dataloaders(path)\ndataloaders.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n\nPadding\nBy padding, the image is surrounded typically by black, meaningless pixels. This results in extra, wasted computation.\n\nbears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeros'))\ndataloaders = bears.dataloaders(path)\ndataloaders.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\nThe best approach is to take random crops of different parts of the same image. This makes sure that the model does not miss out on any details whilst letting it “know” how an object fully looks like.\nBelow, we have unique=True so that the same image is repeated with different variations.\n\nbears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3))\ndataloaders = bears.dataloaders(path)\ndataloaders.train.show_batch(max_n=4, nrows=1, unique=True)\n\n\n\n\nfastai comes with a function that applies a variety of augmentations to images. This can allow a model to “see” and recognize images in a variety of scenarios.\n\nbears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2))\ndataloaders  = bears.dataloaders(path)\ndataloaders.train.show_batch(max_n=8, nrows=2, unique=True)\n\n\n\n\nI have not used RandomResizedCrop here so that the different augmentations can be seen more clearly. RandomResizedCrop will be used when the model is trained.\nbatch_tfms tells fastai that we want to use these transforms on a batch."
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#training-the-model",
    "href": "forblog/posts/2_bear_classifier_model.html#training-the-model",
    "title": "My first AI model",
    "section": "Training the model",
    "text": "Training the model\nWe do not have a lot of data. Only 200 images of each bear at most. Therefore, we will augment our images not only to get more data, but so that the model can recognize data in a variety of situations.\n\nbears = bears.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms(),\n)\ndataloaders = bears.dataloaders(path)\n\nWe will now create our learner and fine-tune it.\nWe will be using the ResNet18 architecture (which is a convolutional neural network, or CNN for short). Error rate will be the metric.\n\nlearn = cnn_learner(dataloaders, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.985666\n0.104632\n0.025000\n00:20\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.132230\n0.073527\n0.012500\n00:22\n\n\n1\n0.106222\n0.054833\n0.018750\n00:22\n\n\n2\n0.087129\n0.058497\n0.012500\n00:20\n\n\n3\n0.069890\n0.058845\n0.018750\n00:19\n\n\n\n\n\nOur model only has a 1.9% error rate! Not bad! Though it seems if I had done an extra epoch, the error rate may have gone down to 1.3%, judging by the previous epochs’ error rates."
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#visualizing-mistakes",
    "href": "forblog/posts/2_bear_classifier_model.html#visualizing-mistakes",
    "title": "My first AI model",
    "section": "Visualizing mistakes",
    "text": "Visualizing mistakes\nWe can visualize the mistakes the model is making by a confusion matrix.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n3 grizzly bears were misclassified as black bears.\nLet us see where the errors are occurring, so we can determine if they are due to a dataset problem or a model problem.\nTo do this, we will sort images by their loss.\n\ninterp.plot_top_losses(5, nrows=1)"
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#data-cleaning",
    "href": "forblog/posts/2_bear_classifier_model.html#data-cleaning",
    "title": "My first AI model",
    "section": "Data cleaning",
    "text": "Data cleaning\nThe intuitive approach to data cleaning is to do it before training the model. However, a trained model can help us clean the data. For example, we can see some mislabaled bears in the above cases.\nfastai includes a GUI for data cleaning. This GUI allows you to choose a category/label and its associated training and validation sets. It then shows you images in order of highest-loss first, from which you can select images for removal or relabeling.\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\nImageClassifierCleaner does not actually delete or relabel. It just returns the indices that are to be deleted or relabeled.\n\n# Delete images selected for deletion.\nfor index in cleaner.delete():\n    cleaner.fns[index].unlink()\n\n# Relabel images selected for relabeling.\nfor index, category in cleaner.change():\n    shutil.move(str(cleaner.fns[index]), path/category)\n\nWe can now retrain and better performance should be expected."
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#saving-the-model",
    "href": "forblog/posts/2_bear_classifier_model.html#saving-the-model",
    "title": "My first AI model",
    "section": "Saving the model",
    "text": "Saving the model\nA model consists of two parts: the architecture and the parameters.\nWhen we use the export() method, both of these are saved.\nThis method also saves the definition of our DataLoaders. This is done so that we do not have to redefine how to transform our data when the model is used in production.\nfastai uses our validation set DataLoader by default, so the data augmentation will not be applied, which is generally what is wanted.\nThe export() method creates a file named “export.pkl”.\n\nlearn.export()\n\nLet us check that the file exists.\n\npath = Path()\npath.ls(file_exts='.pkl')\n\n(#1) [Path('export.pkl')]\n\n\nIf you wish to deploy an app, this is the file you will need."
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#loading-the-model-for-inference",
    "href": "forblog/posts/2_bear_classifier_model.html#loading-the-model-for-inference",
    "title": "My first AI model",
    "section": "Loading the model for inference",
    "text": "Loading the model for inference\nNow obviously we do not need to load the model as we already have the learner variable. But I shall do so anyways.\n\nlearn_inf = load_learner(path/'export.pkl')\n\nWe generally do inference for a single image at a time.\n\nlearn_inf.predict('images/grizzly.jpg')\n\n\n\n\n('grizzly', TensorBase(1), TensorBase([1.4230e-06, 1.0000e+00, 3.9502e-08]))\n\n\nThree things have been returned: the predicted category, the index of the predicted category, and the probabilities of each category.\nThe order of each category is based on the order of the vocabulary of the DataLoaders; that is, the stored tuple of all possible categories.\nThe DataLoaders can be accessed as an attribute of the Learner.\n\nlearn_inf.dataloaders.vocab\n\n['black', 'grizzly', 'teddy']"
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#why-cnns-work-so-well",
    "href": "forblog/posts/2_bear_classifier_model.html#why-cnns-work-so-well",
    "title": "My first AI model",
    "section": "Why CNNs work so well",
    "text": "Why CNNs work so well\nThe ResNet18 architecture is a sort of CNN. Below is my understanding as to why CNNs work so well.\nA neural network is comprised of many layers. Each layer is comprised of many neurons. In a CNN, each neuron in the same layer is given the exact same weights, while being given different input data. This allows all neurons in a layer to fire upon detecting the same pattern.\nBecause of this, CNNs can become really good at detecting objects in various patterns, orientations, shapes, positions, and so on."
  },
  {
    "objectID": "forblog/posts/2_bear_classifier_model.html#conclusion",
    "href": "forblog/posts/2_bear_classifier_model.html#conclusion",
    "title": "My first AI model",
    "section": "Conclusion",
    "text": "Conclusion\nWell then, that wraps up my first deep learning model! I have to say, it is much easier than I thought it would be to implement a model. You do not need to go into the nitty gritty details of artificial intelligence. A high level understanding can suffice in the beginning. It is like playing a sport: you do not need to understand the physics to be able to play it.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/16_einstein_summation.html",
    "href": "forblog/posts/16_einstein_summation.html",
    "title": "Intuitively Approaching Einstein Summation Notation",
    "section": "",
    "text": "This post covers einstein summation notation syntax in terms of programming languages.\nEinstein summation notation (or einsum notation for short) is a handy way of writing various matrix operations in a succinct, universal manner. With it, you can probably forget all the various symbols and operators there are and stick to one common syntax, that once understood, can be more intuitive.\nFor example, matrix multiplication can be written as ik, kj -&gt; ij and a transpose of a matrix could be written as ij -&gt; ji.\nLet’s figure this out."
  },
  {
    "objectID": "forblog/posts/16_einstein_summation.html#general-rules",
    "href": "forblog/posts/16_einstein_summation.html#general-rules",
    "title": "Intuitively Approaching Einstein Summation Notation",
    "section": "General Rules",
    "text": "General Rules\nThe following are two general rules one can use to quickly write einsum notation.\n\nRepeating letters between input arrays means that values along those axes will be multiplied together.\n\n\nOmitting a letter from the output means that values along that axis will be summed.\n\nHowever, I don’t find these rules intuitive, even a little confusing. Why?\nMatrices have the order of row by column. A 2x3 matrix has 2 rows and 3 columns. When we perform matrix multiplication, we multiply each row in the first matrix with each column in the second matrix.\nHowever, when the rules above are used to denote matrix multiplication (\\(ik, kj \\rightarrow ij\\)), the order of a matrix appears to change.\n\nThe first matrix is \\(i \\times j\\). However, \\(j\\) appears to denote the rows meaning we end up with a column by row matrix. But even if we let \\(j\\) denote the columns, we end up doing dot products with each column in the first matrix and with each row in the second matrix.\nNot intuitive."
  },
  {
    "objectID": "forblog/posts/16_einstein_summation.html#a-more-intuitive-way",
    "href": "forblog/posts/16_einstein_summation.html#a-more-intuitive-way",
    "title": "Intuitively Approaching Einstein Summation Notation",
    "section": "A More Intuitive Way",
    "text": "A More Intuitive Way\nThe key to understanding einsum notation is to not think of axes, but of iterators. For example, \\(i\\) is an iterator that returns the rows of a matrix. \\(j\\) is an iterator that returns the columns of a matrix.\nLet’s begin with a relatively more simple example: the hadamard product (also known as the elementwise product or elementwise multiplication.)\n\nHadamard Product\nWe have the following two matrices.\n\\[\nA\n=\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix},\nB\n=\n\\begin{bmatrix}\n9 & 8 & 7 \\\\\n6 & 5 & 4 \\\\\n3 & 2 & 1\n\\end{bmatrix}\n\\]\nTo access the element 8 in matrix \\(A\\), we need to return the second row and first column. This can be denoted as \\(a_{21}\\). The first digit in the subscript refers to the row and the second refers to the column. We can refer to any entry generally as \\(a_{ij}\\).\nTaking the hadamard product looks like this.\n\\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix}\n\\odot\n\\begin{bmatrix}\n9 & 8 & 7 \\\\\n6 & 5 & 4 \\\\\n3 & 2 & 1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 \\cdot 9 & 2 \\cdot 8 & 3 \\cdot 7 \\\\\n4 \\cdot 6 & 5 \\cdot 5 & 6 \\cdot 4 \\\\\n7 \\cdot 3 & 8 \\cdot 2 & 9 \\cdot 1\n\\end{bmatrix}\n=\nC\n\\]\n\n\n\n\n\n\nAn alternative way to look at it…\n\n\n\n\n\n\n\n\n\n\n\nDon’t dwell too much on this; it may help to refer back to this later to help understand the einsum notation below.\n\n\n\n\\[\n\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\odot \\begin{bmatrix} 9 & 8 & 7 \\\\ 6 & 5 & 4 \\\\ 3 & 2 & 1 \\end{bmatrix} = \\begin{bmatrix} a_{00}b_{00} & a_{01}b_{01} & a_{02}b_{02} \\\\ a_{10}b_{10} & a_{11}b_{11} & a_{12}b_{12} \\\\ a_{20}b_{20} & a_{21}b_{21} & a_{22}b_{22} \\\\ \\end{bmatrix} = C\n\\]\n\n\n\nIn words, what’s happening is that we’re looping through all the rows of \\(A\\) and \\(B\\). For each row, we also loop through each column and multiply those columns together.\n\n\\(\\text{for row in } A \\text{ and } B\\)\n  \\(\\text{for col in } A \\text{ and } B\\)\n    \\(a_{\\text{rowcol}} \\cdot b_{\\text{rowcol}} = c_{\\text{rowcol}}\\)\n\n\n\\(\\text{for } i \\text{ in } A \\text{ and } B\\)\n  \\(\\text{for } j \\text{ in } A \\text{ and } B\\)\n    \\(a_{ij} \\cdot b_{ij} = c_{ij}\\)\n\nLet’s focus on that last line above.\n\\[a_{ij} \\cdot b_{ij} = c_{ij}\\]\nThis line represents elementwise multiplication. For each row \\(i\\) in \\(A\\) and \\(B\\), we iterate through each column \\(j\\) in those rows, and take their product.\nIn einsum notation, we can more succinctly write this as \\(ij, ij \\rightarrow ij\\). This has 4 parts.\n\n\\(ij\\) refers to the iterators working on the rows and columns of \\(A\\) — \\(i\\) works on the rows and \\(j\\) works on the columns.\n\\(ij\\) refers to the exact same iterators working on \\(B\\).\n\\(\\rightarrow\\) tells us an output will be returned.\n\\(ij\\) refers to the exact same iterators that will be responsble for making up the output matrix \\(C\\). The location \\(ij\\) says where the product of two elements will be located in \\(C\\).\n\n\n\nMatrix Multiplication\nLet’s cover matrix multiplication in the same manner as above.\n\\[\nA\n=\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n,\nB\n=\n\\begin{bmatrix}\n5 & 6 \\\\\n7 & 8\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n5 & 6 \\\\\n7 & 8\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n(1 \\cdot 5) + (2 \\cdot 7) & (1 \\cdot 6) + (2 \\cdot 8) \\\\\n(3 \\cdot 5) + (4 \\cdot 7) & (3 \\cdot 6) + (4 \\cdot 8)\n\\end{bmatrix}\n=\nC\n\\]\n\n\n\n\n\n\n\nAn alternative way to look at it…\n\n\n\n\n\n\n\n\n\n\n\nDon’t dwell too much on this; it may help to refer back to this later to help understand the einsum notation below.\n\n\n\n\\[\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n5 & 6 \\\\\n7 & 8\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n(a_{00}b_{00} + a_{01}b_{10}) & (a_{00}b_{01} + a_{01}b_{11}) \\\\\n(a_{10}b_{00} + a_{11}b_{10}) & (a_{10}b_{01} + a_{11}b_{11})\n\\end{bmatrix}\n=\nC\n\\]\n\n\n\nMatrix multiplication simply involves taking the dot product of each row in the first matrix with each column in the second matrix.\nWe’ll need to use 3 iterators for this: one iterator \\(i\\) to loop through the rows of \\(A\\), another iterator \\(j\\) to loop through the columns of \\(B\\), and a third iterator \\(k\\) to loop through the elements in a row and column.\n\n\\(\\text{for row in } A\\)\n  \\(\\text{for col in } B\\)\n    \\(\\text{for ele in } A_{\\text{row}} \\text{ and } B_{\\text{col}}\\)\n       \\(a_{\\text{rowele}} \\cdot b_{\\text{elecol}} \\mathrel{+}= c_{\\text{rowcol}}\\)\n\n\n\\(\\text{for } i \\text{ in } A\\)\n  \\(\\text{for } j \\text{ in } B\\)\n     \\(\\text{for } k \\text{ in } A_{i} \\text{ and } B_{j}\\)\n       \\(a_{ik} \\cdot b_{kj} \\mathrel{+}= c_{ij}\\)\n\nLet’s focus on in on the last line above.\n\\[\na_{ik} \\cdot b_{kj} \\mathrel{+}= c_{ij}\n\\]\nThis can more succinctly be written in einsum notation as \\(ik, kj \\rightarrow ij\\) — for each row \\(i\\) in \\(A\\), and for each column \\(j\\) in \\(B\\), iterate through each element \\(k\\), take their product, and sum the those products. The location of the output of the dot product in the output matrix \\(C\\) is \\(c_{ij}\\)."
  },
  {
    "objectID": "forblog/posts/16_einstein_summation.html#various-examples",
    "href": "forblog/posts/16_einstein_summation.html#various-examples",
    "title": "Intuitively Approaching Einstein Summation Notation",
    "section": "Various Examples",
    "text": "Various Examples\n\n1D Operations\n\\[\nA = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, B = \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix}\n\\]\n\nReturning a View of \\(A\\)\n\nPseudocode\nFor each row \\(i\\), output the row.\n\n\nEinsum Notation\n\\(i \\rightarrow i\\)\n\n\n\n\nSumming the Values of \\(A\\)\n\nPseudocode\nIterate through each row \\(i\\), and sum all rows.\n\n\nEinsum Notation\n\\(i \\rightarrow \\phantom{i}\\)\n\n\n\n\n\n\nA scalar is output, hence no output iterator.\n\n\n\n\n\n\n\nHadamard Product of \\(A\\) and \\(B\\)\n\nPseudocode\nFor each row \\(i\\) in \\(A\\) and \\(B\\), and multiply them together.\n\n\nEinsum Notation\n\\(i, i \\rightarrow i\\)\n\n\n\n\nDot Product of \\(A\\) and \\(B\\)\n\nPseudocode\nFor each row \\(i\\) in \\(A\\) and \\(B\\), multiply them together, and sum the products.\n\n\nEinsum Notation\n\\(i, i, \\rightarrow \\phantom{i}\\)\n\n\n\n\n\n\nA scalar is output, hence no output iterator.\n\n\n\n\n\n\n\nOuter Product of \\(A\\) and \\(B\\)\n\nPseudocode\nFor each row \\(i\\) in \\(A\\) and multiply it with each row \\(j\\) in \\(B\\).\n\n\nEinsum Notation\n\\(i, j \\rightarrow ij\\)\n\n\n\n\n\n\nExpanded example\n\n\n\n\n\nThe outer product involves multiplying each element in \\(A\\) with all elements in \\(B\\).\n\n\\(\\text{for row in } A\\)\n  \\(\\text{for another-row in } B\\)\n     \\(a_{\\text{row}} \\cdot b_{\\text{another-row}} = c_{\\text{rowanother-row}}\\)\n\n\n\\(\\text{for } i \\text{ in } A\\)\n   \\(\\text{for } j \\text{ in } B\\)\n     \\(a_{i} \\cdot b_{j} = c_{ij}\\)\n\n\n\n\n\n\n\n\n2D Operations\n\\[\nA\n=\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix}\n,\nB\n=\n\\begin{bmatrix}\n9 & 8 & 7 \\\\\n6 & 5 & 4 \\\\\n3 & 2 & 1\n\\end{bmatrix}\n\\]\n\nReturn a View of \\(A\\)\n\nPseudocode\nFor each row \\(i\\), iterate through each column \\(j\\) and output it.\n\n\nEinsum Notation\n\\(ij \\rightarrow ij\\)\n\n\n\n\nTranspose \\(A\\)\n\nPseudocode\nFor each row \\(i\\), iterate through each column \\(j\\) and output it in \\(C\\) at row \\(j\\) and column \\(i\\).\n\n\nEinsum Notation\n\\(ij \\rightarrow ji\\)\n\n\n\n\nReturn the Main Diagonal of \\(A\\)\n\nPseudocode\nFor each row \\(i\\), iterate through each column \\(i\\) and output it.\n\n\nEinsum Notation\n\\(ii \\rightarrow i\\)\n\n\n\n\nObtain the Trace of \\(A\\)\n\nPseudocode\nFor each row \\(i\\), iterate through each column \\(i\\) and sum them.\n\n\nEinsum Notation\n\\(ii \\rightarrow \\phantom{i}\\)\n\n\n\n\n\n\nA scalar is output, hence no output iterator.\n\n\n\n\n\n\n\nSum the Rows of \\(A\\)\n\nPseudocode\nFor each row \\(i\\), iterate through each column \\(j\\) and sum them.\n\n\nEinsum Notation\n\\(ij \\rightarrow j\\)\n\n\n\n\nSum the Columns of \\(A\\)\n\nPseudocode\nFor each column \\(j\\), iterate through each row \\(i\\) and sum them.\n\n\nEinsum Notation\n\\(ij -&gt; i\\)\n\n\n\n\nHadamard Product of \\(A\\) and \\(B\\)\n\nPseudocode\nFor each row \\(i\\) in \\(A\\) and \\(B\\), iterate throuch each column \\(j\\), and take their product.\n\n\nEinsum Notation\n\\(ij, ij \\rightarrow ij\\)\n\n\n\n\nHadamard Product of \\(A\\) and \\(B\\) Transposed (\\(A \\odot B^{T}\\))\n\nPseudocode\nFor each row \\(i\\) in \\(A\\), and for each row \\(j\\) in \\(B\\), iterate through each column \\(j\\) in \\(A\\) and each column \\(i\\) in \\(B\\), and take their product.\n\n\nEinsum Notation\n\\(ij, ji \\rightarray ij\\)\n\n\n\n\nMatrix Product of \\(A\\) and \\(B\\)\n\nPseudocode\nFor each row \\(i\\) in \\(A\\), and for each column \\(j\\) in \\(B\\), iterate through each element \\(k\\), take their product, and then sum those products.\n\n\nEinsum Notation\n\\(ik, kj \\rightarrow ij\\)\n\n\n\n\nEach Row of \\(A\\) Multiplied with \\(B\\)\n\nPseudocode\n\nFor each row \\(i\\) in \\(A\\), and for each row \\(j\\) in \\(B\\), iterate through each column \\(k\\) and take their product.\n\n\nEinsum Notation\n\\(ik, jk \\rightarrow ijk\\)\n\n\n\n\n\n\nA three dimensional tensor is output, hence the three output iterators.\n\n\n\n\n\n\n\nEvery Element of \\(A\\) Multiplied with \\(B\\)\n\nPseudocode\n\nFor each row \\(i\\) in \\(A\\), iterate through each column \\(j\\) and multiply it with each row \\(k\\) in \\(B\\) by iterating through each column \\(l\\) in that row \\(k\\).\n\n\nEinsum Notation\n\\(ij, kl \\rightarrow ijkl\\)\n\n\n\n\n\n\nA four dimensional tensor is output, hence the four output iterators."
  },
  {
    "objectID": "forblog/posts/16_einstein_summation.html#conclusion",
    "href": "forblog/posts/16_einstein_summation.html#conclusion",
    "title": "Intuitively Approaching Einstein Summation Notation",
    "section": "Conclusion",
    "text": "Conclusion\nAnd that’s that! The key is to think in terms of iterators that return locations in a matrix.\nIt may help to implement the operations above by yourself through pencil and paper, and in a programming languge too.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/11_musings_through_stable_diffusion.html",
    "href": "forblog/posts/11_musings_through_stable_diffusion.html",
    "title": "My Musings Through Stable Diffusion",
    "section": "",
    "text": "Quick tip: Click or tap the images to view them up close.\nI recently began fastai Course Part 2: a course where one dives into the deeper workings of deep learning by fully implementing stable diffusion.\nIn the first lesson, we play around with diffusers using the Hugging Face Diffusers library. Below are things I have noticed; my musings."
  },
  {
    "objectID": "forblog/posts/11_musings_through_stable_diffusion.html#steps",
    "href": "forblog/posts/11_musings_through_stable_diffusion.html#steps",
    "title": "My Musings Through Stable Diffusion",
    "section": "Steps",
    "text": "Steps\nDiffusion is simply a process whereby noise is progressively removed from a noisy image. A single step can be thought of a single portion of noise being removed.\n\n\n\nA depiction of a ring comprised of interwined serpents, topped with a single jewel of emerald.\n\n\nBelow is the evolution of the image above in 48 steps. Each new image has less and less noise (what the diffuser thinks is noise).\n\n\n\nThe gif itself has artefacts due to compression…\n\n\n\n\n\n\n\nIt still managed to generate a pretty good image despite the misspelling of “intertwined”!"
  },
  {
    "objectID": "forblog/posts/11_musings_through_stable_diffusion.html#when-it-doesnt-work-well",
    "href": "forblog/posts/11_musings_through_stable_diffusion.html#when-it-doesnt-work-well",
    "title": "My Musings Through Stable Diffusion",
    "section": "When It Doesn’t Work Well",
    "text": "When It Doesn’t Work Well\nI’ve found that a diffuser doesn’t work well when one prompts it for things, which I assume, it hasn’t “seen” or hasn’t been trained on before. It sounds obvious, but it’s really interesting when you see the result of it.\n\n\n\nA grasshopper riding a bunny.\n\n\n\n\n\n\n\nA quick Google search also doesn’t return any images matching the prompt in the top results."
  },
  {
    "objectID": "forblog/posts/11_musings_through_stable_diffusion.html#cfg-classifier-free-guidance",
    "href": "forblog/posts/11_musings_through_stable_diffusion.html#cfg-classifier-free-guidance",
    "title": "My Musings Through Stable Diffusion",
    "section": "CFG (Classifier Free Guidance)",
    "text": "CFG (Classifier Free Guidance)\nOr simply known as guidance, CFG is a value which tells the diffuser how much it should stick to the prompt.\nA lower guidance leads to more varied and random images that are loosely related to the prompt. A higher guidance produces more relevant images.\nI’ve found that too high of a guidenace leads to images having too much contrast.\n\n\n\nAn antique 18th century painting of a gorilla eating a plate of chips.\n\n\nThe image above shows rows with increasing levels of guidance (1, 2.5, 5, 7.5, 10, 25, 50). 7.5 is the sweetspot."
  },
  {
    "objectID": "forblog/posts/11_musings_through_stable_diffusion.html#negative-prompts",
    "href": "forblog/posts/11_musings_through_stable_diffusion.html#negative-prompts",
    "title": "My Musings Through Stable Diffusion",
    "section": "Negative Prompts",
    "text": "Negative Prompts\nThe best way to think about negative prompts is that a negative prompt guides a diffuser away from generating a certain entity.\nTake the image below as an example.\n\n\n\nAn antique 18th century painting of a gorilla eating a plate of chips.\n\n\nI generated the image again using the exact same seed and prompt, but also used the following negative prompt, “yellow circle”.\n\n\n\nPrompt: An antique 18th century painting of a gorilla eating a plate of chips. | Negative Prompt: yellow circle"
  },
  {
    "objectID": "forblog/posts/11_musings_through_stable_diffusion.html#image-to-image",
    "href": "forblog/posts/11_musings_through_stable_diffusion.html#image-to-image",
    "title": "My Musings Through Stable Diffusion",
    "section": "Image to Image",
    "text": "Image to Image\nInstead of starting from noise, one can make a diffuser begin from an existing image. The diffuser follows the image as guide and doesn’t match it 1 to 1.\nI quickly mocked up the following image.\n\n\n\n\n\nI input it to a diffuser with a prompt, and it output the following.\n\n\n\nA bench under a tree in a park\n\n\nI then further generated another image from this one.\n\n\n\nA low poly 3D render of a bench under a tree in a park"
  },
  {
    "objectID": "forblog/posts/11_musings_through_stable_diffusion.html#further-adapting-a-diffuser",
    "href": "forblog/posts/11_musings_through_stable_diffusion.html#further-adapting-a-diffuser",
    "title": "My Musings Through Stable Diffusion",
    "section": "Further Adapting a Diffuser",
    "text": "Further Adapting a Diffuser\nThere are two ways one can further customize a diffuser to produce desired images: textual inversion and dreambooth.\n\nTextual Inversion\nA diffuser contains a text encoder. This encoder is responsible for parsing the prompt and giving it a mathematical representation.\nA text encoder can only parse according to its vocabulary. If it encounters words not in its vocabulary, the diffuser will be unable to produce an image relevant to the prompt.\nIn a nutshell, textual inversion adds new words to the vocabulary of the text encoder so it can parse prompts with those new words.\nI managed to generate the image below by adding the word “Mr Doodle” to the vocabulary of the diffuser’s text encoder.\n\n\n\nAn antique 18th century painting of a gorilla eating a plate of chips in the style of Mr Doodle\n\n\n\n\nDreambooth\nDreambooth is more akin to traditional fine-tuning methods. A diffuser is further trained on images one supplies to it."
  },
  {
    "objectID": "forblog/posts/11_musings_through_stable_diffusion.html#so-end-my-musings",
    "href": "forblog/posts/11_musings_through_stable_diffusion.html#so-end-my-musings",
    "title": "My Musings Through Stable Diffusion",
    "section": "So End my Musings",
    "text": "So End my Musings\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/14_iterators_and_generators.html",
    "href": "forblog/posts/14_iterators_and_generators.html",
    "title": "Iterators and Generators",
    "section": "",
    "text": "This notebook follows the fastai style guide."
  },
  {
    "objectID": "forblog/posts/14_iterators_and_generators.html#iter",
    "href": "forblog/posts/14_iterators_and_generators.html#iter",
    "title": "Iterators and Generators",
    "section": "iter",
    "text": "iter\niter creates what’s known as an iterator. It is a type of iterable.\nAn iterable is anything that can be looped through (e.g., a list or a string).\niter essentially allows you to loop through an iterable without using a for loop. It gives you finer and more granuler control over when you loop, and how how much you loop.\n\n\n\nDocstring:\niter(iterable) -&gt; iterator\niter(callable, sentinel) -&gt; iterator\n\nGet an iterator from an object.  In the first form, the argument must\nsupply its own iterator, or be a sequence.\nIn the second form, the callable is called until it returns the sentinel.\nType:      builtin_function_or_method\n\n\nl = list(range(10)); l\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\nit = iter(l); it\n\n&lt;list_iterator at 0x11e29a6e0&gt;\n\n\n\nnext(it)\n\n0\n\n\n\nnext(it)\n\n1\n\n\n\nnext(it)\n\n2"
  },
  {
    "objectID": "forblog/posts/14_iterators_and_generators.html#islice",
    "href": "forblog/posts/14_iterators_and_generators.html#islice",
    "title": "Iterators and Generators",
    "section": "islice",
    "text": "islice\nislice is a type of iterator that returns \\(x\\) items from an iterable at a time.\n\n\n\nInit signature: islice(self, /, *args, **kwargs)\nDocstring:     \nislice(iterable, stop) --&gt; islice object\nislice(iterable, start, stop[, step]) --&gt; islice object\n\nReturn an iterator whose next() method returns selected values from an\niterable.  If start is specified, will skip all preceding elements;\notherwise, start defaults to zero.  Step defaults to one.  If\nspecified as another value, step determines how many values are\nskipped between successive calls.  Works like a slice() on a list\nbut returns an iterator.\nType:           type\nSubclasses:     \n\n\nfrom itertools import islice\nit = iter(l)\nlist(islice(it, 5))\n\n[0, 1, 2, 3, 4]\n\n\n\nlist(islice(it, 5))\n\n[5, 6, 7, 8, 9]\n\n\n\nlist(islice(it, 5))\n\n[]"
  },
  {
    "objectID": "forblog/posts/14_iterators_and_generators.html#yield",
    "href": "forblog/posts/14_iterators_and_generators.html#yield",
    "title": "Iterators and Generators",
    "section": "yield",
    "text": "yield\nyield is a substitute for return in a function or method. When yield is used, the function is known as a generator.\nyield essentially allows you to perform multiple returns, and also allows you to treat a function as an iterator.\n\nMultiple Returns\nTo demonstrate multiple returns, let’s create a function that chops a list up into smaller lists.\n\ndef chunks(l, step):\n    for i in range(0, len(l), step): yield l[i:i+step]\n\n\nlist(chunks(l, 5))\n\n[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n\n\n\n\nFunction as an Iterator\n\nl_iter = chunks(l, 5); l_iter\n\n&lt;generator object chunks at 0x11e2a8cf0&gt;\n\n\n\nnext(l_iter)\n\n[0, 1, 2, 3, 4]\n\n\n\nnext(l_iter)\n\n[5, 6, 7, 8, 9]\n\n\n\nnext(l_iter)\n\nStopIteration: \n\n\n\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/15_un_successfully_implementing_diffedit.html",
    "href": "forblog/posts/15_un_successfully_implementing_diffedit.html",
    "title": "(Un)successfully Implementing DiffEdit",
    "section": "",
    "text": "This notebook follows the fastai style guide.\nWell, my implementation was a partial success: I managed to generate a mask, but failed to apply it. If you don’t understand, hold on as I’ll explain DiffEdit.\nIn this notebook, I try to implement the DiffEdit paper: a diffusion algorithm that allows us to replace the subject of an image with another subject, simply through a text prompt.\nIn a nutshell, this is done by generating a mask from the text prompt. This mask cuts out the subject from the image, which allows a new subject to be added to the image.\nWhile I was successful in generating a mask, I wasn’t successful in applying it to an image. So at the end of this notebook, I’ll use the Hugging Face Stable Diffusion Inpaint Pipeline to see the mask in action.\nIf you would like a refresher on how Stable Diffusion can be implemented from its various components, you can read my post on this here."
  },
  {
    "objectID": "forblog/posts/15_un_successfully_implementing_diffedit.html#basic-workings",
    "href": "forblog/posts/15_un_successfully_implementing_diffedit.html#basic-workings",
    "title": "(Un)successfully Implementing DiffEdit",
    "section": "Basic Workings",
    "text": "Basic Workings\nLet’s say we have an image of a horse in front of a forest. We want to replace the horse with a zebra. At a high level, DiffEdit achieves this in the following manner.\n\nUsing our image, we generate a further image with the prompt ‘horse’.\nWe similarly generate another further image with the prompt ‘zebra’.\nThe difference between both generated images is then taken.\nThe difference is normalized1 and binarized2 to obtain the mask.\nWe again generate an image with the prompt ‘zebra’.\n\nHowever this time, after each denoising step, apply the mask to the latent to obtain a cutout of the zebra.\nThen add the noised background pixels of the original image to the cutout.\n\n\n1 In this case, normalizing means scaling the values to be between 0 and 1.2 Binarizing means making values to be any of 2 possible values. In this case, either 0 or 1."
  },
  {
    "objectID": "forblog/posts/15_un_successfully_implementing_diffedit.html#setup",
    "href": "forblog/posts/15_un_successfully_implementing_diffedit.html#setup",
    "title": "(Un)successfully Implementing DiffEdit",
    "section": "Setup",
    "text": "Setup\n\n! pip install -Uqq fastcore transformers diffusers\n\n\n1import logging; logging.disable(logging.WARNING)\nfrom fastcore.all import *\nfrom fastai.imports import *\nfrom fastai.vision.all import *\n\n\n1\n\nHugging Face can be verbose."
  },
  {
    "objectID": "forblog/posts/15_un_successfully_implementing_diffedit.html#get-components",
    "href": "forblog/posts/15_un_successfully_implementing_diffedit.html#get-components",
    "title": "(Un)successfully Implementing DiffEdit",
    "section": "Get Components",
    "text": "Get Components\n\nfrom transformers import CLIPTokenizer, CLIPTextModel\n\ntokz = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14', torch_dtype=torch.float16)\ntxt_enc = CLIPTextModel.from_pretrained('openai/clip-vit-large-patch14', torch_dtype=torch.float16).to('cuda')\n\n\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\n\nvae = AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema', torch_dtype=torch.float16).to('cuda')\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n\n\nfrom diffusers import LMSDiscreteScheduler\n\nsched = LMSDiscreteScheduler(\n    beta_start = 0.00085,\n    beta_end = 0.012,\n    beta_schedule = 'scaled_linear',\n    num_train_timesteps = 1000\n)"
  },
  {
    "objectID": "forblog/posts/15_un_successfully_implementing_diffedit.html#simple-loop",
    "href": "forblog/posts/15_un_successfully_implementing_diffedit.html#simple-loop",
    "title": "(Un)successfully Implementing DiffEdit",
    "section": "Simple Loop",
    "text": "Simple Loop\nIn this simple loop, I’m making sure I can correctly generate an image based on another image as the starting point.\n\nHyperparameters\n\nprompt = ['earth']\nneg_prompt = ['']\nw, h = 512, 512\nn_inf_steps = 50\ng_scale = 8\nbs = 1\nseed = 77\n\n\n\nEncode Prompt\n\ntxt_inp = tokz(\n    prompt,\n    padding = 'max_length',\n    max_length = tokz.model_max_length,\n    truncation = True,\n    return_tensors = 'pt',\n)\n\n\ntxt_emb = txt_enc(txt_inp['input_ids'].to('cuda'))[0].half()\n\n\nneg_inp = tokz(\n    [''] * bs,\n    padding = 'max_length',\n    max_length = txt_inp['input_ids'].shape[-1],\n    return_tensors = 'pt'\n)\n\n\nneg_emb = txt_enc(neg_inp['input_ids'].to('cuda'))[0].half()\n\n\nembs = torch.cat([neg_emb, txt_emb])\n\n\n\nCompress Image\n\n!curl --output planet.png 'https://images.unsplash.com/photo-1630839437035-dac17da580d0?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=2515&q=80'\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  188k  100  188k    0     0  4829k      0 --:--:-- --:--:-- --:--:-- 4829k\n\n\n\nimg = Image.open('/content/planet.png').resize((512, 512)); img\n\n\n\n\n\nimport torchvision.transforms as T\nwith torch.no_grad():\n  img = T.ToTensor()(img).unsqueeze(0).half().to('cuda') * 2 - 1\n  lat = vae.encode(img)\n  lat = 0.18215 * lat.latent_dist.sample(); lat.shape\n\nBelow we can see the all 4 channels of the compressed image.\n\nfig, axs = plt.subplots(1, 4, figsize=(16, 4))\nfor c in range(4):\n  axs[c].imshow(lat[0][c].cpu(), cmap='Greys')\n\n\n\n\n\n\nNoise Image\n\nsched = LMSDiscreteScheduler(\n    beta_start=0.00085,\n    beta_end=0.012,\n    beta_schedule='scaled_linear',\n    num_train_timesteps=1000\n); sched\n\nLMSDiscreteScheduler {\n  \"_class_name\": \"LMSDiscreteScheduler\",\n  \"_diffusers_version\": \"0.16.1\",\n  \"beta_end\": 0.012,\n  \"beta_schedule\": \"scaled_linear\",\n  \"beta_start\": 0.00085,\n  \"num_train_timesteps\": 1000,\n  \"prediction_type\": \"epsilon\",\n  \"trained_betas\": null\n}\n\n\n\nsched.set_timesteps(n_inf_steps)\n\n\ntorch.manual_seed(seed)\nnoise = torch.randn_like(lat)\nsched.timesteps = sched.timesteps.to(torch.float32)\nstart_step = 10\nts = tensor([sched.timesteps[start_step]])\nlat = sched.add_noise(lat, noise, timesteps=ts)\n\n\n\nDenoise\n\nfrom tqdm.auto import tqdm\n\nfor i, ts in enumerate(tqdm(sched.timesteps)):\n  if i &gt;= start_step:\n    inp = torch.cat([lat] * 2)\n    inp = sched.scale_model_input(inp, ts)\n\n    with torch.no_grad(): preds = unet(inp, ts, encoder_hidden_states=embs)['sample']\n\n    pred_neg, pred_txt = preds.chunk(2)\n    pred = pred_neg + g_scale * (pred_txt - pred_neg)\n\n    lat = sched.step(pred, ts, lat).prev_sample\n\n\n\n\n\n\nUncompress\n\nlat.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\n\nlat *= (1/0.18215)\nwith torch.no_grad(): img = vae.decode(lat).sample\nimg = (img / 2 + 0.5).clamp(0, 1)\nimg = img[0].detach().cpu().permute(1, 2, 0).numpy()\nimg = (img * 255).round().astype('uint8')\nImage.fromarray(img)\n\n\n\n\n\n\nEncapsulate\nI’ll encapsulate the code above so we can focus on DiffEdit.\n\ndef get_embs(prompt, neg_prompt):\n  txt_inp = tok_seq(prompt)\n  txt_emb = calc_emb(txt_inp['input_ids'])\n\n  neg_inp = tok_seq(neg_prompt)\n  neg_emb = calc_emb(neg_inp['input_ids'])\n\n  return torch.cat([neg_emb, txt_emb])\n\ndef tok_seq(prompt):\n  return tokz(\n      prompt,\n      padding = 'max_length',\n      max_length = tokz.model_max_length,\n      truncation = True,\n      return_tensors = 'pt',\n  )\n\ndef calc_emb(inp_ids):\n  return txt_enc(inp_ids.to('cuda'))[0].half()\n\n\ndef get_lat(img, start_step=30):\n  return noise_lat(compress_img(img), start_step)\n\ndef compress_img(img):\n  with torch.no_grad():\n    img = T.ToTensor()(img).unsqueeze(0).half().to('cuda') * 2 - 1\n    lat = vae.encode(img)\n    return 0.18215 * lat.latent_dist.sample()\n\ndef noise_lat(lat, start_step):\n  torch.manual_seed(seed)\n  noise = torch.randn_like(lat)\n\n  sched.set_timesteps(n_inf_steps)\n  sched.timesteps = sched.timesteps.to(torch.float32)\n  ts = tensor([sched.timesteps[start_step]])\n\n  return sched.add_noise(lat, noise, timesteps=ts)\n\n\ndef denoise(lat, ts):\n  inp = torch.cat([lat] * 2)\n  inp = sched.scale_model_input(inp, ts)\n\n  with torch.no_grad(): preds = unet(inp, ts, encoder_hidden_states=embs)['sample']\n\n  pred_neg, pred_txt = preds.chunk(2)\n  pred = pred_neg + g_scale * (pred_txt - pred_neg)\n\n  return sched.step(pred, ts, lat).prev_sample\n\n\ndef decompress(lat):\n  with torch.no_grad(): img = vae.decode(lat*(1/0.18215)).sample\n  img = (img / 2 + 0.5).clamp(0, 1)\n  img = img[0].detach().cpu().permute(1, 2, 0).numpy()\n  return (img * 255).round().astype('uint8')\n\n\nprompt = ['basketball']\nneg_prompt = ['']\nw, h = 512, 512\nn_inf_steps = 70\nstart_step = 30\ng_scale = 7.5\nbs = 1\nseed = 77\n\n! curl --output img.png 'https://images.unsplash.com/photo-1630839437035-dac17da580d0?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=2515&q=80'\nimg = Image.open('/content/img.png').resize((512, 512))\n\nembs = get_embs(prompt, neg_prompt)\nlat = get_lat(img)\nfor i, ts in enumerate(tqdm(sched.timesteps)):\n  if i &gt;= start_step: lat = denoise(lat, ts)\nimg = decompress(lat)\nImage.fromarray(img)\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  188k  100  188k    0     0  5232k      0 --:--:-- --:--:-- --:--:-- 5381k"
  },
  {
    "objectID": "forblog/posts/15_un_successfully_implementing_diffedit.html#diffedit",
    "href": "forblog/posts/15_un_successfully_implementing_diffedit.html#diffedit",
    "title": "(Un)successfully Implementing DiffEdit",
    "section": "DiffEdit",
    "text": "DiffEdit\nLet’s review the steps of DiffEdit once more.\n\nUsing our image, we generate a further image with the prompt ‘horse’.\nWe similarly generate another further image with the prompt ‘zebra’.\nThe difference between both generated images is then taken.\nThe difference is normalized3 and binarized4 to obtain the mask.\nWe then again generate an image with the prompt ‘zebra’.\n\nHowever this time, after each denoising step, apply the mask to the latent to obtain a cutout of the zebra.\nThen add the noised background pixels of the original image to the cutout.\n\n\n3 In this case, normalizing means scaling the values to be between 0 and 1.4 Binarizing means making values to be any of 2 possible values. In this case, either 0 or 1.\nObtain two latents\nFirst, we need to obtain an image of a horse and an image of a zebra.\nWe’ll use this as our original image.\n\n! curl --output img.png 'https://images.unsplash.com/photo-1553284965-fa61e9ad4795?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1742&q=80'\nImage.open('/content/img.png').resize((512, 512))\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  515k  100  515k    0     0  10.7M      0 --:--:-- --:--:-- --:--:-- 10.7M\n\n\n\n\n\nThis is the generated image of the horse.\n\nprompt = ['horse']\nimg = Image.open('/content/img.png').resize((512, 512))\nembs = get_embs(prompt, neg_prompt)\nlat1 = get_lat(img)\nfor i, ts in enumerate(tqdm(sched.timesteps)):\n  if i &gt;= start_step: lat1 = denoise(lat1, ts)\n\n\n\n\n\nImage.fromarray(decompress(lat1))\n\n\n\n\nAnd this is the generated image of the zebra.\n\nprompt = ['zebra']\nimg = Image.open('/content/img.png').resize((512, 512))\nembs = get_embs(prompt, neg_prompt)\nlat2 = get_lat(img)\nfor i, ts in enumerate(tqdm(sched.timesteps)):\n  if i &gt;= start_step: lat2 = denoise(lat2, ts)\n\n\n\n\n\nImage.fromarray(decompress(lat2))\n\n\n\n\n\nlat1[:].shape\n\ntorch.Size([1, 4, 64, 64])\n\n\n\n\nCreate Mask\nWe’ll first convert the generated images to grayscale and then take their difference.\n\nimport torchvision.transforms.functional as F\n\nimg1 = F.to_tensor(F.to_grayscale(Image.fromarray(decompress(lat1[:]))))\nimg2 = F.to_tensor(F.to_grayscale(Image.fromarray(decompress(lat2[:]))))\ndiff = torch.abs(img1 - img2)\n\nThen we’ll normalize the difference to have values between 0 and 1.\n\nnorm = diff / torch.max(diff)\nImage.fromarray((norm*255).squeeze().numpy().round().astype(np.uint8))\n\n\n\n\nAnd then finally binarize the values so they are either 0 or 1.\n\nthresh = 0.5\nbin = (norm &gt; thresh).float()\nImage.fromarray((bin.squeeze().numpy()*255).astype(np.uint8))\n\n\n\n\n\nImage.fromarray((bin.squeeze().numpy()*255).astype(np.uint8)).save('mask.png')\n\nNow we need to apply transformations to the binarized mask so it encapsulates the shape of the horbra/zeborse (horse + zebra 🫤).\n\nimport cv2 as cv\nfrom google.colab.patches import cv2_imshow\n\nmask = cv.imread('mask.png', cv.IMREAD_GRAYSCALE)\nkernel = cv.getStructuringElement(cv.MORPH_ELLIPSE, (10, 10))\n\nThe kernel is essentially a shape. Multiple shapes are be applied to the image in order to perform transformations.\nI’ve chosen to use an ellipse of size 10 by 10 units.\nApplying an erosion transformation makes our binarized mask look like this. Such transformations remove can remove small, noisy objects.\n\ncv2_imshow(cv.erode(mask, kernel))\n\n\n\n\nApplying a dilation transformation makes our binarized mask look like this. Such transformations can fill in gaps and smooth edges.\n\ncv2_imshow(cv.dilate(mask, kernel))\n\n\n\n\nTo produce the final mask, I’ll apply the closing transform5 7 times consecutively…5 The closing transform is a dilation transform followed immediately by an erosion transform. This allows holes or small black points to be closed.\n\nmask_closed = mask\nfor _ in range(7):\n  mask_closed = cv.morphologyEx(mask_closed, cv.MORPH_CLOSE, kernel)\n  cv2_imshow(mask_closed)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n…and then apply the dilation transform 3 times consecutively.\n\nmask_dilated = mask_closed\nfor _ in range(3):\n  mask_dilated = cv.dilate(mask_dilated, kernel)\n  cv2_imshow(mask_dilated)\n\n\n\n\n\n\n\n\n\n\nA more concise way of doing the above.\n\nmask_closed = cv.morphologyEx(mask, cv.MORPH_CLOSE, kernel, iterations=7)\nmask_dilated = cv.dilate(mask_closed, kernel, iterations=3)\n\ncv2_imshow(mask_dilated)\n\n\n\n\nThen I’ll stack the mask together so I have a 3 channel image.\n\nmask = np.stack((mask_dilated, mask_dilated, mask_dilated), axis=-1)/255; mask.shape\n\n(512, 512, 3)\n\n\nTo read more about such transformations applied above, you can read them at the OpenCV docs here.\n\n\nApply Mask\nNow for the part I couldn’t figure out how to do.\nBy applying the mask to the original iamge. This is how the cutout of the horse looks like.\n\nfore = torch.mul(F.to_tensor(img).permute(1, 2, 0), torch.from_numpy(mask))\nImage.fromarray((fore*255).numpy().round().astype('uint8'))\n\n\n\n\nYou can see that it does not exactly cut out the outline: this is good because different subjects will have different levels of protrusion.\nAnd this is how the background pixels look like.\n\ninv_mask = 1 - mask\nback = torch.mul(F.to_tensor(img).permute(1, 2, 0), torch.from_numpy(inv_mask))\nImage.fromarray((back*255).numpy().round().astype('uint8'))\n\n\n\n\nAdding both the foreground and the background together…\n\nImage.fromarray(((fore+back)*255).numpy().round().astype(np.uint8))\n\n\n\n\n\n\nDetour\nNote the subtle, yet very important difference in the two cells below, along with their output.\n\nx = tensor([1, 2, 3])\ndef foo(y):\n  y += 1\n  return y\nfoo(x)\nx\n\ntensor([2, 3, 4])\n\n\n\nx = tensor([1, 2, 3])\ndef foo(y):\n  z = y + 1\n  return z\nfoo(x)\nx\n\ntensor([1, 2, 3])\n\n\nThis was the reason for the bug that had me pulling my hair out for hours — when you pass a list or any list-like object (or even just objects I think), a copy is not passed, but rather the same object.\n\nHowever, I can’t quite correctly apply the mask to the latent when denoising.\n\nprompt = ['zebra']\nimg = Image.open('/content/img.png').resize((512, 512))\nembs = get_embs(prompt, neg_prompt)\nlat = get_lat(img)\ninv_mask = 1 - mask\n\n\nfor i, ts in enumerate(tqdm(sched.timesteps)):\n  if i &gt;= start_step: \n    back = torch.mul(torch.from_numpy(decompress(get_lat(img, start_step=i)))/255, torch.from_numpy(inv_mask))\n    fore = torch.mul(torch.from_numpy(decompress(lat))/255, torch.from_numpy(mask))\n    bafo = (back + fore)*255\n    lat = compress_img(Image.fromarray(bafo.numpy().round().astype(np.uint8)))\n\n\n\n\n\nImage.fromarray(decompress(lat))\n\n\n\n\nAfter asking on the fastai forum, and hours of fiddling about, the reason why this is happening is most likely due to the fact that I keep uncompressing and recompressing the latent. The compression that the VAE performs is lossy, so detail is lost during each compression and decompression.\nMy mask is not calculated in the same latent space as my latent. In other words, my mask was calculated as a 512x512 pixel and 3 channel image, whereas my latent is a 64x64 pixel and 4 channel image. I’m uncompressing the latent so that I can apply the mask to cutout the zebra and add the background pixels, and then recompressing.\nTo fix this, I would need to generate the mask as a 64x64 pixel and 3 channel image.\nTo at least see the mask in action, let’s use the Hugging Face Stable Diffusion Pipeline."
  },
  {
    "objectID": "forblog/posts/15_un_successfully_implementing_diffedit.html#pipeline",
    "href": "forblog/posts/15_un_successfully_implementing_diffedit.html#pipeline",
    "title": "(Un)successfully Implementing DiffEdit",
    "section": "Pipeline",
    "text": "Pipeline\nThe Hugging Face Stable Diffusion pipeline works by simply providing the starting image and a mask. The pipeline will handle the rest.\n\nfrom diffusers import StableDiffusionInpaintPipeline\npipe = StableDiffusionInpaintPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-inpainting\",\n    revision=\"fp16\",\n    torch_dtype=torch.float16,\n).to(\"cuda\")\n\n/usr/local/lib/python3.10/dist-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n  warnings.warn(\n\n\n\nimg\n\n\n\n\n\ntorch.manual_seed(77)\n# 35 or 25 steps are good\nout = pipe(\n    prompt=[\"zebra\"], \n    image=img, \n    mask_image=Image.fromarray((mask*255).round().astype(np.uint8)), \n    num_inference_steps = 25\n).images\nout[0]"
  },
  {
    "objectID": "forblog/posts/15_un_successfully_implementing_diffedit.html#takeaways",
    "href": "forblog/posts/15_un_successfully_implementing_diffedit.html#takeaways",
    "title": "(Un)successfully Implementing DiffEdit",
    "section": "Takeaways",
    "text": "Takeaways\nLooking back, the actual problem for me was that I let the paper feel intimidating; all those symbols, variables, jargon, and notation. I ended up glazing over the paper and missing the smaller details.\nTo help prevent this the next time, I should\n\nlist out the variables and what they represent\nwrite out the steps in simpler terms\nand take a deep breath before reading, so I take things slowly.\n\nAnd that’s that.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/6_ai_in_a_nutshell.html",
    "href": "forblog/posts/6_ai_in_a_nutshell.html",
    "title": "AI in a Nutshell",
    "section": "",
    "text": "This blog post was updated on Saturday, 12 November 2022.\nArtificial Intelligence. Machine Learning. Neural Networks. Deep Learning. Fancy Words. Deceptively Simple. All really the same.\nThe basic workflow to create such a system is below.\nVery simple, eh? Of course, it’s a very high level abstraction, but this high level view will make this seemingly complex topic very simple.\nFirst, what’s the main thing modern AI methods try to do? They try to make predictions about certain things.\nSo a function of sorts is needed to achieve this. A function that can make these predictions. Think of a function as a machine. You put something into the machine and then, with whatever was input, the machine then produces an output.\nThe machine that we will be working with has two input slots: one slot is for training and the other slot is for predictions.\nTo create a function that produces predictions, we need to tell the function what sort of predictions it needs to make.\nTo do that, we can pour some data into the training slot. This data will tell the function what sort of predictions to output. This process is known as fitting the function to the data.\nTo fit the function onto data, you train the function."
  },
  {
    "objectID": "forblog/posts/6_ai_in_a_nutshell.html#simple-case-quadratic-function",
    "href": "forblog/posts/6_ai_in_a_nutshell.html#simple-case-quadratic-function",
    "title": "AI in a Nutshell",
    "section": "Simple Case: Quadratic Function",
    "text": "Simple Case: Quadratic Function\nGasp! A quadratic?? What’s this nonsense!\nA quadratic is a very simple equation. When shown on a graph, it looks like this.\n\n\n\n\n\nWe’ll be using this equation to demonstrate a very simple example.\nThe basic workflow for fitting a function to data is below.\n\n\n\n\n\n\n\n\n\nflowchart TB\n    B[Calculate Loss] --&gt; C[Calculate Gradients] --&gt; D[Update Parameters] --&gt; B\n\n\n\n\n\n\n\n\n\n\nIt can seem like a lot at first glance; quite a few new terms too.\nWe’ll break this down by going over the very simple example.\nLet’s say we have the following data points that describe, say, the speed of an object with respect to time. We want to predict what the speed of an object would be outside these data points.\nThe horizontal axis is time and the vertical axis is the object’s speed.\n\n\n\n\n\nWe can see that the data looks like the quadratic function shown above! Therefore, we could use the quadratic to predict what the speed of the object would be after 2.0 s and before -2.0 s.\nA quadratic equation includes three numbers which we will call \\(a\\), \\(b\\), and \\(c\\). These three numbers affect or control how our quadratic function will end up looking. \\(a\\), \\(b\\), and \\(c\\) are our parameters.\nLet’s let \\(a\\), \\(b\\), and \\(c\\) all equal \\(1\\) to begin with.\n\n\n\n\n\nHmm, not a very good fit.\nLet’s try another set of values for the parameters: \\(2\\), \\(1\\), \\(1.5\\).\n\n\n\n\n\nLooking much better now!\nLet’s see what \\(2\\), \\(0\\), and \\(1.5\\) gives us.\n\n\n\n\n\nEyeballing this is difficult. A certain set of parameters we use may be good by looking at the resulting graph, but in reality, it may not be.\nWhat we need is something that can tell us how good our function is; something that tells us whether the changes we are making are actually good or not. To do this, we can calculate a number called the loss. The smaller the loss, the better the function is.\nThere are many different ways loss can be calculated. The way we will be doing it is known as mean absolute error (MAE). In simple terms, it tells us how far off each prediction is from the actual value. For example, if we have a MAE of 1, this means that, on average, each prediction we make is 1 unit off from the real value.\nIn our case, a MAE of 1 would mean that each prediction is on average 1 m/s off from the real value.\nLet’s repeat what we did above, but this time, we’ll also see what the MAE is.\n\n\n\n\n\nAgain, this means that on average, each prediction we will make is 2.61 m/s off from the real value.\n\n\n\n\n\nThat’s a big jump!\n\n\n\n\n\nHmm, things got worse.\nDoing this process by hand is very tedious. How do we know if the new set of parameters we are using would improve the function? There needs to be a way to automate this so we don’t have to sit down and do this by hand.\nWhat we can do is update the parameters based on the loss. This would in turn create new parameters that would decrease the loss.\n\n\n\n\n\n\n\n\n\nflowchart TB\n    A[Loss] -- Updates ---&gt; B[Parameters] -- Updates ---&gt; A\n\n\n\n\n\n\n\n\n\n\nLet’s give \\(a\\), \\(b\\), and \\(c\\) an arbitrary set of parameters \\(1.1\\), \\(1.1\\), and \\(1.1\\).\nNow let’s create a quadratic with this set of parameters and calculate its mean absolute error.\n\n\n\n\n\n\n\n\nCode Output\n\n\n\nThe MAE is 2.42.\n\n\n\n\nNow comes the next step: how do we update the parameters based on this loss we have calculated?\nTo do this, we calculate a new set of quantities known as the gradients. Each parameter has its own gradient.\nLet’s say \\(a\\) has the value of \\(1\\). If \\(a\\) has a gradient of value \\(0.5\\), this would mean that if we increase \\(a\\) by \\(1\\), the loss would increase by \\(0.5\\). Therefore, if we decrease \\(a\\) by \\(1\\), this would mean the loss would decrease by \\(0.5\\), which is what we want!\nRead over this once more and it’ll make sense!\nLet’s quickly go over the inverse: if \\(a\\) has a gradient of value \\(-0.5\\), increasing \\(a\\) by \\(1\\) would decrease the loss by \\(0.5\\) — again, this is what we want! Similarly, decreasing \\(a\\) by \\(1\\) would increase the loss by \\(0.5\\).\nThe gradients are calculated from the loss. Then the gradients, the current parameters, and along with another value, the parameters are updated to new values. The “another value” is known as the learning rate. The learning rate controls how much the gradients update the parameters.\n\n\n\n\n\n\n\n\n\nflowchart TB\n    A[Gradients]\n    B[Current Parameters]\n    C[Learning Rate]\n    D[Magical Box]\n    E[Updated Paramters]\n    A & B & C ---&gt; D ---&gt; E\n\n\n\n\n\n\n\n\n\n\nLets see this tangibly.\n\n\n\n\n\n\n\n\nCode Output\n\n\n\nThe gradients for each parameter respectively are [-1.35, -0.03, -0.5].\n\n\n\n\nOkay, let’s break this down. The gradient for the first parameter \\(a\\) is \\(-1.35\\). This tells us that if we increase the parameter \\(a\\) by \\(1\\), our loss will decrease by \\(-1.35\\). Similary, if we increase the parameter \\(b\\) by \\(1\\), this will result in the loss being decreased by \\(-0.03\\). The same logic holds for \\(c\\).\nLet’s now update the parameters. Remember, the current set of parameters, their gradients, and the learning rate all update the current set of parameters to new values.\n\n\n\n\n\n\n\n\nCode Output\n\n\n\nThe new parameters are [1.11, 1.1, 1.11].\n\n\n\n\nWe can now repeat the process as many times as desired. Let’s do it 4 times.\n\n\nPass: 0; Loss: 2.4010409560416095\nPass: 1; Loss: 1.9847692009423128\nPass: 2; Loss: 1.498316818239171\nPass: 3; Loss: 1.171195547258246\n\n\n\n\n\n\n\n\nCode Output\n\n\n\nThe MAE after 4 passes is 1.17.\n\n\n\n\n\n\n\nAnd there you go! An even better fitting quadratic!\nLet’s see what the object’s speed is at 1 second.\n\n\n\n\n\n\n\n\nCode Output\n\n\n\nThe object’s velocity at 1 seconds is 5.65 m/s.\n\n\n\n\nThat roughly seems right!\nLet’s see what the object’s speed would be at 3 seconds.\n\n\n\n\n\n\n\n\nCode Output\n\n\n\nThe object’s velocity at 1 seconds is 30.31 m/s.\n\n\n\n\nAnd now, the diagram below should make sense!\n\n\n\n\n\n\n\n\n\nflowchart TB\n    B[Calculate Loss] --&gt; C[Calculate Gradients] --&gt; D[Update Parameters] --&gt; B"
  },
  {
    "objectID": "forblog/posts/6_ai_in_a_nutshell.html#the-cool-case-relus",
    "href": "forblog/posts/6_ai_in_a_nutshell.html#the-cool-case-relus",
    "title": "AI in a Nutshell",
    "section": "The Cool Case: ReLUs",
    "text": "The Cool Case: ReLUs\nThe quadratic example above is a nice, simple way to get a grasp of things. However, you may be wondering, “What if the data doesn’t follow a quadratic shape? What do we do then?”\nAnd that’s a good question! What if our data doesn’t follow any sort of mathematical shape? What if we don’t even know the shape the data will follow? How do we know what function to use in that case?\nThere is a solution to that! There is an easy way to create a function that bends and twists itself to fit the data; an “unbound” function of sorts, as I like to call it.\nThis can be achieved by using another equation known as the ReLU. Another fancy word that can make you sound like a professional, while also being really simple. ReLU is short for Rectified Linear Unit.\nThe ReLU takes any value that is less than 0, and converts to 0.\nLet’s see this.\nTake the following line. It has both positive and negative values on the vertical axis.\n\n\n\n\n\nWhen we use a ReLU, all negative values are converted to zero.\n\n\n\n\n\nLet’s return to our original data.\n\n\n\n\n\nNow a single ReLU won’t work as seen below.\n\n\n\n\n\nEven after we try to fit it.\n\n\n\n\n\nBut look at what happens when two ReLUs are, literally, added together!\n\n\n\n\n\n\nPretty neat, hey?\nLet’s add a third ReLU to the mix.\n\n\n\n\n\n\nYou can see here how the function is adapting to the shape of the data.\nWith some extra experimentation, I was able to get the loss down to 1.08!\n\n\n\n\n\nThat said, it’s not too much of a difference when compared to two ReLUs.\nWhat if we add 5 more to the mix, for a total of 8?\n\n\n\n\n\nNice! The MAE has gone below 1!\nIt’s even beat the quadratic function from before! With some expermimenting, I had managed to get the quadratic’s loss down to 1.03.\n\n\n\n\n\n\nLet’s use the model that has 8 ReLUs to predict what the object’s velocity would be at 1 second.\n\n\n\n\n\n\n\n\nCode Output\n\n\n\nThe object’s speed at 1 s is 4.9 m/s.\n\n\n\n\nHmm, yes, that is a bit off. But that is fine because overall, the function is a lot more accurate for all the datapoints."
  },
  {
    "objectID": "forblog/posts/6_ai_in_a_nutshell.html#conclusion",
    "href": "forblog/posts/6_ai_in_a_nutshell.html#conclusion",
    "title": "AI in a Nutshell",
    "section": "Conclusion",
    "text": "Conclusion\nSee how easy this stuff all is? All those fancy terms makes this feel complex when in reality, it’s all really simple.\nWhy not now go and venture off to learn more and implement your own models!\nBelow are two free courses I can recommend:\n\nElements of AI\nA great primer into AI. The course goes over the history, the implementations, and the implications of this field, all without needing the knowledge of programming or complex mathematics.\nPractical Deep Learning for Coders\nThis course is different from other AI courses you’ll find. How? Because instead of starting off with the nitty gritty basics, you begin by actually implementing your own simple image classifier (a model that can tell what thing is in an image). You’ll be surprised at how simple it is to implement models with minimal code, and how little you need to know to get started (hint: you only really need high-school maths).\n\nIf you have any questions, comments, suggestions, or feedback, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/6_ai_in_a_nutshell.html#acknowledgements",
    "href": "forblog/posts/6_ai_in_a_nutshell.html#acknowledgements",
    "title": "AI in a Nutshell",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis article was inspired by the How does a neural net really work Kaggle Notebook by Jeremy Howard, and lesson 3 of Practical Deep Learning for Coders."
  },
  {
    "objectID": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html",
    "href": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html",
    "title": "How to Convert Audio to Spectrogram Images",
    "section": "",
    "text": "You can find this notebook on Kaggle here.\nIn this to-the-point notebook, I go over how one can create images of spectrograms from audio files using the PyTorch torchaudio module.\nThe notebook also goes over how I created the spectrogram images for the BirdCLEF 2023 competition, and how one can create and push a dataset right on Kaggle (useful if your local machine doesn’t have enough storage).\nYou can view the dataset that was generated from this notebook here."
  },
  {
    "objectID": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#setup",
    "href": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#setup",
    "title": "How to Convert Audio to Spectrogram Images",
    "section": "Setup",
    "text": "Setup\n\ntry: from fastkaggle import *\nexcept ModuleNotFoundError:\n    ! pip install -Uqq fastkaggle\n    from fastkaggle import *\n\niskaggle\n\n'Batch'\n\n\n\ncomp = 'birdclef-2023'\nd_path = setup_comp(comp, install='nbdev')\n\n\nfrom fastai.imports import *\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#data",
    "href": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#data",
    "title": "How to Convert Audio to Spectrogram Images",
    "section": "Data",
    "text": "Data\n\nPaths\nLet’s see all the files and directories we have.\n\nd_path.ls()\n\n(#5) [Path('../input/birdclef-2023/sample_submission.csv'),Path('../input/birdclef-2023/train_audio'),Path('../input/birdclef-2023/eBird_Taxonomy_v2021.csv'),Path('../input/birdclef-2023/train_metadata.csv'),Path('../input/birdclef-2023/test_soundscapes')]\n\n\nLet’s get the path to the audio files.\n\naud_files = d_path/'train_audio'\n\nAnd create a directory to store the spectrogram images.\n\nmkdir('/kaggle/train_images', exist_ok=True); Path('/kaggle/train_images').exists()\n\nTrue"
  },
  {
    "objectID": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#single-image",
    "href": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#single-image",
    "title": "How to Convert Audio to Spectrogram Images",
    "section": "Single Image",
    "text": "Single Image\nIt’s always a good idea to try things out on a smaller scale; so let’s begin by converting only a single audio file.\nLet’s get the first audio file.\n\naud_files.ls()\n\n(#264) [Path('../input/birdclef-2023/train_audio/yetgre1'),Path('../input/birdclef-2023/train_audio/moccha1'),Path('../input/birdclef-2023/train_audio/rostur1'),Path('../input/birdclef-2023/train_audio/walsta1'),Path('../input/birdclef-2023/train_audio/ratcis1'),Path('../input/birdclef-2023/train_audio/norfis1'),Path('../input/birdclef-2023/train_audio/macshr1'),Path('../input/birdclef-2023/train_audio/brrwhe3'),Path('../input/birdclef-2023/train_audio/crefra2'),Path('../input/birdclef-2023/train_audio/pabspa1')...]\n\n\n\naud_files.ls()[0].ls()\n\n(#27) [Path('../input/birdclef-2023/train_audio/yetgre1/XC247367.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC574558.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC403259.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC498854.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC289493.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC716763.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC498853.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC338717.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC349660.ogg'),Path('../input/birdclef-2023/train_audio/yetgre1/XC403543.ogg')...]\n\n\n\naud = aud_files.ls()[0].ls()[0]; aud\n\nPath('../input/birdclef-2023/train_audio/yetgre1/XC247367.ogg')\n\n\nNow it’s time to load it in. What we get in return is the waveform and the sample rate.\n\nimport torchaudio\nwvfrm, sr = torchaudio.load(aud); wvfrm\n\ntensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.0518e-05, 0.0000e+00,\n         0.0000e+00]])\n\n\n\n\nBirdCLEF 2023 — Clipping the Audio Files\nThis competition requires predictions to be submitted of all 5 second intervals in each audio clip. This means the audio files need to be clipped.\nBelow is an easy way this can be done. We clip the first 5 seconds of the audio file.\n\nstart_sec = 0\nend_sec = 5\nwvfrm = wvfrm[:, start_sec*sr:end_sec*sr]\nwvfrm.shape[1] / sr\n\n5.0\n\n\nSample rate is simply the number of frames recorded per second. The waveform that torchaudio returns is a tensor of frames. Therefore, we can easily select the desired range of frames by multiplying the sample rate with the desired start and end seconds.\n\nNow let’s create the spectrogram.\n\nimport torchaudio.transforms as T\nspec = T.Spectrogram()(wvfrm); spec\n\ntensor([[[4.3970e-08, 8.2461e-09, 4.7306e-11,  ..., 7.8266e-08,\n          1.7642e-08, 1.5016e-03],\n         [6.3310e-09, 6.5514e-10, 1.6958e-08,  ..., 4.3492e-09,\n          3.6019e-08, 1.5231e-03],\n         [1.1548e-08, 1.7308e-08, 6.5956e-08,  ..., 2.9340e-06,\n          1.2277e-06, 1.4124e-03],\n         ...,\n         [2.4446e-07, 6.1277e-09, 1.4932e-09,  ..., 1.4665e-08,\n          1.0110e-08, 1.8980e-05],\n         [3.1582e-07, 1.4777e-09, 1.2275e-08,  ..., 1.3213e-08,\n          1.9035e-09, 2.0009e-05],\n         [3.1673e-07, 1.1897e-10, 2.7457e-09,  ..., 1.0001e-08,\n          6.0452e-14, 1.9979e-05]]])\n\n\nLet’s scale it logarithmically. This allows for better viewing.\n\nspec = T.AmplitudeToDB()(spec); spec\n\ntensor([[[ -73.5684,  -80.8375, -100.0000,  ...,  -71.0643,  -77.5346,\n           -28.2346],\n         [ -81.9853,  -91.8367,  -77.7063,  ...,  -83.6159,  -74.4347,\n           -28.1726],\n         [ -79.3751,  -77.6177,  -71.8074,  ...,  -55.3254,  -59.1092,\n           -28.5005],\n         ...,\n         [ -66.1180,  -82.1270,  -88.2588,  ...,  -78.3373,  -79.9524,\n           -47.2170],\n         [ -65.0056,  -88.3043,  -79.1097,  ...,  -78.7899,  -87.2045,\n           -46.9877],\n         [ -64.9931,  -99.2458,  -85.6135,  ...,  -79.9997, -100.0000,\n           -46.9943]]])\n\n\nThe PyTorch tensor needs to be converted into a NumPy array so it can then further be converted to an image. I’m using the squeeze method to remove the uneeded axis of length 1, as seen below.\n\nspec.shape\n\ntorch.Size([1, 201, 801])\n\n\n\nspec = spec.squeeze().numpy(); spec\n\narray([[ -73.56842 ,  -80.83749 , -100.      , ...,  -71.064285,\n         -77.53463 ,  -28.234562],\n       [ -81.98525 ,  -91.83666 ,  -77.706314, ...,  -83.615875,\n         -74.43467 ,  -28.172626],\n       [ -79.37505 ,  -77.61765 ,  -71.80743 , ...,  -55.32541 ,\n         -59.109177,  -28.500452],\n       ...,\n       [ -66.118   ,  -82.127014,  -88.25883 , ...,  -78.33732 ,\n         -79.95244 ,  -47.21705 ],\n       [ -65.00563 ,  -88.30426 ,  -79.10974 , ...,  -78.78989 ,\n         -87.20445 ,  -46.987743],\n       [ -64.99306 ,  -99.24575 ,  -85.61355 , ...,  -79.99969 ,\n        -100.      ,  -46.994343]], dtype=float32)\n\n\n\nspec.shape\n\n(201, 801)\n\n\nThe array now needs to be normalized so it contains integers between 0 and 255: the values needed for images.\n\nspec = (spec - spec.min()) / (spec.max() - spec.min()) * 255; spec\n\narray([[ 53.000538 ,  38.424625 ,   0.       , ...,  58.021824 ,\n         45.047504 , 143.90388  ],\n       [ 36.123127 ,  16.369104 ,  44.703243 , ...,  32.853405 ,\n         51.263535 , 144.02808  ],\n       [ 41.35709  ,  44.881027 ,  56.53168  , ...,  89.581375 ,\n         81.99417  , 143.37071  ],\n       ...,\n       [ 67.94011  ,  35.838867 ,  23.543371 , ...,  43.437954 ,\n         40.199318 , 105.84024  ],\n       [ 70.17062  ,  23.452267 ,  41.889095 , ...,  42.530468 ,\n         25.6576   , 106.30005  ],\n       [ 70.19584  ,   1.5124193,  28.847677 , ...,  40.104576 ,\n          0.       , 106.28681  ]], dtype=float32)\n\n\n\nspec = spec.astype('uint8'); spec\n\narray([[ 53,  38,   0, ...,  58,  45, 143],\n       [ 36,  16,  44, ...,  32,  51, 144],\n       [ 41,  44,  56, ...,  89,  81, 143],\n       ...,\n       [ 67,  35,  23, ...,  43,  40, 105],\n       [ 70,  23,  41, ...,  42,  25, 106],\n       [ 70,   1,  28, ...,  40,   0, 106]], dtype=uint8)\n\n\nNow we can finally convert the array to an image!\n\nimg = Image.fromarray(spec)\nprint(img.shape)\nimg\n\n(201, 801)\n\n\n\n\n\nCool, hey? We’ve just visualized audio!\n\n\n\nBirdCLEF 2023 — Resizing the Images\nTo allow the images to easily be used by various models, I resized the spectrograms to be 512 by 512 pixels as shown below.\n\nimg_size = (512, 512)\nimg = img.resize(img_size)\nprint(img.shape); img\n\n(512, 512)\n\n\n\n\n\n\nTo save the image, we can simply use the save method.\n\nimg.save('img.png')"
  },
  {
    "objectID": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#all-the-images",
    "href": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#all-the-images",
    "title": "How to Convert Audio to Spectrogram Images",
    "section": "All the Images",
    "text": "All the Images\nNow that we have verified that our algorithm works fine, we can extend it to convert all audio files.\n\ndef create_imgs(duration, f):\n    for step in range(0, duration, 5):\n        wvfrm, sr = torchaudio.load(f)\n        wvfrm = cut_wvfrm(wvfrm, sr, step)\n        spec = create_spec(wvfrm)\n        img = spec2img(spec)\n        end_sec = step + 5\n        img.save(f'/kaggle/train_images/{bird.stem}/{f.stem}_{end_sec}.png')\n\ndef cut_wvfrm(wvfrm, sr, step):\n    start_sec, end_sec = step, step + 5\n    return wvfrm[:, start_sec * sr: end_sec * sr]\n            \ndef create_spec(wvfrm):\n    spec = T.Spectrogram()(wvfrm)\n    return T.AmplitudeToDB()(spec)\n        \ndef spec2img(spec, img_size=(512, 512)):\n    spec = np.real(spec.squeeze().numpy())\n    spec = ((spec - spec.min()) / (spec.max() - spec.min()) * 255).astype('uint8')\n    return Image.fromarray(spec).resize(img_size)\n\n\nif not iskaggle:\n    for bird in aud_files.ls().sorted():\n        mkdir(f'/kaggle/train_images/{bird.stem}', exist_ok=True)\n        for f in bird.ls().sorted():\n            info = torchaudio.info(f)\n            duration = info.num_frames / info.sample_rate\n            if duration &gt;= 5:\n                create_imgs(round(duration/5)*5, f)\n            else: continue\n\n\nNote: Ignore the if not iskaggle statement when replicating. I added it since I edited this notebook and needed to save changes without reproducing the entire dataset.\n\nIn the first for loop below, we loop through all the bird folders. For each folder, a folder with the same name is created in the directory where we want to store the images.\nIn the second for loop, we loop through all audio files within the folder and then convert them to spectrogram images through the create_images function I defined.\n\n\nBirdCLEF 2023 — Clipping the Audio Files\nSome audio files in the training set are of different durations. Therefore, we obtain the duration of the audio file so it can correctly be clipped into 5 second intervals.\ninfo = torchaudio.info(f)\nduration = info.num_frames / info.sample_rate\nif duration &gt;= 5:\n    create_images(round(duration/5)*5, f)\nelse: continue\nAgain, since sample rate is the number of frames recorded per second, we can divide the total number of frames by the sample rate to obtain the duration in seconds of a clip.\nduration = info.num_frames / info.sample_rate\nThen we round the duration to the nearest 5 for easy clipping.\nround(duration/5)*5\n\nThe images now created! The rest of this notebook covers how one can generate a dataset in a Kaggle Notebook and push it directly to Kaggle within it."
  },
  {
    "objectID": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#api-setup",
    "href": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#api-setup",
    "title": "How to Convert Audio to Spectrogram Images",
    "section": "API Setup",
    "text": "API Setup\nWe need to configure the user keys so we can push to the correct account.\nTo do this, first obtain your Kaggle API key. Then, while in the notebook editor, click Add-ons -&gt; Secrets -&gt; Add a New Secret…\n\n\n\n…input your key and give it a name…\n\n…and click save. Then click the checkbox next to the secret to activate it for your notebook.\n\nRepeat for your Kaggle username.\nNow we can set the keys for the notebook as shown below (input the name of your key into get_secret).\n\nimport os\nfrom kaggle_secrets import UserSecretsClient\n\n\nsecrets = UserSecretsClient()\nos.environ['KAGGLE_USERNAME'] = secrets.get_secret('KAGGLE_USERNAME')\nos.environ['KAGGLE_KEY'] = secrets.get_secret('KAGGLE_KEY')"
  },
  {
    "objectID": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#push-dataset",
    "href": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#push-dataset",
    "title": "How to Convert Audio to Spectrogram Images",
    "section": "Push Dataset",
    "text": "Push Dataset\nThe fastkaggle library offers a convenient way to easily create and push a dataset to Kaggle.\n\ndoc(mk_dataset)\n\n\nmk_dataset\nmk_dataset(dataset_path, title, force=False, upload=True)Creates minimal dataset metadata needed to push new dataset to kaggle\n\n\n\nNote: Ignore the if not iskaggle statement when replicating. I added it since I edited this notebook and needed to save changes without reproducing the entire dataset.\n\n\nif not iskaggle:\n    mk_dataset('/kaggle/train_images', 'spectrograms-birdclef-2023', force=True, upload=True)\n\nAnd we can verify our dataset has been created by having a look at the generated metadata file.\n\nif not iskaggle:\n    ! cat /kaggle/train_images/dataset-metadata.json\n\nFrom here, we can go directly to the dataset page on Kaggle and fill out the rest of the details."
  },
  {
    "objectID": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#and-there-you-have-it",
    "href": "forblog/posts/10_how_to_convert_audio_to_spectrogram_images.html#and-there-you-have-it",
    "title": "How to Convert Audio to Spectrogram Images",
    "section": "And there you have it!",
    "text": "And there you have it!\nIn summary, you saw how to: * Generate spectrogram images from audio files using torchaudio and fastai * How to cut audio tracks * And how to create and push a dataset directly on Kaggle\nYou can view the dataset that was generated from this notebook here.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html",
    "href": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html",
    "title": "A No Nonsense Guide on how to use an M-Series Mac GPU with PyTorch",
    "section": "",
    "text": "This blog post was updated on Saturday, 28 January 2023.\nIf you have one of those fancy Macs with an M-Series chip (M1/M2, etc.), here’s how to make use of its GPU in PyTorch for increased performance.\nIt’s a bit annoying and a little tedious, but here we go."
  },
  {
    "objectID": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#requirements",
    "href": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#requirements",
    "title": "A No Nonsense Guide on how to use an M-Series Mac GPU with PyTorch",
    "section": "1 Requirements",
    "text": "1 Requirements\n\nHave an M-Series chip\nHave at least PyTorch 1.12\nHave at least macOS Monterey 12.3"
  },
  {
    "objectID": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#installing-pytorch",
    "href": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#installing-pytorch",
    "title": "A No Nonsense Guide on how to use an M-Series Mac GPU with PyTorch",
    "section": "2 Installing PyTorch",
    "text": "2 Installing PyTorch\nInstall PyTorch as you usually would. Just make sure it’s PyTorch 1.12.\n# Installing with Pip.\n$ pip3 install torch torchvision torchaudio\n\n# Installing using Conda.\n$ conda install pytorch torchvision torchaudio -c pytorch\nBy using these commands, the latest version of the library is installed so there is no need to specify the version number.\nHowever, if you have an existing installation, you can run the following Pip command instead.\n$ pip3 install --upgrade torch torchvision torchaudio"
  },
  {
    "objectID": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#import-pytorch",
    "href": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#import-pytorch",
    "title": "A No Nonsense Guide on how to use an M-Series Mac GPU with PyTorch",
    "section": "3 Import PyTorch",
    "text": "3 Import PyTorch\n\nimport torch"
  },
  {
    "objectID": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#check-requirements-are-met",
    "href": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#check-requirements-are-met",
    "title": "A No Nonsense Guide on how to use an M-Series Mac GPU with PyTorch",
    "section": "4 Check Requirements are Met",
    "text": "4 Check Requirements are Met\nBelow is a convenient code snippet taken from the PyTorch documentation that checks whether requirements are met.\n\nif not torch.backends.mps.is_available():\n    if not torch.backends.mps.is_built():\n        print(\"MPS not available because the current PyTorch install was not built with MPS enabled.\")\n    else:\n        print(\"MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.\")\n\nIf neither of the two above messages print, you’re good to go!"
  },
  {
    "objectID": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#the-annoying-part-enabling-the-gpu",
    "href": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#the-annoying-part-enabling-the-gpu",
    "title": "A No Nonsense Guide on how to use an M-Series Mac GPU with PyTorch",
    "section": "5 The Annoying Part: Enabling the GPU",
    "text": "5 The Annoying Part: Enabling the GPU\nAs far as I know, you must explicitly enable the use of the GPU for whatever model or tensor you wish to use the GPU for.\nThere are different ways you can do this.\nUse a string.\n\nt = torch.tensor([1, 2, 3], device='mps')\n\n\n\n\ntensor([1, 2, 3], device='mps:0')\n\n\n\nStore as a variable.\n\ndevice='mps'\nt = torch.tensor([1, 2, 3], device=device)\n\n\n\n\ntensor([1, 2, 3], device='mps:0')\n\n\n\nConvert existing objects.\n\nt = torch.tensor([1, 2, 3])\nt.to(device)\n\n\n\n\ntensor([1, 2, 3], device='mps:0')\n\n\n\nNote that converting existing objects creates a copy and does not modify the original.\n\nt\n\n\n\n\ntensor([1, 2, 3])\n\n\n\nThough the above operations have been performed on tensors, they can also be performed on models."
  },
  {
    "objectID": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#points-to-note",
    "href": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#points-to-note",
    "title": "A No Nonsense Guide on how to use an M-Series Mac GPU with PyTorch",
    "section": "6 Points to Note",
    "text": "6 Points to Note\n\nGPU enabled means operations are done on the GPU.\nA GPU enabled tensor can only perform operations with another GPU enabled tensor.\nAs of writing this, GPU support is still in its early stages. So certain features are unsupported and further optimizations await."
  },
  {
    "objectID": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#relevant-links",
    "href": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#relevant-links",
    "title": "A No Nonsense Guide on how to use an M-Series Mac GPU with PyTorch",
    "section": "Relevant Links",
    "text": "Relevant Links\nRelevant links:\n\nInstalling PyTorch: https://pytorch.org/get-started/locally/\nDocs on using GPU: https://pytorch.org/docs/stable/notes/mps.html\nPerformance gains (note that nightly builds are no longer needed): https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/"
  },
  {
    "objectID": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#closing-words",
    "href": "forblog/posts/8_how_to_use_apple_gpu_with_pytorch.html#closing-words",
    "title": "A No Nonsense Guide on how to use an M-Series Mac GPU with PyTorch",
    "section": "Closing Words",
    "text": "Closing Words\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/posts/9_transformers_explained.html#at-a-high-view",
    "href": "forblog/posts/9_transformers_explained.html#at-a-high-view",
    "title": "Transformers, Simply Explained",
    "section": "At a High View",
    "text": "At a High View\nTransformers are all the rage right now. They’re what’s powering the current wave of chat bots. Here’s a high level view of how transformers work, so you know how these bots really work.\nSimply put, a transformer is a type of architecture used for Natural Language Processing (NLP) tasks that either fills-in-the-blanks or autocompletes.\nTransformers consist of either an encoder, decoder, or both. Encoders and decoders contain attention layers.\nLanguage models need numbers to work. Text is given a numerical representation after breaking it down into smaller pieces. To keep this explanation simple, these pieces are words.\nThe numerical representation given to a word describes the word itself and its relation to the surrounding words."
  },
  {
    "objectID": "forblog/posts/9_transformers_explained.html#encoders",
    "href": "forblog/posts/9_transformers_explained.html#encoders",
    "title": "Transformers, Simply Explained",
    "section": "Encoders",
    "text": "Encoders\nEncoder-only transformers are good for “understanding” text, such as classifying sentences by sentiment or figuring out what parts of a sentence refers, for example, to a person or location.\nWhen training encoders, words are given a numerical representation by the attention layers considering adjacent words. For example, let’s say we have the sentence, “I am really hungry.”. The attention layers consider the words ‘am’ and ‘hungry’ when giving the word ‘really’ a numerical representation.\nThe goal of training encoders is to predict words omitted from text (e.g., “I … really hungry.”). This is how encoders can “understand” text."
  },
  {
    "objectID": "forblog/posts/9_transformers_explained.html#decoders",
    "href": "forblog/posts/9_transformers_explained.html#decoders",
    "title": "Transformers, Simply Explained",
    "section": "Decoders",
    "text": "Decoders\nDecoder-only transformers are good for text generation. An example is the autocomplete feature on a smartphone’s keyboard.\nDecoders similary give text a numerical representation, except that the attention layers consider only the previous words. When giving ‘am’ a numerical representation from “I am hungry.”, the attention layers will only consider the word ‘I’. When giving ‘hungry’ a numerical representation, the attention layers will consider the words ‘I’ and ‘am’.\nThe goal of training decoders is to predict the most likely word to continue a piece of text (e.g., “I am ….”). All generated words are used in conjunction to generate the next word."
  },
  {
    "objectID": "forblog/posts/9_transformers_explained.html#encoders-and-decoders",
    "href": "forblog/posts/9_transformers_explained.html#encoders-and-decoders",
    "title": "Transformers, Simply Explained",
    "section": "Encoders and Decoders",
    "text": "Encoders and Decoders\nTransformers that use both encoders and decoders are known as encoder-decoder models or sequence-to-sequence models. Such models are good for translation and summarization.\nEncoder-decoder models are trained by first letting the encoder give the input text a numerical representation. Next, this representation is input to the decoder which generates text as described above. The encoder part of the model provides the “understanding”, while the decoder part of the model generates based off of this “understanding”."
  },
  {
    "objectID": "forblog/posts/9_transformers_explained.html#closing-words",
    "href": "forblog/posts/9_transformers_explained.html#closing-words",
    "title": "Transformers, Simply Explained",
    "section": "Closing Words",
    "text": "Closing Words\nAnd there you have it! It’s as simple as that!\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "forblog/index.html",
    "href": "forblog/index.html",
    "title": "Welcome to ForBlog by ForBo7",
    "section": "",
    "text": "Here you can experience my various ventures into learning, as well as read about other various useful tid-bits.\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nIntuitively Approaching Einstein Summation Notation\n\n\nPutting it in an einsum-ple manner.\n\n\nAn alternative way to write matrix operations.\n\n\n\n\n\n\n04 June 2023\n\n\nSalman Naqvi\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\n(Un)successfully Implementing DiffEdit\n\n\nThe (Un)expected Difficulties of Editing\n\n\nAn attempt at implementing the DiffEdit paper.\n\n\n\n\n\n\n29 May 2023\n\n\nSalman Naqvi\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nIterators and Generators\n\n\nIteratively Generating Iterative Generators\n\n\nIterators and generators shown by example.\n\n\n\n\n\n\n03 May 2023\n\n\nSalman Naqvi\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nImplementing Stable Diffusion From Its Components\n\n\nCreating a Diffuser that Diffuses Stable-y\n\n\nImplementing stable diffusion from the 🤗 🧨 library.\n\n\n\n\n\n\n28 April 2023\n\n\nSalman Naqvi\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nMy Musings Through Stable Diffusion\n\n\nDiffusing the Musings\n\n\nExploring the various knobs and dials of stable diffusion.\n\n\n\n\n\n\n13 April 2023\n\n\nSalman Naqvi\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nStable Diffusion, Summarized\n\n\nTaking a Look at how Diffusers Dream\n\n\nA concise, high level overview on the mechanisms of stable diffusion.\n\n\n\n\n\n\n13 April 2023\n\n\nSalman Naqvi\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nHow to Convert Audio to Spectrogram Images\n\n\nVisualizing Sound\n\n\nA no nonsense guide to creating spectrograms from audio with PyTorch torchaudio.\n\n\n\n\n\n\n05 April 2023\n\n\nSalman Naqvi\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nTransformers, Simply Explained\n\n\nAutobots or Decepticons?\n\n\nAll transformers really do is fill in the blanks and autocomplete.\n\n\n\n\n\n\n28 February 2023\n\n\nSalman Naqvi\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nA No Nonsense Guide on how to use an M-Series Mac GPU with PyTorch\n\n\nM-Series Macs is better than saying M1/M2 Macs\n\n\nSqueezing out that extra performance.\n\n\n\n\n\n\n26 January 2023\n\n\nSalman Naqvi\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nAdding Subscriptions to a Quarto Site\n\n\nSubscribable Subscriptions\n\n\nA no nonsense, to the point guide to implementing subscriptions in your Quarto site.\n\n\n\n\n\n\n23 December 2022\n\n\nIsaac Flath, Salman Naqvi\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nAI in a Nutshell\n\n\nThis nutshell contains very little math!\n\n\nAI models are much, much simpler than you think.\n\n\n\n\n\n\n04 October 2022\n\n\nSalman Naqvi\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nDetecting Floods for Disaster Relief\n\n\nHow good are you at detecting floods?\n\n\nA rundown of the creation of my flood classifier.\n\n\n\n\n\n\n12 September 2022\n\n\nSalman Naqvi\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nData Quality is Important | Car Classifier\n\n\nClassy Cars\n\n\nMost of the time, data matters more than the model.\n\n\n\n\n\n\n04 June 2022\n\n\nSalman Naqvi\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nA No Nonsense Guide to Reading a Confusion Matrix\n\n\nAre you confused yet?\n\n\nA straight to the point guide about reading a confusion matrix.\n\n\n\n\n\n\n03 June 2022\n\n\nSalman Naqvi\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nMy first AI model\n\n\nCan you bare reading through this bear classifier?\n\n\nA rundown on my first attempt at creating model.\n\n\n\n\n\n\n28 May 2022\n\n\nSalman Naqvi\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nHow to Approach Creating AI Models\n\n\nPutting the Drive into the Train\n\n\nThere’s more to AI than just creating models.\n\n\n\n\n\n\n27 May 2022\n\n\nSalman Naqvi\n\n\n5 min\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\nLoading…\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the world of ForBo7",
    "section": "",
    "text": "I’m a curious individual who dabbles in AI, does 3D CG, loves learning, and is a scitech geek and space nerd. I’ve also been to 18 countries.\nRead more about me here.\nFeel free to contact me with one of the fancy buttons below!\n \n  \n   \n  \n    \n     fastai Forums\n  \n  \n    \n     Twitter\n  \n  \n    \n     Artstation\n  \n  \n    \n     Kaggle\n  \n  \n    \n     GitHub\n  \n  \n    \n     Email"
  },
  {
    "objectID": "index.html#forblog",
    "href": "index.html#forblog",
    "title": "Welcome to the world of ForBo7",
    "section": "ForBlog",
    "text": "ForBlog\nClick here to check out the latest on the ForBlog.\n\n\n\n\n  \n\n\n\n\nIntuitively Approaching Einstein Summation Notation\n\n\nPutting it in an einsum-ple manner.\n\n\nAn alternative way to write matrix operations.\n\n\n\n\n\n\nJun 4, 2023\n\n\nSalman Naqvi\n\n\n\n\n\n\n  \n\n\n\n\n(Un)successfully Implementing DiffEdit\n\n\nThe (Un)expected Difficulties of Editing\n\n\nAn attempt at implementing the DiffEdit paper.\n\n\n\n\n\n\nMay 29, 2023\n\n\nSalman Naqvi\n\n\n\n\n\n\n  \n\n\n\n\nIterators and Generators\n\n\nIteratively Generating Iterative Generators\n\n\nIterators and generators shown by example.\n\n\n\n\n\n\nMay 3, 2023\n\n\nSalman Naqvi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#playground",
    "href": "index.html#playground",
    "title": "Welcome to the world of ForBo7",
    "section": "Playground",
    "text": "Playground\n\nThe playground is currently offline."
  },
  {
    "objectID": "unsubscribe.html",
    "href": "unsubscribe.html",
    "title": "Unsubscribe from ForBlog and App Playground Notifications",
    "section": "",
    "text": "Loading…\n\n\n\n\n Back to top"
  },
  {
    "objectID": "feedback.html",
    "href": "feedback.html",
    "title": "Site Feedback",
    "section": "",
    "text": "Loading…"
  },
  {
    "objectID": "dictionary/index.html",
    "href": "dictionary/index.html",
    "title": "The AI Dictionary",
    "section": "",
    "text": "I often find explanations online to be more complicated than they need to be. Here, I hope to fix that. New terms will continue to be added over time.\nClick terms to view expanded definitions.\nDo let me know of any corrections and improvements, and of any terms you would like added!\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\nAccuracy\n\n\n\n\n\nA type of metric. It is a value that tells us how often a model produces correct predictions. The higher the accuracy, the better.\n\n\n\n\n \n\n\n\n\n\n\nArchitecture\n\n\n\n\n\nA model that is used as a template or a starting point for another model.\n\n\n\n\n \n\n\n\n\n\n\nBagging\n\n\n\n\n\nAn ensembling technique. When bagging, each model is trained on random subset of the rows, and a random subset of the columns, with replacement.\n\n\n\n\n \n\n\n\n\n\n\nCross Entropy Loss\n\n\n\n\n\nA technique for calculating the loss for categorical models with multiple categories.\n\n\n\n\n \n\n\n\n\n\n\nDecision Tree\n\n\n\n\n\nA type of model that acts like an if-else statement.\n\n\n\n\n \n\n\n\n\n\n\nDecoder (Transformers)\n\n\n\n\n\nA compoent of a trasnformer that is used for generating text. An example is the autocomplete feature on a smartphone’s keyboard.\n\n\n\n\n \n\n\n\n\n\n\nDocument\n\n\n\n\n\nThe name given to a piece or collection of text. It can range from anything from a single word to a sentence to a paragraph to a page of text to a full book, and so on. Also referred to as sequence.\n\n\n\n\n \n\n\n\n\n\n\nDot Product\n\n\n\n\n\nThe operation given to the process of taking the product of each corresponding element in a vector, and summing all products. Also known as linear combination.\n\n\n\n\n \n\n\n\n\n\n\nEmbedding\n\n\n\n\n\nA table, or matrix, where each row represents an item and each column describes the items in some way. The real magic of embeddings happen when you combine two embeddings together in some way to obtain further information.\n\n\n\n\n \n\n\n\n\n\n\nEncoder (Transformers)\n\n\n\n\n\nA component of a transformer that is used for “understanding” text. Encoders are typically used for classifying sentences by sentiment and figuring out what parts of a sentence refers, for example, to a person or location.\n\n\n\n\n \n\n\n\n\n\n\nEnsemble\n\n\n\n\n\nA collection of models whos’ predictions are averaged to obtain the final prediction.\n\n\n\n\n \n\n\n\n\n\n\nError Rate\n\n\n\n\n\nA type of metric. It is a value that tells us how often a model produces incorrect predictions. The lower the error rate, the better.\n\n\n\n\n \n\n\n\n\n\n\nGradient\n\n\n\n\n\nA numerical value which adjusts the parameters of a model. How much it adjusts is controlled by the learning rate.\n\n\n\n\n \n\n\n\n\n\n\nGradient Accumulation\n\n\n\n\n\nA technique for running or fitting large models on a not-so-powerful GPU.\n\n\n\n\n \n\n\n\n\n\n\nGradient Boosting Machine (GBM)\n\n\n\n\n\nAn ensembling technique where instead of averaging the predictions of all models, each successive model predicts the error of the previous model. The errors are then summed to obtain the final prediction.\n\n\n\n\n \n\n\n\n\n\n\nK-Fold Cross Validation\n\n\n\n\n\nAn ensembling technique where models are trained on a different set percent of the dataset. For example each model is trained on a different 80% of the dataset.\n\n\n\n\n \n\n\n\n\n\n\nLearning Rate\n\n\n\n\n\nA numerical value which controls how much the gradients update the parameters of a model.\n\n\n\n\n \n\n\n\n\n\n\nLinear Combination\n\n\n\n\n\nThe operation given to the process of taking the product of each corresponding element in a vector, and summing all products. Also known as dot product.\n\n\n\n\n \n\n\n\n\n\n\nLoss\n\n\n\n\n\nA meaasure of performance of a model. It is used by the model to improve itself. Typically, the lower the loss, the better.\n\n\n\n\n \n\n\n\n\n\n\nMatrix\n\n\n\n\n\nA table of values. See also vector\n\n\n\n\n \n\n\n\n\n\n\nMean Absolute Error (MAE)\n\n\n\n\n\nA type of metric. It is a value that tells us, on average, how close a set of predicted values is from the actual values. The smaller the MAE, the better.\n\n\n\n\n \n\n\n\n\n\n\nMean Squared Error (MSE)\n\n\n\n\n\nA type of metric. It is a value that tells us, on average, how close a set of predicted values is from the actual values. The smaller the MSE, the better.\n\n\n\n\n \n\n\n\n\n\n\nMetric\n\n\n\n\n\nA measure of performance of a model. It is used by humans to judge the performance of the model.\n\n\n\n\n \n\n\n\n\n\n\nModel\n\n\n\n\n\nA mathematical equation that mimicks a real life phenomenon. This equation can be used to predict desired quantities.\n\n\n\n\n \n\n\n\n\n\n\nNamed Entity Recognition (NER)\n\n\n\n\n\nA NLP classification task where a sentence is broken into its components, and the model attempts to assign each component to a specific entity (e.g., person, place, organization).\n\n\n\n\n \n\n\n\n\n\n\nNumericalization\n\n\n\n\n\nA process where numbers are assigned to each token. Occurs after tokenization.\n\n\n\n\n \n\n\n\n\n\n\nOne Hot Encoding\n\n\n\n\n\nA data processing technique where each class in a categorical feature is given its own column that contains true and false values.\n\n\n\n\n \n\n\n\n\n\n\nOneR Classifier\n\n\n\n\n\nThe simplest type of decision tree. The tree only contains a single split.\n\n\n\n\n \n\n\n\n\n\n\nRandom Forest\n\n\n\n\n\nThe name given to a bagged ensemble of decision trees.\n\n\n\n\n \n\n\n\n\n\n\nRoot Mean Squared Error (RMSE)\n\n\n\n\n\nA type of metric. It is a value that tells us, on average, how close a set of predicted values is from the actual values. The smaller the RMSE, the better.\n\n\n\n\n \n\n\n\n\n\n\nRoot Mean Squared Logarithmic Error (RMSLE)\n\n\n\n\n\nA type of metric. It is a value that tells us, on average, how close a set of predicted values is from the actual values. The smaller the RMSLE, the better.\n\n\n\n\n \n\n\n\n\n\n\nSequence\n\n\n\n\n\nThe name given to a piece or collection of text. It can range from anything from a single word to a sentence to a paragraph to a page of text to a full book, and so on. Also referred to as document.\n\n\n\n\n \n\n\n\n\n\n\nSoftmax\n\n\n\n\n\nA function that calculates the probabilities of a set of predictions.\n\n\n\n\n \n\n\n\n\n\n\nTabular Data\n\n\n\n\n\nData in the form of a table.\n\n\n\n\n \n\n\n\n\n\n\nTabular Model\n\n\n\n\n\nA model trained on tabular data. It is used to predict a specified column in the data.\n\n\n\n\n \n\n\n\n\n\n\nTokenization\n\n\n\n\n\nSplitting a document into its component words.\n\n\n\n\n \n\n\n\n\n\n\nTransformer\n\n\n\n\n\nThe name given to a Natural Language Processing (NLP) architecture that, in a nutshell, either fills-in-the-blanks or autocompletes text. Transformers consist of either an encoder, decoder, or both.\n\n\n\n\n \n\n\n\n\n\n\nVector\n\n\n\n\n\nA table of values that has either a single row or a single column. See also matrix.\n\n\n\n\n \n\n\n\n\n\n\nWeight Decay\n\n\n\n\n\nA technique for making sure your weights do not grow too large, and in turn overfit the data.\n\n\n\n\n \n\n\n\n\n\n\nZero-shot\n\n\n\n\n\nA prefix given to a pretrained model that can be used without finetuning.\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\nA compressed image.\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\nUsing a trained model for predictions.\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/gradient.html",
    "href": "dictionary/terms/gradient.html",
    "title": "Gradient",
    "section": "",
    "text": "The gradients update the parameters by multiplying the two together. How much the gradients update the parameters is controlled by the learning rate.\nA positive gradient tells us that increasing the parameters will increase the loss. On the other hand, decreasing the parameters will decrease the loss.\nA negative gradient tells us that decreasing the parameters will increase the loss On the other hand, increasing the parameters will decrease the loss.\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/root_mean_squared_error_rmse.html",
    "href": "dictionary/terms/root_mean_squared_error_rmse.html",
    "title": "Root Mean Squared Error (RMSE)",
    "section": "",
    "text": "It is calculated by:\n\nFirst taking the difference between each respective predicted and actual value.\nThen the squaring all obtained values.\nTaking the average.\nAnd finally taking the square root.\n\nLet’s say we have a set of predicted values \\(1, 2, 3, 4\\). The set of actual values is \\(1, 4, 3, 3\\)\n\n\\(1-1, 2-4, 3-3, 4-3, = 0, -2, 0, 1\\)\n\\((0)^2, (-2)^2, (0)^2, (1)^2 = 0, 4, 0, 1\\)\n\\(\\frac{0 + 4 + 0 + 1}{4} = \\frac{5}{4} = 1.25\\)\n\\(\\sqrt{1.25} \\approx 1.12\\)\n\nThis tells us, that on average, our set of predicted values is \\(1.12\\) units off from the actual values.\nIn a nutshell, you take the root of the mean of the square of the differences between the predicted and actual values.\n\nThe main difference between MSE and RMSE is that RMSE undoes the squaring step by taking the square root.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe reason the square value is taken is due to the averaging step. Let’s say the first predicted value is off from the first actual value by \\(-3\\) units. And let’s say that the second predicted value is off from the second actual value by \\(3\\) units.\nIf we didn’t take the square, the average would be zero \\(\\left( \\frac{-3 + 3}{2} = \\frac{0}{2} = 0 \\right)\\). This is incorrect as both values are off from the actual value.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/decoder.html",
    "href": "dictionary/terms/decoder.html",
    "title": "Decoder (Transformers)",
    "section": "",
    "text": "To learn more about decoders, you can read this to the point rundown.\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/gradient_boosting_machine_gbm.html",
    "href": "dictionary/terms/gradient_boosting_machine_gbm.html",
    "title": "Gradient Boosting Machine (GBM)",
    "section": "",
    "text": "The first model produces a prediction.\nThe difference between this prediction and the actual value is obtained.\nThe difference is now set as the target.\nThe next model now attempts to predict this difference.\nRepeat steps 2-4 for as many models as desired.\nSum all obtained differences.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nWhile this technique tends to produce better results, is more likely to overfit. This is because the machine is trying to minimize the difference between the predictions and actual values in the training set.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/learning_rate.html",
    "href": "dictionary/terms/learning_rate.html",
    "title": "Learning Rate",
    "section": "",
    "text": "The learning rate controls how much the gradients adjust the parameters by multiplying the learning rate and gradients together.\n\n\n\n\n\n\nNote\n\n\n\n\n\nA learning rate that is too high can cause the training system to either get stuck in a loop or diverge from the optimal weights.\nA learning rate that is too low can cause the training system to take a very long time to reach the optimal weights.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/sequence.html",
    "href": "dictionary/terms/sequence.html",
    "title": "Sequence",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/softmax.html",
    "href": "dictionary/terms/softmax.html",
    "title": "Softmax",
    "section": "",
    "text": "Let’s say that we have a model that tells us what sort of vehicle is in a picture. It outputs the following predictions.\n\n\n\n\n\n\n\nVehicle\nPrediction\n\n\n\n\ncar\n\\(-4.89\\)\n\n\nbus\n\\(2.60\\)\n\n\ntruck\n\\(0.59\\)\n\n\nmotorbike\n\\(-2.07\\)\n\n\nbicycle\n\\(-4.57\\)\n\n\n\nThese predictions aren’t very meaningful to us as humans. So what we can do is convert these predictions into probabilities. The steps to do this are below.\n1. Take the exponent of each prediction to base \\(e\\). So for the car category, \\(e^{-4.89} \\approx 7.52 \\cdot 10^{-3}\\).\nThe results of the calculations below are displayed with 3 significant figures.\n\n\n\n\n\n\n\n\nVehicle\nPrediction\n\\(e^{\\text{prediction}}\\)\n\n\n\n\ncar\n\\(-4.89\\)\n\\(7.52 \\cdot 10^{-3}\\)\n\n\nbus\n\\(2.60\\)\n\\(13.4\\)\n\n\ntruck\n\\(0.59\\)\n\\(1.80\\)\n\n\nmotorbike\n\\(-2.07\\)\n\\(0.126\\)\n\n\nbicycle\n\\(-4.57\\)\n\\(0.010\\)\n\n\n\n2. Sum all the calculated values.\n\n\n\n\n\n\n\n\n\nVehicle\nPrediction\n\\(e^{\\text{prediction}}\\)\n\\(\\text{sum of} e^{\\text{prediction}}\\)\n\n\n\n\ncar\n\\(-4.89\\)\n\\(7.52 \\cdot 10^{-3}\\)\n\\(15.4\\)\n\n\nbus\n\\(2.60\\)\n\\(13.4\\)\n\\(15.4\\)\n\n\ntruck\n\\(0.59\\)\n\\(1.80\\)\n\\(15.4\\)\n\n\nmotorbike\n\\(-2.07\\)\n\\(0.126\\)\n\\(15.4\\)\n\n\nbicycle\n\\(-4.57\\)\n\\(0.010\\)\n\\(15.4\\)\n\n\n\n3. For each respective category, divide \\(e^{\\text{prediction}}\\) by \\(\\text{sum of} e^{\\text{prediction}}\\). This is your probability. So the probability of the vehicle in the picture being a car is\n\\[\n\\frac{7.52 \\cdot 10^{-3}}{15.4} \\approx 4.88 \\cdot 10^{-4} = 0.000488 = 0.0488 \\%\n\\]\n\n\n\n\n\n\n\n\n\n\nVehicle\nPrediction\n\\(e^{\\text{prediction}}\\)\n\\(\\text{sum of} e^{\\text{prediction}}\\)\n\\(\\frac{e^{\\text{prediction}}}{\\text{sum of}e^{\\text{prediction}}}\\)\n\n\n\n\ncar\n\\(-4.89\\)\n\\(7.52 \\cdot 10^{-3}\\)\n\\(15.4\\)\n\\(4.88 \\cdot 10^{-4}\\)\n\n\nbus\n\\(2.60\\)\n\\(13.4\\)\n\\(15.4\\)\n\\(0.874\\)\n\n\ntruck\n\\(0.59\\)\n\\(1.80\\)\n\\(15.4\\)\n\\(0.117\\)\n\n\nmotorbike\n\\(-2.07\\)\n\\(0.126\\)\n\\(15.4\\)\n\\(8.19 \\cdot 10^{-3}\\)\n\n\nbicycle\n\\(-4.57\\)\n\\(0.010\\)\n\\(15.4\\)\n\\(6.72 \\cdot 10^{-4}\\)\n\n\n\nFrom the table above, it can be seen that the vehicle in the picture is most likely a bus with probability \\(87.4\\%\\).\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/mean_squared_error_mse.html",
    "href": "dictionary/terms/mean_squared_error_mse.html",
    "title": "Mean Squared Error (MSE)",
    "section": "",
    "text": "It is calculated by:\n\nFirst taking the difference between each respective predicted and actual value.\nThen the squaring all obtained values.\nAnd finally taking the average.\n\nLet’s say we have a set of predicted values \\(1, 2, 3, 4\\). The set of actual values is \\(1, 4, 3, 3\\)\n\n\\(1-1, 2-4, 3-3, 4-3, = 0, -2, 0, 1\\)\n\\((0)^2, (-2)^2, (0)^2, (1)^2 = 0, 4, 0, 1\\)\n\\(\\frac{0 + 4 + 0 + 1}{4} = \\frac{5}{4} = 1.25\\)\n\nThis tells us, that on average, our set of predicted values is \\(1.25\\) units off from the actual values.\nIn a nutshell, you take the mean of the square of the differences between the predicted and actual values.\n\nThe main difference between MAE and MSE is that MSE penalizes smaller differences more heavily.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe reason the square value is taken is due to the averaging step. Let’s say the first predicted value is off from the first actual value by \\(-3\\) units. And let’s say that the second predicted value is off from the second actual value by \\(3\\) units.\nIf we didn’t take the square, the average would be zero \\(\\left( \\frac{-3 + 3}{2} = \\frac{0}{2} = 0 \\right)\\). This is incorrect as both values are off from the actual value.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/mean_absolute_error_mae.html",
    "href": "dictionary/terms/mean_absolute_error_mae.html",
    "title": "Mean Absolute Error (MAE)",
    "section": "",
    "text": "It is calculated by: 1. First taking the difference between each respective predicted and actual value. 1. Then removing all negative signs — this is known as taking the absolute value. 1. And finally taking the average.\nLet’s say we have a set of predicted values \\(1, 2, 3, 4\\). The set of actual values is \\(1, 4, 3, 3\\)\n\n\\(1-1, 2-4, 3-3, 4-3, = 0, -2, 0, 1\\)\n\\(|0|, |-2|, |0|, |1| = 0, 2, 0, 1\\)\n\\(\\frac{0 + 2 + 0 + 1}{4} = \\frac{2}{4} = 0.5\\)\n\nThis tells us, that on average, our set of predicted values is \\(0.5\\) units off from the actual values.\nIn a nutshell, you take the mean of the absolute differences between the predicted and actual values.\n\nThe main difference between MAE and MSE is that MSE penalizes smaller differences more heavily.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe reason the absolute value is taken is due to the averaging step. Let’s say the first predicted value is off from the first actual value by \\(-3\\) units. And let’s say that the second predicted value is off from the second actual value by \\(3\\) units.\nIf we didn’t take the absolute value, the average would be zero \\(\\left( \\frac{-3 + 3}{2} = \\frac{0}{2} = 0 \\right)\\). This is incorrect as both values are off from the actual value.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/cross_entropy_loss.html",
    "href": "dictionary/terms/cross_entropy_loss.html",
    "title": "Cross Entropy Loss",
    "section": "",
    "text": "Let’s say that we have a model that tells us what sort of vehicle is in a picture. It outputs the following predictions.\n\n\n\n\n\n\n\n\nVehicle\nActuals\nPrediction\n\n\n\n\ncar\n0\n\\(-4.89\\)\n\n\nbus\n1\n\\(2.60\\)\n\n\ntruck\n0\n\\(0.59\\)\n\n\nmotorbike\n0\n\\(-2.07\\)\n\n\nbicycle\n0\n\\(-4.57\\)\n\n\n\nActuals is a one hot encoded column that tells us what is the correct vehicle in the picture.\nTo convert these predictions into loss, first take the softmax of each prediction.\n\n\n\n\n\n\n\n\n\nVehicle\nActuals\nPrediction\nSoftmax\n\n\n\n\ncar\n0\n\\(-4.89\\)\n\\(4.88 \\cdot 10^{-4}\\)\n\n\nbus\n1\n\\(2.60\\)\n\\(0.874\\)\n\n\ntruck\n0\n\\(0.59\\)\n\\(0.117\\)\n\n\nmotorbike\n0\n\\(-2.07\\)\n\\(8.19 \\cdot 10^{-3}\\)\n\n\nbicycle\n0\n\\(-4.57\\)\n\\(6.72 \\cdot 10^{-4}\\)\n\n\n\nNext take the logarithm of each softmax value.\n\n\n\n\n\n\n\n\n\n\nVehicle\nActuals\nPrediction\nSoftmax\n\\(\\ln(\\text{Softmax})\\)\n\n\n\n\ncar\n0\n\\(-4.89\\)\n\\(4.88 \\cdot 10^{-4}\\)\n\\(-7.63\\)\n\n\nbus\n1\n\\(2.60\\)\n\\(0.874\\)\n\\(-1.35\\)\n\n\ntruck\n0\n\\(0.59\\)\n\\(0.117\\)\n\\(-2.14\\)\n\n\nmotorbike\n0\n\\(-2.07\\)\n\\(8.19 \\cdot 10^{-3}\\)\n\\(-4.81\\)\n\n\nbicycle\n0\n\\(-4.57\\)\n\\(6.72 \\cdot 10^{-4}\\)\n\\(-7.31\\)\n\n\n\nMultiply the actuals with the computed logarithms.\n\n\n\n\n\n\n\n\n\n\n\nVehicle\nActuals\nPrediction\nSoftmax\n\\(\\ln(\\text{Softmax})\\)\n\\(\\text{Actuals} \\cdot \\ln(\\text{Softmax})\\)\n\n\n\n\ncar\n0\n\\(-4.89\\)\n\\(4.88 \\cdot 10^{-4}\\)\n\\(-7.63\\)\n\\(0\\)\n\n\nbus\n1\n\\(2.60\\)\n\\(0.874\\)\n\\(-1.35\\)\n\\(-1.35\\)\n\n\ntruck\n0\n\\(0.59\\)\n\\(0.117\\)\n\\(-2.14\\)\n\\(0\\)\n\n\nmotorbike\n0\n\\(-2.07\\)\n\\(8.19 \\cdot 10^{-3}\\)\n\\(-4.81\\)\n\\(0\\)\n\n\nbicycle\n0\n\\(-4.57\\)\n\\(6.72 \\cdot 10^{-4}\\)\n\\(-7.31\\)\n\\(0\\)\n\n\n\nSum the the results of the multiplications.\n\\[\n0 + -1.35 + 0 + 0 + 0 = -1.35\n\\]\nAnd there you have your loss!\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/error_rate.html",
    "href": "dictionary/terms/error_rate.html",
    "title": "Error Rate",
    "section": "",
    "text": "It can be calculated by dividing the number of incorrect predictions by the number of total predictions. Optionally multiply the result by 100 to obtain a percentage.\n\\[\n\\frac{\\text{number of incorrect predictions}}{\\text{number of total predictions}}\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nError rate is also 1 - accuracy.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/decision_tree.html",
    "href": "dictionary/terms/decision_tree.html",
    "title": "Decision Tree",
    "section": "",
    "text": "A split is made for each feature in the data. If the feature of a certain data sample is larger than or less than the split for that respective feature, the next appropriate split is made.\nBelow is an example determining whether a car is fast or slow.\n\n\n\n\nflowchart TB\n  A([Weight &lt; 2000kg])\n  B([Is Engine Powerful])\n  C([Is Windy Day])\n  D1([Car Is Fast])\n  E1([Car Is Slow])\n  D2([Car Is Fast])\n  E2([Car Is Slow])\n\n\n  A -- Yes --&gt; B\n  A -- No --&gt; C\n  B -- Yes --&gt; D1\n  B -- No --&gt; E1\n  C -- Yes --&gt; E2\n  C -- No --&gt; D2\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/k_fold_cross_validation.html",
    "href": "dictionary/terms/k_fold_cross_validation.html",
    "title": "K-Fold Cross Validation",
    "section": "",
    "text": "Another way to think of it is that the dataset is split into \\(K\\) pieces. Then each model is trained on a different set of \\(K-1\\) pieces.\nFor example, let’s say that the dataset is split into 5 pieces. Then each model is trained on a different set of 4 pieces.\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/vector.html",
    "href": "dictionary/terms/vector.html",
    "title": "Vector",
    "section": "",
    "text": "The order of a vector is row by column.\nBelow is a \\(1 \\times 3\\) column vector. \\[\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3 \\\\\n\\end{bmatrix}\n\\]\nBelow is a \\(3 \\times 1\\) row vector. \\[\n\\begin{bmatrix}\n1 & 2 & 3\n\\end{bmatrix}\n\\]\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/one_hot_encoding.html",
    "href": "dictionary/terms/one_hot_encoding.html",
    "title": "One Hot Encoding",
    "section": "",
    "text": "Let’s say we have a categorical feature, speed, that can be either slow or fast. Instead of assigning a value of 0 to slow and a value of 1 to fast, a slow column can be created and a fast column can be created. If the speed is slow, slow is true and fast is false. If the speed is fast, slow is false and fast is true.\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/random_forest.html",
    "href": "dictionary/terms/random_forest.html",
    "title": "Random Forest",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/ensemble.html",
    "href": "dictionary/terms/ensemble.html",
    "title": "Ensemble",
    "section": "",
    "text": "The reason why this works is that some models will overestimate while others will underestimate, cancelling out each others’ errors.\nThere are different methods for ensembling.\n\n\n\n\n\n\nEnsembling only works when all models are independent of each other. That is, the models do not depend on one another.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/linear_combination.html",
    "href": "dictionary/terms/linear_combination.html",
    "title": "Linear Combination",
    "section": "",
    "text": "\\[\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3 \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n4 \\\\\n5 \\\\\n6 \\\\\n\\end{bmatrix}\n= (1 \\cdot 4) + (2 \\cdot 5) + (3 \\cdot 6) = 32\n\\]\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/bagging.html",
    "href": "dictionary/terms/bagging.html",
    "title": "Bagging",
    "section": "",
    "text": "“with replacement” means that if a model, for examples, randomly chooses row number 5, another model can also randomly choose row number 5.\n\n\n\n\n\n\nNote\n\n\n\n\n\nThrough this technique, each model ends up training on roughly 63% of the entire dataset.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/root_mean_squared_logarithmic_error_rmsle.html",
    "href": "dictionary/terms/root_mean_squared_logarithmic_error_rmsle.html",
    "title": "Root Mean Squared Logarithmic Error (RMSLE)",
    "section": "",
    "text": "It is calculated by:\n\nFirst taking the logarithm of all predicted values.\nTaking the logarithm of all actual values.\nThen taking the difference between each respective predicted and actual value.\nNext squaring all obtained values.\nTaking the averge.\nAnd lastly taking the square root.\n\nLet’s say we have a set of predicted values \\(1, 2, 3, 4\\). The set of actual values is \\(1, 4, 3, 3\\)\n\n\\(\\ln(1), \\ln(2), \\ln(3), \\ln(4) \\approx 0, 0.69, 1.10, 1.39\\)\n\\(\\ln(1), \\ln(4), \\ln(3), \\ln(3) \\approx 0, 1.39, 1.10, 1.10\\)\n\\(0-0, 0.69-1.39, 1.10-1.10, 1.39-1.10 = 0, -0.70, 0, 0.29\\)\n\\((0)^2, (-0.70)^2, (0)^2, (0.29)^2 \\approx 0, 0.49, 0, 0.08\\)\n\\(\\frac{0 + 0.49 + 0 + 0.08}{4} = \\frac{0.57}{4}\\)\n\\(\\sqrt{\\frac{0.57}{4}} \\approx 0.38\\)\n\nThis tells us, that on average, our set of predicted values is \\(0.38\\) units off from the actual values.\nIn a nutshell, you take the root of the mean of the square of the differences between the predicted and actual values.\n\nThe main difference between RMSE and RMSLE is that RMSLE works better for very large values, since the logarithm of the predicted values and actual values is taken. The downside is that negative values will not work, since the logarithm of a negative value is undefined.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe reason the square value is taken is due to the averaging step. Let’s say the first predicted value is off from the first actual value by \\(-3\\) units. And let’s say that the second predicted value is off from the second actual value by \\(3\\) units.\nIf we didn’t take the square, the average would be zero \\(\\left( \\frac{-3 + 3}{2} = \\frac{0}{2} = 0 \\right)\\). This is incorrect as both values are off from the actual value.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/gradient_accumulation.html",
    "href": "dictionary/terms/gradient_accumulation.html",
    "title": "Gradient Accumulation",
    "section": "",
    "text": "Let’s say you want to use a batch size of 64, but the model doesn’t fit with that size on your GPU.\n\nFirst determine the largest possible batch size that can fit on your GPU. Let’s say it’s 16. It may be better to use batch sizes that are a power of 2.\nCalculate the gradients for \\(X\\) batches without updating the parameters.\n\n\\(X\\) is your desired batch size divided by the batch size you are using.\nDesired batch size is 64; batch size we are using is 16.\n\\(64 ÷ 16 = 4\\)\n\\(X\\) is 4. This is because the size of 4 batches, in this case, sums to 64.\n\nNext, sum all respective gradients — hence the term ‘gradient accumulation’.\nNow update your parameters based on these summed gradients. This will have the same effect as if you used a batch size of 64.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nUsing a smaller batch size to fit a larger model onto your GPU isn’t optimal. A smaller batch size means you would have to tweak your optimal hyperparameters, such as the learning rate. Your loss would also become less accurate since it is being calculated on a smaller group of items.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/metric.html",
    "href": "dictionary/terms/metric.html",
    "title": "Metric",
    "section": "",
    "text": "Examples of metrics are, but not limited to, accuracy, error rate, MAE, and MSE\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/oner_classifier.html",
    "href": "dictionary/terms/oner_classifier.html",
    "title": "OneR Classifier",
    "section": "",
    "text": "Below is an example determining whether a car is fast or slow.\n\n\n\n\nflowchart TB\n  A([Weight &lt; 2000kg])\n  B([Car Is Fast])\n  C([Car Is Slow])\n\n  A -- Yes --&gt; B\n  A -- No --&gt; C\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/dot_product.html",
    "href": "dictionary/terms/dot_product.html",
    "title": "Dot Product",
    "section": "",
    "text": "\\[\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3 \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n4 \\\\\n5 \\\\\n6 \\\\\n\\end{bmatrix}\n= (1 \\cdot 4) + (2 \\cdot 5) + (3 \\cdot 6) = 32\n\\]\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/accuracy.html",
    "href": "dictionary/terms/accuracy.html",
    "title": "Accuracy",
    "section": "",
    "text": "It can be calculated by dividing the number of correct predictions by the number of total predictions. Optionally multiply the result by 100 to obtain a percentage.\n\\[\n\\frac{\\text{number of correct predictions}}{\\text{number of total predictions}}\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nAccuracy is also 1 - error rate.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/numericalization.html",
    "href": "dictionary/terms/numericalization.html",
    "title": "Numericalization",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/tokenization.html",
    "href": "dictionary/terms/tokenization.html",
    "title": "Tokenization",
    "section": "",
    "text": "Note\n\n\n\n\n\nIf a word is too long or very uncommon, the word itself may be split. Take the word “supercalifragilisticexpialidocious” as an example. It could be split into “super”, “cali”, “fragilistic”, “expi”, “ali”, and “docious”.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/embedding.html",
    "href": "dictionary/terms/embedding.html",
    "title": "Embedding",
    "section": "",
    "text": "An example of how two embeddings can be combined together is shown below.\nLet’s say we have an embedding of users, where each column represents a feature about movies. Users like certain features of movies, and a value between -1 and 1 represents this.\n\n\n\nUser\nLong Duration\nSci-Fi\nFantasy\nAnimated\nAction\n\n\n\n\nBilly\n-0.9\n0.3\n0.2\n0.8\n0.25\n\n\nBob\n-0.85\n1\n-0.25\n0\n0.75\n\n\nJoe\n0.9\n0.85\n0.95\n0.35\n1\n\n\n\nNow let’s say we have an embedding of movies, where each column represents a feature about movies.\n\n\n\n\n\n\n\n\n\n\n\nMovie\nLong Duration\nSci-Fi\nFantasy\nAnimated\nAction\n\n\n\n\nThe Lord of the Rings\n1\n-1\n1\n-0.5\n1\n\n\nCars\n-0.9\n-1\n0.8\n1\n0\n\n\nInterstellar\n0.75\n1\n0\n0\n0.3\n\n\n\nWe want to find out which movie would be the best for Billy to watch. To do so, let’s take the dot product between Billy and each of the respective movies.\n\n\n\n\n\n\nBilly & The Lord of the Rings\n\n\n\n\n\n\\[\n(-0.9 \\cdot 1) + (0.3 \\cdot -1) + (0.2 \\cdot 1) + (0.8 \\cdot -0.5) + (0.25 \\cdot 1) = -1.15\n\\]\n\n\n\n\n\n\n\n\n\nBilly & Cars\n\n\n\n\n\n\\[\n(-0.9 \\cdot -0.9) + (0.3 \\cdot -1) + (0.2 \\cdot 0.8) + (0.8 \\cdot 1) + (0.25 \\cdot 0) = 1.47\n\\]\n\n\n\n\n\n\n\n\n\nBilly & Interstellar\n\n\n\n\n\n\\[\n(-0.9 \\cdot 0.75) + (0.3 \\cdot 1) + (0.2 \\cdot 0) + (0.8 \\cdot 0) + (0.25 \\cdot 0.3) = -0.3\n\\]\n\n\n\nWe have obtained the values \\(-1.15\\), \\(1.47\\), and \\(-0.3\\) for each of the movies respectively. From this, we can deduce that Cars ($1.47) is probably the best movie for Billy to watch, based on his taste.\nAfter similarly calculating the dot product between Joe and each of the movies, we get the following respective values: \\(1.82\\), \\(-0.55\\), \\(1.82\\). This tells us that both The Lord of the Rings and Interstellar are equally the best movies for Joe to watch.\nAs for Bob, it would be Interstellar.\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/zero_shot.html",
    "href": "dictionary/terms/zero_shot.html",
    "title": "Zero-shot",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/weight_decay.html",
    "href": "dictionary/terms/weight_decay.html",
    "title": "Weight Decay",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/model.html",
    "href": "dictionary/terms/model.html",
    "title": "Model",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/tabular_data.html",
    "href": "dictionary/terms/tabular_data.html",
    "title": "Tabular Data",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/loss.html",
    "href": "dictionary/terms/loss.html",
    "title": "Loss",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/architecture.html",
    "href": "dictionary/terms/architecture.html",
    "title": "Architecture",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/named_entity_recognition.html",
    "href": "dictionary/terms/named_entity_recognition.html",
    "title": "Named Entity Recognition (NER)",
    "section": "",
    "text": "Take the sentence “Tim went to the Moon.” as an example. The sentence would first be broken into ‘Tim’, ‘went’, ‘to’, the’, ‘Moon’. The model could then give ‘Tim’ the label of ‘person’, and ‘Moon’ the label of ‘location’.\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/transformer.html",
    "href": "dictionary/terms/transformer.html",
    "title": "Transformer",
    "section": "",
    "text": "To learn more about transformers, you can read this to the point rundown.\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/encoder.html",
    "href": "dictionary/terms/encoder.html",
    "title": "Encoder (Transformers)",
    "section": "",
    "text": "To learn more about decoders, you can read this to the point rundown.\n\n\n\n Back to top"
  },
  {
    "objectID": "dictionary/terms/latent.html",
    "href": "dictionary/terms/latent.html",
    "title": "ForBo7 // Salman Naqvi",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/inference.html",
    "href": "dictionary/terms/inference.html",
    "title": "ForBo7 // Salman Naqvi",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/neuron.html",
    "href": "dictionary/terms/neuron.html",
    "title": "ForBo7 // Salman Naqvi",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/tabular_model.html",
    "href": "dictionary/terms/tabular_model.html",
    "title": "Tabular Model",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/document.html",
    "href": "dictionary/terms/document.html",
    "title": "Document",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "dictionary/terms/matrix.html",
    "href": "dictionary/terms/matrix.html",
    "title": "Matrix",
    "section": "",
    "text": "The order of a matrix is row by column.\nBelow is \\(3 \\times 2\\) matrix. \\[\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6 \\\\\n\\end{bmatrix}\n\\]\nBelow is \\(2 \\times 3\\) matrix. \\[\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n\\] \\end{bmatrix}\n\n\n\n Back to top"
  },
  {
    "objectID": "testing_page.html",
    "href": "testing_page.html",
    "title": "Testing Page",
    "section": "",
    "text": "ForBlog\n\n\n\n\n  \n\n\n\n\nIntuitively Approaching Einstein Summation Notation\n\n\nPutting it in an einsum-ple manner.\n\n\nAn alternative way to write matrix operations.\n\n\n\n\n\n\nJun 4, 2023\n\n\nSalman Naqvi\n\n\n\n\n\n\n  \n\n\n\n\n(Un)successfully Implementing DiffEdit\n\n\nThe (Un)expected Difficulties of Editing\n\n\nAn attempt at implementing the DiffEdit paper.\n\n\n\n\n\n\nMay 29, 2023\n\n\nSalman Naqvi\n\n\n\n\n\n\n  \n\n\n\n\nIterators and Generators\n\n\nIteratively Generating Iterative Generators\n\n\nIterators and generators shown by example.\n\n\n\n\n\n\nMay 3, 2023\n\n\nSalman Naqvi\n\n\n\n\n\n\nNo matching items\n\n\n\n\nPlayground\n\n\n\n\n\n\n\n\n\n\nMore apps coming soon…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlood Classifier\n\n\nHow well can you classify floods?\n\n\n\nSalman Naqvi\n\n\nSep 20, 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Salman Naqvi",
    "section": "",
    "text": "Greetings, fellow human! I’m a curious individual who dabbles in AI, does 3D CG, loves learning, and is a scitech geek and space nerd. I’ve also been to 18 countries."
  },
  {
    "objectID": "about.html#lived-in",
    "href": "about.html#lived-in",
    "title": "Salman Naqvi",
    "section": "Lived in",
    "text": "Lived in\nUnited Kingdom (5 years) | Saudi Arabia (6 years) | Pakistan (9 years)"
  },
  {
    "objectID": "about.html#been-to",
    "href": "about.html#been-to",
    "title": "Salman Naqvi",
    "section": "Been to",
    "text": "Been to\n\n\nEgypt* | England | France | Greece | Italy | Jordan* | Lebanon* | Malaysia | Pakistan | Qatar* | Saudi Arabia | Sri Lanka | Switzerland | Tanzania | Thailand | Turkey | UAE | Wales\n*Transit destinations; I count them still because I was technically within the country’s border."
  },
  {
    "objectID": "web_apps/apps/coming_soon.html",
    "href": "web_apps/apps/coming_soon.html",
    "title": "More apps coming soon…",
    "section": "",
    "text": "I never told you how soon…\n\n\n\nThis image was generated by Dall-E 2!\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "web_apps/apps/bear_detector.html",
    "href": "web_apps/apps/bear_detector.html",
    "title": "Bear Classifier",
    "section": "",
    "text": "This webapp was remade on Tuesday, 8 November 2022.\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "web_apps/apps/flood_detector.html",
    "href": "web_apps/apps/flood_detector.html",
    "title": "Flood Classifier",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "web_apps/index.html",
    "href": "web_apps/index.html",
    "title": "App Playground",
    "section": "",
    "text": "The playground is currently offline."
  }
]