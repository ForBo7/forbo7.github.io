<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Salman Naqvi">
<meta name="description" content="Where does a VLM actually look at when looking at an image?">

<title>Visualising VLM attention – ForBo7 // Salman Naqvi</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-7c1e9744246cb75393526fa05decce7b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/quarto-contrib/line-highlight-1.0.0/line-highlight.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Visualising VLM attention – ForBo7 // Salman Naqvi">
<meta property="og:description" content="Where does a VLM actually look at when looking at an image?">
<meta property="og:image" content="https://forbo7.github.io/forblog/images/28_visualising_vlm_attention/thumbnail.png">
<meta property="og:site_name" content="ForBo7 // Salman Naqvi">
<meta property="og:image:height" content="1535">
<meta property="og:image:width" content="2730">
<meta name="twitter:title" content="ForBo7 // Salman Naqvi">
<meta name="twitter:description" content="Where does a VLM actually look at when looking at an image?">
<meta name="twitter:image" content="https://forbo7.github.io/forblog/images/28_visualising_vlm_attention/thumbnail.png">
<meta name="twitter:creator" content="@ForBo7">
<meta name="twitter:site" content="@ForBo7">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image-height" content="1535">
<meta name="twitter:image-width" content="2730">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">ForBo7 // Salman Naqvi</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> <i class="bi bi-house-door" role="img">
</i> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../forblog/index.html"> <i class="bi bi-newspaper" role="img">
</i> 
<span class="menu-text">ForBlog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../bitsandbobs/index.html"> <i class="bi bi-puzzle" role="img">
</i> 
<span class="menu-text">Bits and Bobs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../dictionary/index.html"> <i class="bi bi-book" role="img">
</i> 
<span class="menu-text">Dictionary</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> <i class="bi bi-file-person" role="img">
</i> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
    <a href="https://github.com/ForBo7/forbo7.github.io" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-code-slash"></i></a>
    <a href="../../forblog/index.xml" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-rss"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#setup" id="toc-setup" class="nav-link active" data-scroll-target="#setup">Setup</a></li>
  <li><a href="#getting-started" id="toc-getting-started" class="nav-link" data-scroll-target="#getting-started">Getting Started</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="28_visualising_vlm_attention.out.ipynb" download="28_visualising_vlm_attention.out.ipynb"><i class="bi bi-journal-code"></i>Jupyter</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Visualising VLM attention</h1>
<p class="subtitle lead">Visualising the visualiser</p>
  <div class="quarto-categories">
    <div class="quarto-category">Computer Vision</div>
    <div class="quarto-category">LLMs</div>
  </div>
  </div>

<div>
  <div class="description">
    Where does a VLM actually look at when looking at an image?
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Salman Naqvi </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">Monday, 17 November 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>In this notebook, I attempt to visualize how a VLM places attention on any input image, and on any input instruction.</p>
<blockquote class="blockquote">
<p>This notebook follows the <a href="https://docs.fast.ai/dev/style.html">fastai style guide</a>.</p>
</blockquote>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>Some of the the cell outputs have been deleted to keep the PDF concise. Rerun the notebook to see full outputs.</p>
</div>
</div>
<section id="setup" class="level2">
<h2 class="anchored" data-anchor-id="setup">Setup</h2>
<div id="1a0653ff" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="9d8b9d06-81e3-4a12-8240-fd0d700129a8" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> contextlib <span class="im">import</span> contextmanager</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> traceback</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="at">@contextmanager</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> suplog():</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">try</span>: <span class="cf">yield</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">except</span> <span class="pp">Exception</span>: traceback.print_exc()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> suplog(): <span class="dv">5</span><span class="op">/</span><span class="dv">0</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre data-code-line-numbers=""><code>Traceback (most recent call last):
  File "/tmp/ipykernel_265136/256214625.py", line 5, in suplog
    try: yield
  File "/tmp/ipykernel_265136/256214625.py", line 7, in &lt;module&gt;
    with suplog(): 5/0
ZeroDivisionError: division by zero</code></pre>
</div>
</div>
<p>This is a nifty snippet to keep errors in a notebook for future reference, but to allow myself to run all cells without halting execution.</p>
<div id="43383f77" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastcore.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="LRHBKi4ktdHs" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="9899a0d2-8bd1-4af0-a848-73c0f6168e83" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> in_colab():</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="op">!</span> pip install qwen<span class="op">-</span>vl<span class="op">-</span>utils[decord]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="71431714" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:105,&quot;referenced_widgets&quot;:[&quot;8e7a1225575b4349b4f1ffe7922ee9d7&quot;,&quot;139cb1a39be24fe3b662e0d573337a4a&quot;,&quot;1b411723702c4cf19df72320e4717d9a&quot;,&quot;ed263a410d3846019c68cc883d9b1c56&quot;,&quot;36478b0e63714b739e7dff2eee85ec26&quot;,&quot;28d6305cdf724553a83b1a48aefc7363&quot;,&quot;001dde98014b4daca7312ea569bdfc31&quot;,&quot;ea69eb487d424d8c9e8f03701df998f1&quot;,&quot;c45c84229c694ddca013ee6aa5ab3569&quot;,&quot;7286e200dc4b4f9bbde05d746f7f7269&quot;,&quot;37aa80f78a444452867c0982c5a37a43&quot;]}}" data-outputid="9df20deb-bda7-4c17-bbd5-1125ababead7" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor, AutoConfig</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>cp <span class="op">=</span> <span class="st">'BAAI/RoboBrain2.0-3B'</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>vlm <span class="op">=</span> Qwen2_5_VLForConditionalGeneration.from_pretrained(cp, attn_implementation<span class="op">=</span><span class="st">'eager'</span>, torch_dtype<span class="op">=</span><span class="st">"float16"</span>, device_map<span class="op">=</span><span class="st">"auto"</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>proc <span class="op">=</span> AutoProcessor.from_pretrained(cp)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>tokz <span class="op">=</span> AutoTokenizer.from_pretrained(cp)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>cfg <span class="op">=</span> AutoConfig.from_pretrained(cp)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"13e2b23a03e84f50b196e6925fc6bdbf","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre data-code-line-numbers=""><code>Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.</code></pre>
</div>
</div>
<div id="8a770d47" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="cb41a941-8e3f-4a1e-a5b8-a0277dd1e6f0" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>vlm.model.config._attn_implementation, vlm.visual.config._attn_implementation</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre data-code-line-numbers=""><code>('eager', 'eager')</code></pre>
</div>
</div>
<div id="cb62f342" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1fbbe99f-1dac-4e17-c3a9-8849d37039ad" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>cfg</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre data-code-line-numbers=""><code>Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 128000,
  "max_window_layers": 70,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 16,
  "num_hidden_layers": 36,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.50.0",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 2048,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "torch_dtype": "bfloat16",
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}</code></pre>
</div>
</div>
</section>
<section id="getting-started" class="level2">
<h2 class="anchored" data-anchor-id="getting-started">Getting Started</h2>
<p>In this notebook, I’ll be using the recent RoboBrain 2.0 model, which blows other VLMs out of the water. It’s a refreshing model, since the reason it performed so well was because the team worked directly with the data.</p>
<p>Future note to self: if too much attention is placed on the system prompt, remove it.</p>
<div id="b5681281" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:497}}" data-outputid="4f4edf8b-8217-4884-de82-89e7c541a1ad" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Image</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>img_p <span class="op">=</span> <span class="st">'http://images.cocodataset.org/val2017/000000039769.jpg'</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>Image(img_p)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<div>
<figure class="figure">
<p><img src="28_visualising_vlm_attention_files/figure-html/cell-8-output-1.jpeg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="60999a6b" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># q = "Which way is the cat on the right facing?&lt;think&gt;"</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> <span class="st">"Describe the cat on the left.&lt;think&gt;"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="1b39800d" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>msgs <span class="op">=</span> [</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'role'</span>: <span class="st">'user'</span>,</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'content'</span>: [</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        {<span class="st">'type'</span>: <span class="st">'image'</span>, <span class="st">'image'</span>: img_p <span class="cf">if</span> img_p.startswith(<span class="st">'http'</span>) <span class="cf">else</span> <span class="ss">f'file://</span><span class="sc">{</span>img_p<span class="sc">}</span><span class="ss">'</span>},</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        {<span class="st">'type'</span>: <span class="st">'text'</span>, <span class="st">'text'</span>: <span class="ss">f'</span><span class="sc">{</span>q<span class="sc">}</span><span class="ss">'</span>},</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>  },</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="b088764a" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:53}}" data-outputid="6873936e-1d36-4934-a52e-ca56cf48acbf" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>txt_inp <span class="op">=</span> proc.apply_chat_template(msgs, tokenize<span class="op">=</span><span class="va">False</span>, add_generation_prompt<span class="op">=</span><span class="va">True</span>)<span class="op">;</span> txt_inp</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre data-code-line-numbers=""><code>'&lt;|im_start|&gt;system\nYou are a helpful assistant.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n&lt;|vision_start|&gt;&lt;|image_pad|&gt;&lt;|vision_end|&gt;Describe the cat on the left.&lt;think&gt;&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n'</code></pre>
</div>
</div>
<p>I’ll remove the system prompt as it seems the VLM places a lot of attention there. If I leave it in, the visualizations become skewed.</p>
<div id="4bbd9780" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>txt_inp <span class="op">=</span> txt_inp.replace(<span class="st">'&lt;|im_start|&gt;system</span><span class="ch">\n</span><span class="st">You are a helpful assistant.&lt;|im_end|&gt;</span><span class="ch">\n</span><span class="st">'</span>, <span class="st">''</span>)<span class="op">;</span> txt_inp</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre data-code-line-numbers=""><code>'&lt;|im_start|&gt;user\n&lt;|vision_start|&gt;&lt;|image_pad|&gt;&lt;|vision_end|&gt;Describe the cat on the left.&lt;think&gt;&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n'</code></pre>
</div>
</div>
<div id="d09d003c" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> qwen_vl_utils <span class="im">import</span> process_vision_info</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>img_inp, vid_inp <span class="op">=</span> process_vision_info(msgs)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Over here, the image has been padded so that each dimension is a multiple of 14, which matches the kernel size.</p>
<div id="23f448c0" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="0ab4c64b-c704-425d-9cc6-7e6f46b84f6d" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>img_inp, cfg.vision_config.patch_size</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre data-code-line-numbers=""><code>([&lt;PIL.Image.Image image mode=RGB size=644x476&gt;], 14)</code></pre>
</div>
</div>
<div id="27bf1ea5" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="afa50f16-e6ee-4d79-d2ed-70768d1d78d9" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="dv">644</span><span class="op">/</span><span class="dv">14</span>,<span class="dv">476</span><span class="op">/</span><span class="dv">14</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre data-code-line-numbers=""><code>(46.0, 34.0)</code></pre>
</div>
</div>
<div id="5af8bf84" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>vid_inp</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="9ee8346e" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1b04a592-fa6a-47c5-8068-911af8a3070b" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>inps <span class="op">=</span> proc(text<span class="op">=</span>txt_inp, images<span class="op">=</span>img_inp, videos<span class="op">=</span>vid_inp, padding<span class="op">=</span><span class="va">True</span>, return_tensors<span class="op">=</span><span class="st">'pt'</span>).to(<span class="st">'cuda'</span>)<span class="op">;</span> inps</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre data-code-line-numbers=""><code>{'input_ids': tensor([[151644,    872,    198, 151652, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,
         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653,
          74785,    279,   8251,    389,    279,   2115,  15757,  26865,     29,
         151645,    198, 151644,  77091,    198]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1]], device='cuda:0'), 'pixel_values': tensor([[ 0.2515,  0.3099,  0.3391,  ..., -0.6270, -0.3995, -0.4990],
        [ 0.5143,  0.2807,  0.5581,  ..., -0.2857, -0.4137, -0.2573],
        [-0.0113,  0.0909, -0.0842,  ..., -1.0963, -1.0252, -0.9683],
        ...,
        [ 1.7114,  1.6238,  1.6238,  ...,  1.1505,  1.0652,  1.0225],
        [ 1.4486,  1.5800,  1.5216,  ..., -0.3426, -0.2146,  0.2688],
        [ 1.6530,  1.6676,  1.5508,  ..., -1.0678, -0.8545, -0.8830]],
       device='cuda:0'), 'image_grid_thw': tensor([[ 1, 34, 46]], device='cuda:0')}</code></pre>
</div>
</div>
<div id="e0c062ed" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="116c2de6-68f9-4677-e7a4-2db31f6d1293" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>inps.keys()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre data-code-line-numbers=""><code>dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw'])</code></pre>
</div>
</div>
<div id="1074828e" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="47025b06-2895-43bf-dbc8-e57441cb6d4a" data-execution_count="19">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>inps.input_ids.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre data-code-line-numbers=""><code>torch.Size([1, 410])</code></pre>
</div>
</div>
<p>Here I can see the tokens I might need to be aware off when visualizing the attention. If I keep them, it may skew the visualization as a lot of attention may be placed on those tokens.</p>
<p>Most likely, I need to be wary of the very first <code>&lt;|im_start|&gt;</code>, and set its attention to zero when visualizing.</p>
<div id="8c437410" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="7995d97d-7615-4954-dba5-c68980d84e15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>tokz.convert_ids_to_tokens(inps.input_ids[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="6227b11c" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="694e7c92-c957-425b-f699-185ea4e29169">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb31" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>o <span class="op">=</span> vlm.generate(<span class="op">**</span>inps, max_new_tokens<span class="op">=</span><span class="dv">768</span>, do_sample<span class="op">=</span><span class="va">True</span>, temperature<span class="op">=</span><span class="fl">.7</span>, return_dict_in_generate<span class="op">=</span><span class="va">True</span>, output_attentions<span class="op">=</span><span class="va">True</span>)<span class="op">;</span> o</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="a83d7f06" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="9c36707f-e52d-43f3-9693-7aedd3c76196">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>proc.batch_decode(o.sequences, skip_special_tokens<span class="op">=</span><span class="va">False</span>, clean_up_tokenization_spaces<span class="op">=</span><span class="va">False</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Both the input prompt and the generated answer in total have this many tokens (the second dimension in the shape below.)</p>
<div id="c6dae8b4" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1881927e-659e-43c1-cde8-1b5e0765a8d6" data-execution_count="23">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>o.sequences.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre data-code-line-numbers=""><code>torch.Size([1, 437])</code></pre>
</div>
</div>
<div id="b4df7de1" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="127abde8-053a-4730-d42e-5ec3bfd7dd55" data-execution_count="24">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb35" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>trimmed_o <span class="op">=</span> L(out_ids[<span class="bu">len</span>(in_ids):] <span class="cf">for</span> in_ids,out_ids <span class="kw">in</span> <span class="bu">zip</span>(inps.input_ids, o[<span class="st">'sequences'</span>]))</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>proc.batch_decode(trimmed_o, skip_special_tokens<span class="op">=</span><span class="va">False</span>, clean_up_tokenization_spaces<span class="op">=</span><span class="va">False</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre data-code-line-numbers=""><code>['The cat on the left is a tiger-striped cat wearing a green tag. It is laying down comfortably and appears to be relaxed.&lt;|im_end|&gt;']</code></pre>
</div>
</div>
<p>The generated answer itself has this many tokens.</p>
<div id="599ffa7b" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="47403764-5975-427b-d7aa-49cdcf7268e7" data-execution_count="25">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb37" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>trimmed_o[<span class="dv">0</span>].shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre data-code-line-numbers=""><code>torch.Size([27])</code></pre>
</div>
</div>
<p>What I’m going to do next is generate the attention matrix of the model before it generated the first token, by averaging the attention of every layer.</p>
<p>There were so many tokens generated in the answer. Therefore, we have so many attention states.</p>
<div id="57b4feb6" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="e7848f27-2239-4152-8b28-189b27e35462">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb39" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>L(o.attentions)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>For each of the so many tokens generated, I can access the attention state of each of the 28 layers in the model.</p>
<div id="ba56246e" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="d19667c0-0123-479f-b899-ee3ee93fd225">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb40" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>L(o.attentions[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The input prompt had so many tokens (the second dimension).</p>
<div id="f1809814" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="75e190cc-326f-4109-fc4d-6826632b45b0" data-execution_count="28">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb41" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>inps.input_ids.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre data-code-line-numbers=""><code>torch.Size([1, 410])</code></pre>
</div>
</div>
<p>Every layer stores the attention for each of the so many tokens.</p>
<div id="0e7cad19" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="46afd84b-f565-4e34-a540-df2dbc738324" data-execution_count="29">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb43" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>o.attentions[<span class="dv">0</span>][<span class="dv">0</span>].shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre data-code-line-numbers=""><code>torch.Size([1, 16, 410, 410])</code></pre>
</div>
</div>
<div id="4ea924b3" class="cell" data-execution_count="30">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb45" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> suplog(): o.attentions[<span class="dv">0</span>][<span class="dv">28</span>].shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Looking at the output shape, there are two repeating dimensions. 424 corresponds to the input sequence length. However, it appears twice. The reason for this is that attention is all about relationships.</p>
<p>It’s best to think of this as a grid or embedding of sorts. Each token is mapped to every other token.</p>
<p>I’ll take a simple input sequence as an example <code>['&lt;image&gt;', 'the', 'cat']</code>. To calculate the attention of this sequence, the model needs to know how important each token is to each other.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>·</th>
<th><code>&lt;image&gt;</code> (being looked at)</th>
<th><code>the</code> (being looked at)</th>
<th><code>cat</code> (being looked at)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>&lt;image&gt;</code> (looking)</td>
<td>score(<code>image</code>, <code>image</code>)</td>
<td>score(<code>image</code>, <code>the</code>)</td>
<td>score(<code>image</code>, <code>cat</code>)</td>
</tr>
<tr class="even">
<td><code>the</code> (looking)</td>
<td>score(<code>the</code>, <code>image</code>)</td>
<td>score(<code>the</code>, <code>the</code>)</td>
<td>score(<code>the</code>, <code>cat</code>)</td>
</tr>
<tr class="odd">
<td><code>cat</code> (looking)</td>
<td>score(<code>cat</code>, <code>image</code>)</td>
<td>score(<code>cat</code>, <code>the</code>)</td>
<td>score(<code>cat</code>, <code>cat</code>)</td>
</tr>
</tbody>
</table>
<p>Specifically speaking, the first sequence length (the rows) is the query dimension–the token that is doing the looking. The second sequence length (the columns) is the key dimension–the token that is being looked at.</p>
<p>So a layer with shape <code>[1, 28, 424, 424]</code> is a layer with 1 batch, with 28 attention heads, and a 424x424 attention matrix. An entry <code>[i,j]</code> tells us how much attention token <code>i</code> is paying to token <code>j</code>.</p>
<p>When the first token is generated, it gets appended to the input prompt. This appended form is then used to generate the next token.</p>
<p>I can see that the sequence length has indeed been increased by 1 after the first token has been generated.</p>
<div id="469f8905" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="976fd1df-013f-4baa-912c-bc26c99d258b" data-execution_count="31">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb46" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>o.attentions[<span class="dv">1</span>][<span class="dv">0</span>].shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre data-code-line-numbers=""><code>torch.Size([1, 16, 1, 411])</code></pre>
</div>
</div>
<p>However, the last two dimensions are now not the same. Something smart is happening here: computation is being saved.</p>
<p>Let’s say that the last two dimensions remained at 425x425, that would mean a lot of duplicated, redundant computation.</p>
<ul>
<li><strong>Step 0 (Prompt):</strong> compute attention for 424 tokens. Matrix is 424x424.</li>
<li><strong>Step 1 (Generate 1st Token):</strong> The sequence is now 425 tokens long. Recompute attention for all 425 tokens. Matrix would be 425x425.</li>
<li><strong>Step 2 (Generate 2nd Token):</strong> Sequence is 426 tokens. Recalculate attention for all 426. Matrix would be 426x426.</li>
</ul>
<p>The attention for tokens that have already been calculated, are repeatedly being calculated. That’s computationally wasteful. Instead, the model is performing what’s called KV caching. In KV caching, the model knows the attention scores for certain tokens won’t change. They can therefore be cached.</p>
<ol type="1">
<li><strong>Processing the Prompt</strong>
<ul>
<li>The model processes the initial 424 tokens and calculates the query, key, and value vectors for each token.</li>
<li>Full self-attention is performed, where every token’s query is compared against every token’s key.</li>
<li>The key and value vectors for all 424 tokens are then saved to a KV cache</li>
</ul></li>
<li><strong>Generating the First New Token</strong>
<ul>
<li>The query vector for the single new token that is about to be generated is calculated</li>
<li>This single query is compared against the keys of all 425 tokens (424 cached keys + the key for the new token itself)</li>
</ul></li>
</ol>
<p>Only the single row of the attention matrix that is need to predict the next token is calculated.</p>
<p>I’ll begin by calculating the average attention for the prompt, and for the first layer of the model</p>
<div id="50de8f92" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="bfe69314-355e-4651-c9e5-63f986604505" data-execution_count="32">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb48" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> o.attentions[<span class="dv">0</span>][<span class="dv">0</span>]<span class="op">;</span> l.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre data-code-line-numbers=""><code>torch.Size([1, 16, 410, 410])</code></pre>
</div>
</div>
<div id="514e3a96" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1319541c-f12b-452c-f342-c98b166db015" data-execution_count="33">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb50" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>l_attns <span class="op">=</span> l.squeeze(<span class="dv">0</span>)<span class="op">;</span> l_attns.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre data-code-line-numbers=""><code>torch.Size([16, 410, 410])</code></pre>
</div>
</div>
<p>I squeeze because the batch size is 1. Now, I’ll average the attention scores across all heads in this layer to produce a single attention map/grid/matrix.</p>
<div id="0a07237e" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="051e4f71-4761-488b-cbf5-973295c38fd9" data-execution_count="34">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb52" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>avg_attns <span class="op">=</span> l_attns.mean(dim<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span> avg_attns.shape, avg_attns.<span class="bu">min</span>(), avg_attns.<span class="bu">max</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre data-code-line-numbers=""><code>(torch.Size([410, 410]),
 tensor(0., device='cuda:0', dtype=torch.float16),
 tensor(0.9375, device='cuda:0', dtype=torch.float16))</code></pre>
</div>
</div>
<p>In autoregressive models, the attention for token <code>i</code> is used to predict token <code>i+1</code>. The final token in the prompt doesn’t predict a new token within the prompt itself, so we don’t need to visualize its attention.</p>
<div id="c92629c9" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="af3b0f84-5917-49f2-e1a0-a51657660faf" data-execution_count="35">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb54" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>avg_attns[:<span class="op">-</span><span class="dv">1</span>].shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre data-code-line-numbers=""><code>torch.Size([409, 410])</code></pre>
</div>
</div>
<div id="cf23d0e2" class="cell" data-execution_count="36">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb56" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>cur <span class="op">=</span> avg_attns[:<span class="op">-</span><span class="dv">1</span>].cpu().clone()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The first token is typically a special “beginning of sentence” token. Including it can skew the visualization (we don’t want the other tokens to pay attention to it). In this case, it’s <code>&lt;|im_start|&gt;</code>.</p>
<div id="5a1ab2af" class="cell" data-execution_count="37">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb57" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>cur[<span class="dv">1</span>:,<span class="dv">0</span>] <span class="op">=</span> <span class="fl">0.</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Now since some values are 0, the attention scores for each token no longer sum to 1.</p>
<div id="db4e1381" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="71800e5d-60e1-47a8-8de6-78199577c144" data-execution_count="38">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb58" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">2</span>): <span class="bu">print</span>(<span class="ss">f'Token </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>avg_attns[:<span class="op">-</span><span class="dv">1</span>][i,:]<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">}</span><span class="ss">'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre data-code-line-numbers=""><code>Token 0: 1.0
Token 2: 1.0
Token 4: 1.0
Token 6: 1.0
Token 8: 1.0</code></pre>
</div>
</div>
<div id="ede1963a" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="d49f59aa-57ee-4fe7-df38-170cf61f8072" data-execution_count="39">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb60" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">2</span>): <span class="bu">print</span>(<span class="ss">f'Token </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>cur[i,:]<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">}</span><span class="ss">'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre data-code-line-numbers=""><code>Token 0: 1.0
Token 2: 0.85107421875
Token 4: 0.93994140625
Token 6: 0.927734375
Token 8: 0.966796875</code></pre>
</div>
</div>
<p>I’ll renormalize.</p>
<div id="4292cd25" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="381ddac6-ab73-45f8-ec47-03239b74750b" data-execution_count="40">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb62" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>cur[<span class="dv">1</span>:].shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre data-code-line-numbers=""><code>torch.Size([408, 410])</code></pre>
</div>
</div>
<div id="ec1ab485" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="0eaaa6e1-169a-4efe-96d2-074d623d9ebb" data-execution_count="41">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb64" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>cur[<span class="dv">1</span>:].<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>).shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre data-code-line-numbers=""><code>torch.Size([408])</code></pre>
</div>
</div>
<div id="716d0de2" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="adb8c9eb-dc4f-42b6-f59e-5596c3a83ab7" data-execution_count="42">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb66" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>cur[<span class="dv">1</span>:].<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>,keepdim<span class="op">=</span><span class="va">True</span>).shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="42">
<pre data-code-line-numbers=""><code>torch.Size([408, 1])</code></pre>
</div>
</div>
<div id="1cf11a3a" class="cell" data-execution_count="43">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb68" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>cur[<span class="dv">1</span>:] <span class="op">=</span> cur[<span class="dv">1</span>:]<span class="op">/</span>cur[<span class="dv">1</span>:].<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="1420d03c" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="219155cf-2d5c-4cf6-ea26-810f11f4e771" data-execution_count="44">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb69" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">2</span>): <span class="bu">print</span>(<span class="ss">f'Token </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>cur[i,:]<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">}</span><span class="ss">'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre data-code-line-numbers=""><code>Token 0: 1.0
Token 2: 1.0
Token 4: 0.99951171875
Token 6: 0.99951171875
Token 8: 1.0</code></pre>
</div>
</div>
<p>And there I have the aggregated attention map for this layer!</p>
<p>Now I’ll aggregate the attention map for every layer, and then take the average aross all layers.</p>
<div id="b4f4c61c" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="cc2948a9-6a3e-4cc0-f8bf-2eca6466fcea">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb71" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>agg_prompt_attn <span class="op">=</span> L()</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i,l <span class="kw">in</span> <span class="bu">enumerate</span>(o[<span class="st">'attentions'</span>][<span class="dv">0</span>]):</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>  l_attns <span class="op">=</span> l.squeeze(<span class="dv">0</span>)</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>  avg_attns <span class="op">=</span> l_attns.mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>  cur <span class="op">=</span> avg_attns[:<span class="op">-</span><span class="dv">1</span>].cpu().clone()</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>  cur[<span class="dv">1</span>:,<span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>  cur[<span class="dv">1</span>:] <span class="op">=</span> cur[<span class="dv">1</span>:]<span class="op">/</span>cur[<span class="dv">1</span>:].<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>  agg_prompt_attn.append(cur)</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>agg_prompt_attn, agg_prompt_attn[<span class="dv">0</span>].shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="6bb4f1f2" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="ad038ff5-a6dc-4bb7-aed0-c5c64f309dc5" data-execution_count="46">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb72" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch <span class="im">as</span> t</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>agg_prompt_attn <span class="op">=</span> t.stack(<span class="bu">tuple</span>(agg_prompt_attn)).mean(dim<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span> agg_prompt_attn.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre data-code-line-numbers=""><code>torch.Size([409, 410])</code></pre>
</div>
</div>
<p>What I’ve just done is that for each layer, I’ve averaged the attention across the 28 heads. Then I’ve averaged the averages across all layers.</p>
<p>Now I want to aggregate the attentions map for generating only the 1st token, the map used to generate only the 2nd token, and so on. Rather than prompt+1st token, prompt+2nd token, and so on. The main difference here is how the vector is created.</p>
<div id="9d672bab" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="4ea5432a-a89d-4087-aba2-944d56d1479c" data-execution_count="47">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb74" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>avg_attns.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<pre data-code-line-numbers=""><code>torch.Size([410, 410])</code></pre>
</div>
</div>
<div id="abe40e0b" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="2d3f48fa-76b8-4579-8bea-bbc9953ad90c" data-execution_count="48">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb76" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>avg_attns[<span class="op">-</span><span class="dv">1</span>].shape, avg_attns[<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>:].shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="48">
<pre data-code-line-numbers=""><code>(torch.Size([410]), torch.Size([409]))</code></pre>
</div>
</div>
<div id="3f1a89d7" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="bb5605b8-de9d-42d4-dfa3-f8a11dc2f797" data-execution_count="49">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb78" data-source-line-numbers="nil" data-code-line-numbers="2,3,4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> t.concat((</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>  t.tensor([<span class="fl">0.</span>]), </span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>  avg_attns[<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>:].cpu(), </span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>  t.tensor([<span class="fl">0.</span>]) </span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>v.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre data-code-line-numbers=""><code>torch.Size([411])</code></pre>
</div>
</div>
<ol type="1">
<li>The first entry is zero, because that’s null attention.</li>
<li>On first generation, there’s a row for each token in the prompt. So I do [-1] to get the most recent token.</li>
<li>The attention for the last token is set to zero. This token never gets any attention since it doesn’t exist.</li>
</ol>
<div id="a5ebc431" class="cell" data-execution_count="50">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb80" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> aggregrate_llm_attention(attn):</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>  avg <span class="op">=</span> L()</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> l <span class="kw">in</span> attn:</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>    l <span class="op">=</span> l.squeeze(<span class="dv">0</span>)</span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>    avg_attns <span class="op">=</span> l.mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> t.concat((t.tensor([<span class="fl">0.</span>]), avg_attns[<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>:].cpu(), t.tensor([<span class="fl">0.</span>])))</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>    avg.append(v<span class="op">/</span>v.<span class="bu">sum</span>())</span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> t.stack(<span class="bu">tuple</span>(avg)).mean(dim<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="fde07481" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="ae8c9d26-70a3-4eda-b427-3ca643ea711e">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb81" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>L(o.attentions)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="ab91240c" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="3086d5c7-7d95-4bef-b574-666dfdec6ec0">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb82" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>agg_llm_attn <span class="op">=</span> L(<span class="bu">map</span>(aggregrate_llm_attention, o.attentions))<span class="op">;</span> agg_llm_attn</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Now I’ll perform a heterogenous stack on the prompt attention and the response attention. In other words, shorter vectors are padded to equal the length of the longest vector.</p>
<div id="4eb96fa1" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="18718b7c-6517-40d8-b8bd-b727e22fdc30" data-execution_count="53">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb83" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>vecs <span class="op">=</span> [t.tensor([<span class="dv">1</span>])]<span class="op">+</span><span class="bu">list</span>(agg_prompt_attn)<span class="op">+</span><span class="bu">list</span>(agg_llm_attn)</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>max_len <span class="op">=</span> <span class="bu">max</span>(v.shape[<span class="dv">0</span>] <span class="cf">for</span> v <span class="kw">in</span> vecs)<span class="op">;</span> max_len</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="53">
<pre data-code-line-numbers=""><code>437</code></pre>
</div>
</div>
<div id="8161de5b" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="ebdafa05-4c36-44f4-b463-5929e4e6480d">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb85" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(vecs), vecs</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="47dd2d28" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="5e57f93a-00b4-4885-9b6d-c1668eb7fdde" data-execution_count="55">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb86" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>vecs[<span class="dv">1</span>].shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="55">
<pre data-code-line-numbers=""><code>torch.Size([410])</code></pre>
</div>
</div>
<p>Now I’ll turn this into a PyTorch tensor.</p>
<div id="9bac3aa2" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="e223be05-66c3-40bb-e5a3-c30cbc881c98" data-execution_count="56">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb88" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>attn_mtx <span class="op">=</span> t.stack([t.concat((v, t.zeros(max_len<span class="op">-</span>v.shape[<span class="dv">0</span>]))) <span class="cf">for</span> v <span class="kw">in</span> vecs])<span class="op">;</span> attn_mtx.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="56">
<pre data-code-line-numbers=""><code>torch.Size([437, 437])</code></pre>
</div>
</div>
<p>Now it’s time to visualize! A higher gamma factor highlights lower attention values more.</p>
<div id="f69de41f" class="cell" data-execution_count="57">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb90" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>?np.power</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre data-code-line-numbers=""><code>Object `np.power` not found.</code></pre>
</div>
</div>
<div id="0e902904" class="cell" data-execution_count="58">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb92" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>enh_attn_mtx <span class="op">=</span> attn_mtx.<span class="bu">pow</span>(<span class="dv">1</span><span class="op">/</span>(γ<span class="op">:=</span><span class="dv">5</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="21448276" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:670}}" data-outputid="42aa1770-98f7-4c87-cbda-4e3ee45ffbb5" data-execution_count="59">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb93" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>fig,ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">5</span>,<span class="dv">10</span>),dpi<span class="op">=</span><span class="dv">150</span>)</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>ax.imshow(enh_attn_mtx, vmin<span class="op">=</span>enh_attn_mtx.<span class="bu">min</span>(), vmax<span class="op">=</span>enh_attn_mtx.<span class="bu">max</span>(), interpolation<span class="op">=</span><span class="st">'nearest'</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="28_visualising_vlm_attention_files/figure-html/cell-59-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The sharp spike of attention at the beginning of the graph is most likely the system prompt.</p>
<p>What I now want to do is visualize how the total attention on the image itself varied as each token was generated.</p>
<div id="b4b34147" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="d6446c8d-3abe-4b02-800f-00a7152c7828" data-execution_count="60">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb94" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>inp_tok_len <span class="op">=</span> <span class="bu">len</span>(inps.input_ids[<span class="dv">0</span>])<span class="op">;</span> inp_tok_len</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="60">
<pre data-code-line-numbers=""><code>410</code></pre>
</div>
</div>
<div id="82771d83" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="761c9b72-feae-4694-b6d9-895457c8c674">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb96" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>tokz.convert_ids_to_tokens(inps.input_ids[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>I’ll save the indicies of the first and last vision tokens.</p>
<div id="07def1c1" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="045d3a82-5830-49c8-a93c-bbee14540de6" data-execution_count="62">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb97" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>vis_tok_idxs <span class="op">=</span> tokz.convert_ids_to_tokens(inps.input_ids[<span class="dv">0</span>]).index(<span class="st">'&lt;|vision_start|&gt;'</span>), tokz.convert_ids_to_tokens(inps.input_ids[<span class="dv">0</span>]).index(<span class="st">'&lt;|vision_end|&gt;'</span>)<span class="op">;</span> vis_tok_idxs</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="62">
<pre data-code-line-numbers=""><code>(3, 395)</code></pre>
</div>
</div>
<p>I’ll now sum the attentions for each generated token, over all image tokens.</p>
<div id="181766ae" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="b2d4bd57-b084-43c7-fba6-dbf355576ff9" data-execution_count="63">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb99" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>attn_mtx[inp_tok_len:].shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="63">
<pre data-code-line-numbers=""><code>torch.Size([27, 437])</code></pre>
</div>
</div>
<div id="c482d98d" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="50c1c009-e215-4ce7-dbf5-4e3942dcc4d8" data-execution_count="64">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb101" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>attn_mtx[inp_tok_len:][<span class="dv">0</span>].shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="64">
<pre data-code-line-numbers=""><code>torch.Size([437])</code></pre>
</div>
</div>
<div id="f6167a45" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="0f38f296-1e6a-403e-9f0c-49572b578397" data-execution_count="65">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb103" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>attn_mtx[inp_tok_len:][<span class="dv">0</span>][vis_tok_idxs[<span class="dv">0</span>]:vis_tok_idxs[<span class="dv">1</span>]].shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="65">
<pre data-code-line-numbers=""><code>torch.Size([392])</code></pre>
</div>
</div>
<div id="86ec0d9c" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="16f2b79f-63f1-4ef5-81e2-9149dd5db816" data-execution_count="66">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb105" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="co"># vis_tok_attn_w = [r[vis_tok_idxs[0]:vis_tok_idxs[1]].sum().item() for i,(r,t) in enumerate(zip(attn_mtx[inp_tok_len:],o.sequences[0].tolist()))]; vis_tok_attn_w</span></span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>vis_tok_attn_w <span class="op">=</span> [r[vis_tok_idxs[<span class="dv">0</span>]:vis_tok_idxs[<span class="dv">1</span>]].<span class="bu">sum</span>().item() <span class="cf">for</span> r <span class="kw">in</span> attn_mtx[inp_tok_len:]]<span class="op">;</span> vis_tok_attn_w</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="66">
<pre data-code-line-numbers=""><code>[0.1649223268032074,
 0.14924326539039612,
 0.24785716831684113,
 0.15989810228347778,
 0.18078750371932983,
 0.19855275750160217,
 0.2221490442752838,
 0.25423598289489746,
 0.33117902278900146,
 0.2534129023551941,
 0.23635193705558777,
 0.27973249554634094,
 0.2829405665397644,
 0.2884763479232788,
 0.20834580063819885,
 0.13748674094676971,
 0.19547797739505768,
 0.20929457247257233,
 0.2825366258621216,
 0.24532952904701233,
 0.19346649944782257,
 0.1775532066822052,
 0.16503523290157318,
 0.15991057455539703,
 0.19774067401885986,
 0.1700247824192047,
 0.10200894623994827]</code></pre>
</div>
</div>
<p>Now I can visualize the sum of all attentions on all image tokens, as each new token was generated.</p>
<div id="3bdd4972" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:156}}" data-outputid="56afa332-805c-4bae-c3e6-57b76c9416fc" data-execution_count="67">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb107" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>fig,ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">40</span>,<span class="dv">5</span>), dpi<span class="op">=</span><span class="dv">227</span>)</span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a>ax.plot(vis_tok_attn_w)</span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a>ax.set_xticks(<span class="bu">range</span>(<span class="bu">len</span>(vis_tok_attn_w)))</span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels([tokz.decode(t, add_special_tokens<span class="op">=</span><span class="va">False</span>).strip() <span class="cf">for</span> t <span class="kw">in</span> trimmed_o[<span class="dv">0</span>].tolist()], rotation<span class="op">=</span><span class="dv">75</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="28_visualising_vlm_attention_files/figure-html/cell-67-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Next up, I want to visualize the attention weights of the vision encoder.</p>
<div id="cd907a19" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="e4962c8f-d3f0-4563-a7a0-e4787abfb636" data-execution_count="68">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb108" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>vlm.visual.blocks[<span class="dv">0</span>].attn</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="68">
<pre data-code-line-numbers=""><code>Qwen2_5_VLVisionAttention(
  (qkv): Linear(in_features=1280, out_features=3840, bias=True)
  (proj): Linear(in_features=1280, out_features=1280, bias=True)
)</code></pre>
</div>
</div>
<p>The vision encoder uses SPDA attention. This implementation uses C++/CUDA under the hood, so I’ll have to use a manual implementation instead to be able to access the weights.</p>
<div id="20fed762" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:35}}" data-outputid="84b8c06c-f4ea-4da2-8d51-3253a875cbf8" data-execution_count="69">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb110" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a>vlm.visual.config._attn_implementation</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="69">
<pre data-code-line-numbers=""><code>'eager'</code></pre>
</div>
</div>
<div id="9d9e5dcc" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:204}}" data-outputid="b0a39962-616b-4229-b8a8-9a822594b14c" data-execution_count="70">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb112" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>vlm.visual.named_modules</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="70">
<pre data-code-line-numbers=""><code>&lt;bound method Module.named_modules of Qwen2_5_VisionTransformerPretrainedModel(
  (patch_embed): Qwen2_5_VisionPatchEmbed(
    (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
  )
  (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()
  (blocks): ModuleList(
    (0-31): 32 x Qwen2_5_VLVisionBlock(
      (norm1): Qwen2RMSNorm((1280,), eps=1e-06)
      (norm2): Qwen2RMSNorm((1280,), eps=1e-06)
      (attn): Qwen2_5_VLVisionAttention(
        (qkv): Linear(in_features=1280, out_features=3840, bias=True)
        (proj): Linear(in_features=1280, out_features=1280, bias=True)
      )
      (mlp): Qwen2_5_VLMLP(
        (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)
        (up_proj): Linear(in_features=1280, out_features=3420, bias=True)
        (down_proj): Linear(in_features=3420, out_features=1280, bias=True)
        (act_fn): SiLU()
      )
    )
  )
  (merger): Qwen2_5_VLPatchMerger(
    (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)
    (mlp): Sequential(
      (0): Linear(in_features=5120, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=5120, out_features=2048, bias=True)
    )
  )
)&gt;</code></pre>
</div>
</div>
<div id="6470a7b8" class="cell" data-execution_count="71">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb114" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> suplog():</span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">with</span> t.no_grad(): x <span class="op">=</span> vlm.visual(inps.pixel_values, inps.image_grid_thw, output_attentions<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre data-code-line-numbers=""><code>Traceback (most recent call last):
  File "/tmp/ipykernel_265136/256214625.py", line 5, in suplog
    try: yield
  File "/tmp/ipykernel_265136/862200727.py", line 2, in &lt;module&gt;
    with t.no_grad(): x = vlm.visual(inps.pixel_values, inps.image_grid_thw, output_attentions=True)
  File "/home/data/Salman/miniforge3/envs/robobrain2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/data/Salman/miniforge3/envs/robobrain2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: Qwen2_5_VisionTransformerPretrainedModel.forward() got an unexpected keyword argument 'output_attentions'</code></pre>
</div>
</div>
<p>The <code>output_attentions</code> parameter doesn’t exist.</p>
<div id="0227cd52" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:204}}" data-outputid="7b34debd-dc51-43d8-de8c-38195e4764e1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb116" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a>vlm.visual.named_modules</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="4c182cf5" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="63b735a8-89ee-4ece-f95f-1c72070e7223" data-execution_count="73">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb117" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a>vlm.visual.blocks[<span class="dv">0</span>].attn??</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">Signature:</span>       vlm<span class="ansi-blue-fg">.</span>visual<span class="ansi-blue-fg">.</span>blocks<span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">.</span>attn<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>args<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">Type:</span>            Qwen2_5_VLVisionAttention

<span class="ansi-red-fg">String form:</span>    

Qwen2_5_VLVisionAttention(

  (qkv): Linear(in_features=1280, out_features=3840, bias=True)

  (proj): Linear(in_features=1280, out_features=1280, bias=True)

)

<span class="ansi-red-fg">File:</span>            ~/miniforge3/envs/robobrain2/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py

<span class="ansi-red-fg">Source:</span>         

<span class="ansi-green-fg">class</span> Qwen2_5_VLVisionAttention<span class="ansi-blue-fg">(</span>nn<span class="ansi-blue-fg">.</span>Module<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

    <span class="ansi-green-fg">def</span> __init__<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> dim<span class="ansi-blue-fg">:</span> int<span class="ansi-blue-fg">,</span> num_heads<span class="ansi-blue-fg">:</span> int <span class="ansi-blue-fg">=</span> <span class="ansi-cyan-fg">16</span><span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">-&gt;</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>

        super<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>__init__<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

        self<span class="ansi-blue-fg">.</span>num_heads <span class="ansi-blue-fg">=</span> num_heads

        self<span class="ansi-blue-fg">.</span>head_dim <span class="ansi-blue-fg">=</span> dim <span class="ansi-blue-fg">//</span> num_heads

        self<span class="ansi-blue-fg">.</span>qkv <span class="ansi-blue-fg">=</span> nn<span class="ansi-blue-fg">.</span>Linear<span class="ansi-blue-fg">(</span>dim<span class="ansi-blue-fg">,</span> dim <span class="ansi-blue-fg">*</span> <span class="ansi-cyan-fg">3</span><span class="ansi-blue-fg">,</span> bias<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">True</span><span class="ansi-blue-fg">)</span>

        self<span class="ansi-blue-fg">.</span>proj <span class="ansi-blue-fg">=</span> nn<span class="ansi-blue-fg">.</span>Linear<span class="ansi-blue-fg">(</span>dim<span class="ansi-blue-fg">,</span> dim<span class="ansi-blue-fg">)</span>



    <span class="ansi-green-fg">def</span> forward<span class="ansi-blue-fg">(</span>

        self<span class="ansi-blue-fg">,</span>

        hidden_states<span class="ansi-blue-fg">:</span> torch<span class="ansi-blue-fg">.</span>Tensor<span class="ansi-blue-fg">,</span>

        cu_seqlens<span class="ansi-blue-fg">:</span> torch<span class="ansi-blue-fg">.</span>Tensor<span class="ansi-blue-fg">,</span>

        rotary_pos_emb<span class="ansi-blue-fg">:</span> Optional<span class="ansi-blue-fg">[</span>torch<span class="ansi-blue-fg">.</span>Tensor<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>

        position_embeddings<span class="ansi-blue-fg">:</span> Optional<span class="ansi-blue-fg">[</span>Tuple<span class="ansi-blue-fg">[</span>torch<span class="ansi-blue-fg">.</span>Tensor<span class="ansi-blue-fg">,</span> torch<span class="ansi-blue-fg">.</span>Tensor<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>

    <span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">-&gt;</span> torch<span class="ansi-blue-fg">.</span>Tensor<span class="ansi-blue-fg">:</span>

        seq_length <span class="ansi-blue-fg">=</span> hidden_states<span class="ansi-blue-fg">.</span>shape<span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">]</span>

        q<span class="ansi-blue-fg">,</span> k<span class="ansi-blue-fg">,</span> v <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>qkv<span class="ansi-blue-fg">(</span>hidden_states<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>reshape<span class="ansi-blue-fg">(</span>seq_length<span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">3</span><span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>num_heads<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>permute<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">3</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>unbind<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">)</span>

        <span class="ansi-green-fg">if</span> position_embeddings <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>

            logger<span class="ansi-blue-fg">.</span>warning_once<span class="ansi-blue-fg">(</span>

                <span class="ansi-blue-fg">"The attention layers in this model are transitioning from computing the RoPE embeddings internally "</span>

                <span class="ansi-blue-fg">"through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed "</span>

                <span class="ansi-blue-fg">"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be "</span>

                <span class="ansi-blue-fg">"removed and `position_embeddings` will be mandatory."</span>

            <span class="ansi-blue-fg">)</span>

            emb <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>cat<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">(</span>rotary_pos_emb<span class="ansi-blue-fg">,</span> rotary_pos_emb<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> dim<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span>

            cos <span class="ansi-blue-fg">=</span> emb<span class="ansi-blue-fg">.</span>cos<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

            sin <span class="ansi-blue-fg">=</span> emb<span class="ansi-blue-fg">.</span>sin<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

        <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>

            cos<span class="ansi-blue-fg">,</span> sin <span class="ansi-blue-fg">=</span> position_embeddings

        q<span class="ansi-blue-fg">,</span> k <span class="ansi-blue-fg">=</span> apply_rotary_pos_emb_vision<span class="ansi-blue-fg">(</span>q<span class="ansi-blue-fg">,</span> k<span class="ansi-blue-fg">,</span> cos<span class="ansi-blue-fg">,</span> sin<span class="ansi-blue-fg">)</span>



        attention_mask <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>full<span class="ansi-blue-fg">(</span>

            <span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> seq_length<span class="ansi-blue-fg">,</span> seq_length<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">,</span> torch<span class="ansi-blue-fg">.</span>finfo<span class="ansi-blue-fg">(</span>q<span class="ansi-blue-fg">.</span>dtype<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>min<span class="ansi-blue-fg">,</span> device<span class="ansi-blue-fg">=</span>q<span class="ansi-blue-fg">.</span>device<span class="ansi-blue-fg">,</span> dtype<span class="ansi-blue-fg">=</span>q<span class="ansi-blue-fg">.</span>dtype

        <span class="ansi-blue-fg">)</span>

        <span class="ansi-green-fg">for</span> i <span class="ansi-green-fg">in</span> range<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> len<span class="ansi-blue-fg">(</span>cu_seqlens<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

            attention_mask<span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">...</span><span class="ansi-blue-fg">,</span> cu_seqlens<span class="ansi-blue-fg">[</span>i <span class="ansi-blue-fg">-</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">:</span> cu_seqlens<span class="ansi-blue-fg">[</span>i<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">,</span> cu_seqlens<span class="ansi-blue-fg">[</span>i <span class="ansi-blue-fg">-</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">:</span> cu_seqlens<span class="ansi-blue-fg">[</span>i<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> <span class="ansi-cyan-fg">0</span>



        q <span class="ansi-blue-fg">=</span> q<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span>

        k <span class="ansi-blue-fg">=</span> k<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span>

        v <span class="ansi-blue-fg">=</span> v<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span>

        attn_weights <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>q<span class="ansi-blue-fg">,</span> k<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">/</span> math<span class="ansi-blue-fg">.</span>sqrt<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>head_dim<span class="ansi-blue-fg">)</span>

        attn_weights <span class="ansi-blue-fg">=</span> attn_weights <span class="ansi-blue-fg">+</span> attention_mask

        attn_weights <span class="ansi-blue-fg">=</span> nn<span class="ansi-blue-fg">.</span>functional<span class="ansi-blue-fg">.</span>softmax<span class="ansi-blue-fg">(</span>attn_weights<span class="ansi-blue-fg">,</span> dim<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span> dtype<span class="ansi-blue-fg">=</span>torch<span class="ansi-blue-fg">.</span>float32<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>to<span class="ansi-blue-fg">(</span>q<span class="ansi-blue-fg">.</span>dtype<span class="ansi-blue-fg">)</span>

        attn_output <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>matmul<span class="ansi-blue-fg">(</span>attn_weights<span class="ansi-blue-fg">,</span> v<span class="ansi-blue-fg">)</span>

        attn_output <span class="ansi-blue-fg">=</span> attn_output<span class="ansi-blue-fg">.</span>transpose<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span>

        attn_output <span class="ansi-blue-fg">=</span> attn_output<span class="ansi-blue-fg">.</span>reshape<span class="ansi-blue-fg">(</span>seq_length<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span>

        attn_output <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>proj<span class="ansi-blue-fg">(</span>attn_output<span class="ansi-blue-fg">)</span>

        <span class="ansi-green-fg">return</span> attn_output

<span class="ansi-red-fg">Class docstring:</span>

Base class for all neural network modules.



Your models should also subclass this class.



Modules can also contain other Modules, allowing to nest them in

a tree structure. You can assign the submodules as regular attributes::



    import torch.nn as nn

    import torch.nn.functional as F



    class Model(nn.Module):

        def __init__(self) -&gt; None:

            super().__init__()

            self.conv1 = nn.Conv2d(1, 20, 5)

            self.conv2 = nn.Conv2d(20, 20, 5)



        def forward(self, x):

            x = F.relu(self.conv1(x))

            return F.relu(self.conv2(x))



Submodules assigned in this way will be registered, and will have their

parameters converted too when you call :meth:`to`, etc.



.. note::

    As per the example above, an ``__init__()`` call to the parent class

    must be made before assignment on the child.



:ivar training: Boolean represents whether this module is in training or

                evaluation mode.

:vartype training: bool

<span class="ansi-red-fg">Init docstring:</span>  Initialize internal Module state, shared by both nn.Module and ScriptModule.</pre>
</div>
</div>
</div>
<div id="7c79840f" class="cell" data-execution_count="74">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb118" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a>vlm.visual.blocks[<span class="dv">0</span>].attn.<span class="op">*</span>?</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre data-code-line-numbers=""><code>`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre data-code-line-numbers=""><code></code></pre>
</div>
</div>
<p>Attaching a forward hook to this module will probably be the most straightforward method.</p>
<div id="470a3381" class="cell" data-execution_count="75">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb121" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a>attn_ws <span class="op">=</span> []</span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_attn_hook(mod, inp, out):</span>
<span id="cb121-3"><a href="#cb121-3" aria-hidden="true" tabindex="-1"></a>  attn_ws.append(out)</span>
<span id="cb121-4"><a href="#cb121-4" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb121-5"><a href="#cb121-5" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f"MODULE: </span><span class="sc">{</span><span class="bu">type</span>(mod)<span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb121-6"><a href="#cb121-6" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f"Module attrs: </span><span class="sc">{</span>[attr <span class="cf">for</span> attr <span class="kw">in</span> <span class="bu">dir</span>(mod) <span class="cf">if</span> <span class="kw">not</span> attr.startswith(<span class="st">'_'</span>)]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb121-7"><a href="#cb121-7" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"-"</span><span class="op">*</span><span class="dv">30</span>)</span>
<span id="cb121-8"><a href="#cb121-8" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f"INPUT: </span><span class="sc">{</span><span class="bu">type</span>(inp)<span class="sc">}</span><span class="ss"> | Shape: </span><span class="sc">{</span>inp[<span class="dv">0</span>]<span class="sc">.</span>shape <span class="cf">if</span> <span class="bu">hasattr</span>(inp[<span class="dv">0</span>], <span class="st">'shape'</span>) <span class="cf">else</span> <span class="st">'N/A'</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb121-9"><a href="#cb121-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="bu">hasattr</span>(inp, <span class="st">'__len__'</span>) <span class="kw">and</span> <span class="bu">len</span>(inp) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb121-10"><a href="#cb121-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Input[0] attrs: </span><span class="sc">{</span>[attr <span class="cf">for</span> attr <span class="kw">in</span> <span class="bu">dir</span>(inp[<span class="dv">0</span>]) <span class="cf">if</span> <span class="kw">not</span> attr.startswith(<span class="st">'_'</span>)]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb121-11"><a href="#cb121-11" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"-"</span><span class="op">*</span><span class="dv">30</span>)</span>
<span id="cb121-12"><a href="#cb121-12" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f"OUTPUT: </span><span class="sc">{</span><span class="bu">type</span>(out)<span class="sc">}</span><span class="ss"> | Shape: </span><span class="sc">{</span>out<span class="sc">.</span>shape <span class="cf">if</span> <span class="bu">hasattr</span>(out, <span class="st">'shape'</span>) <span class="cf">else</span> <span class="st">'N/A'</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb121-13"><a href="#cb121-13" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f"Output attrs: </span><span class="sc">{</span>[attr <span class="cf">for</span> attr <span class="kw">in</span> <span class="bu">dir</span>(out) <span class="cf">if</span> <span class="kw">not</span> attr.startswith(<span class="st">'_'</span>)]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb121-14"><a href="#cb121-14" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"*"</span><span class="op">*</span><span class="dv">30</span> <span class="op">+</span> <span class="st">" CHILDREN "</span> <span class="op">+</span> <span class="st">"*"</span><span class="op">*</span><span class="dv">30</span>)</span>
<span id="cb121-15"><a href="#cb121-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> name, child <span class="kw">in</span> mod.named_children():</span>
<span id="cb121-16"><a href="#cb121-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  └── </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="bu">type</span>(child)<span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb121-17"><a href="#cb121-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> subname, subchild <span class="kw">in</span> child.named_children():</span>
<span id="cb121-18"><a href="#cb121-18" aria-hidden="true" tabindex="-1"></a>      <span class="bu">print</span>(<span class="ss">f"      └── </span><span class="sc">{</span>subname<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="bu">type</span>(subchild)<span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb121-19"><a href="#cb121-19" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="5392be32" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="f3184e39-55de-4874-b559-51e4a2aa7769">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb122" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n,m <span class="kw">in</span> vlm.visual.named_modules():</span>
<span id="cb122-2"><a href="#cb122-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> n.endswith(<span class="st">'.attn'</span>):</span>
<span id="cb122-3"><a href="#cb122-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'attaching hook to </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb122-4"><a href="#cb122-4" aria-hidden="true" tabindex="-1"></a>    m.register_forward_hook(get_attn_hook)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="e2b8a070" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="6e77a9db-aba0-4c4a-dc6b-f022813be93a">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb123" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n,m <span class="kw">in</span> vlm.visual.named_modules():</span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> m._forward_hooks: <span class="bu">print</span>(<span class="ss">f'Forward hooks found on module: </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss"> -&gt; </span><span class="sc">{</span>m<span class="sc">.</span>_forward_hooks<span class="sc">}</span><span class="ss">'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="9c5a98c8" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="cde63da0-95bd-4987-f009-2042f3cc2ba1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb124" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> t.no_grad(): x <span class="op">=</span> vlm.visual(inps.pixel_values, inps.image_grid_thw)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The output is a 1564x1280 tensor. There are (644/14)·(476/14)=46·34=1564 patches. The reason why this doesn’t match the number of image tokens produced by the tokenizer is that these 1564 patches haven’t been passed through the projection layer yet.</p>
<p>The Qwen 2.5 VL projection layer combines patches into 2x2 blocks. This will result in, in this case, (46/2)·(34/2)=392. This matches the number of image tokens.</p>
<div id="47f459d7" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="7bdc59b5-6027-4510-b1f0-24fe67241e52" data-execution_count="79">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb125" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(attn_ws), attn_ws[<span class="dv">0</span>].shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="79">
<pre data-code-line-numbers=""><code>(32, torch.Size([1564, 1280]))</code></pre>
</div>
</div>
<p>Adding a hook doesn’t work as I only get the output feature vectors back, and not the intermediate attention weights. I need to monkey patch the forward method so it captures the weights.</p>
<div id="e1ac2c59" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb127" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a>vlm.visual??</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>It seems I’ll need to monkey patch something within the blocks themselves.</p>
<div id="eb6e5251" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="90ee1785-1ab3-471d-9220-e94c1f8d0491">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb128" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a>vlm.visual.blocks[<span class="dv">0</span>].attn??</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>I’ll need to monkey patch the forward method of the attention class.</p>
<div id="52503c96" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="4f30bdba-ea39-472e-8c31-3e13350541a4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb129" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a>vlm.visual.blocks[<span class="dv">0</span>].attn.forward??</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="905f474d" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb130" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers.models.qwen2_5_vl.modeling_qwen2_5_vl <span class="im">import</span> Qwen2_5_VLVisionAttention, apply_rotary_pos_emb_vision</span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb130-3"><a href="#cb130-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb130-4"><a href="#cb130-4" aria-hidden="true" tabindex="-1"></a>??Qwen2_5_VLVisionAttention</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="d2cc65a1" class="cell" data-execution_count="84">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb131" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a>attn_ws <span class="op">=</span> []</span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a><span class="at">@patch</span></span>
<span id="cb131-3"><a href="#cb131-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>: Qwen2_5_VLVisionAttention, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings):</span>
<span id="cb131-4"><a href="#cb131-4" aria-hidden="true" tabindex="-1"></a>  seq_length <span class="op">=</span> hidden_states.shape[<span class="dv">0</span>]</span>
<span id="cb131-5"><a href="#cb131-5" aria-hidden="true" tabindex="-1"></a>  q, k, v <span class="op">=</span> <span class="va">self</span>.qkv(hidden_states).reshape(seq_length, <span class="dv">3</span>, <span class="va">self</span>.num_heads, <span class="op">-</span><span class="dv">1</span>).permute(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>).unbind(<span class="dv">0</span>)</span>
<span id="cb131-6"><a href="#cb131-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> position_embeddings <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb131-7"><a href="#cb131-7" aria-hidden="true" tabindex="-1"></a>    logger.warning_once(</span>
<span id="cb131-8"><a href="#cb131-8" aria-hidden="true" tabindex="-1"></a>      <span class="st">"The attention layers in this model are transitioning from computing the RoPE embeddings internally "</span></span>
<span id="cb131-9"><a href="#cb131-9" aria-hidden="true" tabindex="-1"></a>      <span class="st">"through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed "</span></span>
<span id="cb131-10"><a href="#cb131-10" aria-hidden="true" tabindex="-1"></a>      <span class="st">"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be "</span></span>
<span id="cb131-11"><a href="#cb131-11" aria-hidden="true" tabindex="-1"></a>      <span class="st">"removed and `position_embeddings` will be mandatory."</span></span>
<span id="cb131-12"><a href="#cb131-12" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb131-13"><a href="#cb131-13" aria-hidden="true" tabindex="-1"></a>    emb <span class="op">=</span> torch.cat((rotary_pos_emb, rotary_pos_emb), dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb131-14"><a href="#cb131-14" aria-hidden="true" tabindex="-1"></a>    cos <span class="op">=</span> emb.cos()</span>
<span id="cb131-15"><a href="#cb131-15" aria-hidden="true" tabindex="-1"></a>    sin <span class="op">=</span> emb.sin()</span>
<span id="cb131-16"><a href="#cb131-16" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb131-17"><a href="#cb131-17" aria-hidden="true" tabindex="-1"></a>    cos, sin <span class="op">=</span> position_embeddings</span>
<span id="cb131-18"><a href="#cb131-18" aria-hidden="true" tabindex="-1"></a>  q, k <span class="op">=</span> apply_rotary_pos_emb_vision(q, k, cos, sin)</span>
<span id="cb131-19"><a href="#cb131-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-20"><a href="#cb131-20" aria-hidden="true" tabindex="-1"></a>  attention_mask <span class="op">=</span> torch.full(</span>
<span id="cb131-21"><a href="#cb131-21" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, seq_length, seq_length], torch.finfo(q.dtype).<span class="bu">min</span>, device<span class="op">=</span>q.device, dtype<span class="op">=</span>q.dtype</span>
<span id="cb131-22"><a href="#cb131-22" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb131-23"><a href="#cb131-23" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(cu_seqlens)):</span>
<span id="cb131-24"><a href="#cb131-24" aria-hidden="true" tabindex="-1"></a>    attention_mask[..., cu_seqlens[i <span class="op">-</span> <span class="dv">1</span>] : cu_seqlens[i], cu_seqlens[i <span class="op">-</span> <span class="dv">1</span>] : cu_seqlens[i]] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb131-25"><a href="#cb131-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-26"><a href="#cb131-26" aria-hidden="true" tabindex="-1"></a>  q <span class="op">=</span> q.transpose(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb131-27"><a href="#cb131-27" aria-hidden="true" tabindex="-1"></a>  k <span class="op">=</span> k.transpose(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb131-28"><a href="#cb131-28" aria-hidden="true" tabindex="-1"></a>  v <span class="op">=</span> v.transpose(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb131-29"><a href="#cb131-29" aria-hidden="true" tabindex="-1"></a>  attn_weights <span class="op">=</span> torch.matmul(q, k.transpose(<span class="dv">1</span>, <span class="dv">2</span>)) <span class="op">/</span> math.sqrt(<span class="va">self</span>.head_dim)</span>
<span id="cb131-30"><a href="#cb131-30" aria-hidden="true" tabindex="-1"></a>  attn_weights <span class="op">=</span> attn_weights <span class="op">+</span> attention_mask</span>
<span id="cb131-31"><a href="#cb131-31" aria-hidden="true" tabindex="-1"></a>  attn_weights <span class="op">=</span> nn.functional.softmax(attn_weights, dim<span class="op">=-</span><span class="dv">1</span>, dtype<span class="op">=</span>torch.float32).to(q.dtype)</span>
<span id="cb131-32"><a href="#cb131-32" aria-hidden="true" tabindex="-1"></a>  attn_ws.append(attn_weights) <span class="co">#&lt;&lt; Addition</span></span>
<span id="cb131-33"><a href="#cb131-33" aria-hidden="true" tabindex="-1"></a>  attn_output <span class="op">=</span> torch.matmul(attn_weights, v)</span>
<span id="cb131-34"><a href="#cb131-34" aria-hidden="true" tabindex="-1"></a>  attn_output <span class="op">=</span> attn_output.transpose(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb131-35"><a href="#cb131-35" aria-hidden="true" tabindex="-1"></a>  attn_output <span class="op">=</span> attn_output.reshape(seq_length, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb131-36"><a href="#cb131-36" aria-hidden="true" tabindex="-1"></a>  attn_output <span class="op">=</span> <span class="va">self</span>.proj(attn_output)</span>
<span id="cb131-37"><a href="#cb131-37" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> attn_output</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Confirming patch has be introduced.</p>
<div id="5fb64c53" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="9854f812-5914-48ca-8d8d-0390b7564db8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb132" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a>??vlm.visual.blocks[<span class="dv">0</span>].attn</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="bb74cc30" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="fd0a9608-d8a7-4ce0-82e4-0a1fdfc7a40f">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb133" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a>vlm.visual.blocks[<span class="dv">0</span>].attn.forward??</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Let’s give it a run. But first, remove the existing hooks.</p>
<div id="42b3ec87" class="cell" data-execution_count="87">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb134" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n,m <span class="kw">in</span> vlm.visual.named_modules():</span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="bu">hasattr</span>(m, <span class="st">'_forward_hooks'</span>) <span class="kw">and</span> m._forward_hooks: m._forward_hooks.clear()</span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true" tabindex="-1"></a>attn_ws <span class="op">=</span> []</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="ea73c789" class="cell" data-execution_count="88">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb135" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb135-1"><a href="#cb135-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> t.no_grad(): x <span class="op">=</span> vlm.visual(inps.pixel_values, inps.image_grid_thw)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="d87a5967" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="4d0a2499-803b-433c-9335-fc7cf228409e" data-execution_count="89">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb136" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(attn_ws), attn_ws[<span class="dv">0</span>].shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="89">
<pre data-code-line-numbers=""><code>(32, torch.Size([16, 1564, 1564]))</code></pre>
</div>
</div>
<p>It’s worked! The reason why the attention weights is a different shape is that the patches/feature vectors to which the attention weights correspond to haven’t pass through what’s known as the projection/pooling layer.</p>
<p>The RoboBrain model is based on Qwen 2.5 VL. The vision encoder in this model uses a kernel size of 14.</p>
<div id="bc0c3851" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="ede88be9-4299-496e-e3f9-987ae04693d6" data-execution_count="90">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb138" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a>vlm.visual.config.patch_size</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="90">
<pre data-code-line-numbers=""><code>14</code></pre>
</div>
</div>
<p>The image at the beginning of the notebook has dimensions, after padding, 644x476.</p>
<div id="76788479" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1eb10f33-3b0c-44e4-ab61-50d184fffcf7" data-execution_count="91">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb140" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a>img_inp[<span class="dv">0</span>].size</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="91">
<pre data-code-line-numbers=""><code>(644, 476)</code></pre>
</div>
</div>
<p>Therefore, the number of patches produced is (644/14)·(476/14)=46·34=1564. After the vision encoder produces the feature vector for each of these 1564 patches, the vectors are passed through the projection/pooling layer. In the Qwen 2.5 VL model, each 2x2 patch is combined, which results in (46/2)·(34/2)=392 patches. This matches the number of image tokens produced by the tokenizer.</p>
<p>I will now aggregate all the attention weights.</p>
<div id="f82d3494" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="07557be3-1fa0-49b1-d9da-2dc4f19df39d" data-execution_count="92">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb142" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb142-1"><a href="#cb142-1" aria-hidden="true" tabindex="-1"></a>attns_per_head <span class="op">=</span> l.mean(dim<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span> attns_per_head.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="92">
<pre data-code-line-numbers=""><code>torch.Size([16, 410, 410])</code></pre>
</div>
</div>
<p>In the reference notebook, a normalization step occurs after taking the mean across all heads. This is because in the LLaVa implementation, the first token is a special token which doens’t need to be visualized. Here, I do not need to perform such processes, since far as I’m aware, there is no special patch token.</p>
<p>If there were, I would have to perform the following.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb144" data-code-line-numbers=""><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a>vec <span class="op">=</span> attns_per_head[<span class="dv">1</span>:,<span class="dv">1</span>:].cpu()</span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a>vec<span class="op">/</span>vec.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>,keepdim<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div id="407c4e5b" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="31e02f95-7b4f-451b-a50e-fda394bb2ae0" data-execution_count="93">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb145" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb145-1"><a href="#cb145-1" aria-hidden="true" tabindex="-1"></a>vis_attn_mtx <span class="op">=</span> t.stack([l.mean(dim<span class="op">=</span><span class="dv">0</span>).cpu() <span class="cf">for</span> l <span class="kw">in</span> attn_ws]).mean(dim<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span> vis_attn_mtx.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="93">
<pre data-code-line-numbers=""><code>torch.Size([1564, 1564])</code></pre>
</div>
</div>
<p>Time to visualize. I’ll begin by visualizing the attention of only the first token.</p>
<div id="2ab22fae" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="a99e4d27-44fa-4906-eb0d-75015b0b4ac5" data-execution_count="94">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb147" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb147-1"><a href="#cb147-1" aria-hidden="true" tabindex="-1"></a>vis_attn_mtx.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="94">
<pre data-code-line-numbers=""><code>torch.Size([1564, 1564])</code></pre>
</div>
</div>
<div id="Gz7-cES6HHML" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="36c88b5a-d085-4735-8e97-7852e7798ae2" data-execution_count="95">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb149" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb149-1"><a href="#cb149-1" aria-hidden="true" tabindex="-1"></a>out_tok_len <span class="op">=</span> <span class="bu">len</span>(o.sequences[<span class="dv">0</span>])<span class="op">;</span> out_tok_len</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="95">
<pre data-code-line-numbers=""><code>437</code></pre>
</div>
</div>
<div id="044b5825" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="99c01bfc-3552-41aa-b723-982d825f3ff0" data-execution_count="96">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb151" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb151-1"><a href="#cb151-1" aria-hidden="true" tabindex="-1"></a>out_tok_idxs <span class="op">=</span> L(<span class="bu">range</span>(inp_tok_len, out_tok_len))<span class="op">;</span> out_tok_idxs</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="96">
<pre data-code-line-numbers=""><code>(#27) [410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429...]</code></pre>
</div>
</div>
<div id="aWVScaGHTUaH" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="3064402e-5a12-4493-9495-e982677bd821" data-execution_count="97">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb153" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb153-1"><a href="#cb153-1" aria-hidden="true" tabindex="-1"></a>out_tok_len <span class="op">=</span> <span class="bu">len</span>(out_tok_idxs)<span class="op">;</span> out_tok_len</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="97">
<pre data-code-line-numbers=""><code>27</code></pre>
</div>
</div>
<p>The target token is currently the first generated token.</p>
<div id="fb21547b" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="eb7f94cb-37a1-4de9-aaf1-93937ed42f2d" data-execution_count="98">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb155" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb155-1"><a href="#cb155-1" aria-hidden="true" tabindex="-1"></a>target_tok_idx <span class="op">=</span> out_tok_idxs[<span class="dv">0</span>]<span class="op">;</span> target_tok_idx</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="98">
<pre data-code-line-numbers=""><code>410</code></pre>
</div>
</div>
<div id="1c60dd68" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="57c9f321-f3b5-47c7-f3e3-aeee82215a29" data-execution_count="99">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb157" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb157-1"><a href="#cb157-1" aria-hidden="true" tabindex="-1"></a>attn_mtx.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="99">
<pre data-code-line-numbers=""><code>torch.Size([437, 437])</code></pre>
</div>
</div>
<div id="e6caef2d" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="c4ac35cf-2235-41d9-ec0b-a48294ee03ea" data-execution_count="100">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb159" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb159-1"><a href="#cb159-1" aria-hidden="true" tabindex="-1"></a>attn_mtx[target_tok_idx].shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="100">
<pre data-code-line-numbers=""><code>torch.Size([437])</code></pre>
</div>
</div>
<div id="99d5d9c7" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="19b9b4a0-a8b7-4f67-baa2-073e8204d847" data-execution_count="101">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb161" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb161-1"><a href="#cb161-1" aria-hidden="true" tabindex="-1"></a>vis_tok_idxs, attn_mtx[target_tok_idx][vis_tok_idxs[<span class="dv">0</span>]:vis_tok_idxs[<span class="dv">1</span>]].shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="101">
<pre data-code-line-numbers=""><code>((3, 395), torch.Size([392]))</code></pre>
</div>
</div>
<p>I’ll obtain the attention weights over the vision tokens, and then normalize those weights.</p>
<div id="4d8dee05" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="17c466f1-7f48-42f8-ec6a-196a7414b73a" data-execution_count="102">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb163" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb163-1"><a href="#cb163-1" aria-hidden="true" tabindex="-1"></a>attn_ws_over_vis_toks <span class="op">=</span> attn_mtx[target_tok_idx][vis_tok_idxs[<span class="dv">0</span>]:vis_tok_idxs[<span class="dv">1</span>]]</span>
<span id="cb163-2"><a href="#cb163-2" aria-hidden="true" tabindex="-1"></a>attn_ws_over_vis_toks.<span class="bu">min</span>(), attn_ws_over_vis_toks.<span class="bu">max</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="102">
<pre data-code-line-numbers=""><code>(tensor(3.0008e-05), tensor(0.0208))</code></pre>
</div>
</div>
<div id="6ef6e63e" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="58f9a099-205e-45c6-8e74-b2bdc3a7432a" data-execution_count="103">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb165" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb165-1"><a href="#cb165-1" aria-hidden="true" tabindex="-1"></a>attn_ws_over_vis_toks <span class="op">=</span> attn_ws_over_vis_toks <span class="op">/</span> attn_ws_over_vis_toks.<span class="bu">sum</span>()</span>
<span id="cb165-2"><a href="#cb165-2" aria-hidden="true" tabindex="-1"></a>attn_ws_over_vis_toks.<span class="bu">min</span>(), attn_ws_over_vis_toks.<span class="bu">max</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="103">
<pre data-code-line-numbers=""><code>(tensor(0.0002), tensor(0.1263))</code></pre>
</div>
</div>
<div id="4f38e36c" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="deb07eed-81f8-4a15-eccf-e324ca263c45" data-execution_count="104">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb167" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb167-1"><a href="#cb167-1" aria-hidden="true" tabindex="-1"></a>attn_ws_over_vis_toks.shape, vis_attn_mtx.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="104">
<pre data-code-line-numbers=""><code>(torch.Size([392]), torch.Size([1564, 1564]))</code></pre>
</div>
</div>
<div id="bf14f4fc" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="5fc3db80-27d2-44e0-85ea-f68e39bd9ff1" data-execution_count="105">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb169" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb169-1"><a href="#cb169-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> L(<span class="bu">zip</span>(attn_ws_over_vis_toks, vis_attn_mtx))<span class="op">;</span> y</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="105">
<pre data-code-line-numbers=""><code>(#392) [(tensor(0.0114), tensor([7.2815e-02, 2.7817e-02, 2.7298e-02,  ..., 3.5703e-05, 9.5844e-05,
        1.3089e-04], dtype=torch.float16)),(tensor(0.1002), tensor([5.5511e-02, 3.5126e-02, 2.1759e-02,  ..., 6.6400e-05, 6.7353e-05,
        8.6725e-05], dtype=torch.float16)),(tensor(0.0592), tensor([5.8105e-02, 1.9745e-02, 4.0253e-02,  ..., 4.3392e-05, 5.0604e-05,
        7.5161e-05], dtype=torch.float16)),(tensor(0.1263), tensor([4.4434e-02, 2.7466e-02, 2.9800e-02,  ..., 4.5955e-05, 4.0114e-05,
        5.9068e-05], dtype=torch.float16)),(tensor(0.0875), tensor([4.9561e-02, 2.5208e-02, 1.5625e-02,  ..., 5.1498e-05, 6.5625e-05,
        1.1742e-04], dtype=torch.float16)),(tensor(0.0013), tensor([4.0619e-02, 2.0660e-02, 1.8677e-02,  ..., 1.0329e-04, 8.6665e-05,
        1.9145e-04], dtype=torch.float16)),(tensor(0.0005), tensor([3.3173e-02, 1.9882e-02, 1.4587e-02,  ..., 5.0008e-05, 7.7903e-05,
        9.0420e-05], dtype=torch.float16)),(tensor(0.0036), tensor([1.8967e-02, 1.4717e-02, 1.7731e-02,  ..., 7.9393e-05, 4.9829e-05,
        4.2319e-05], dtype=torch.float16)),(tensor(0.0086), tensor([0.0320, 0.0208, 0.0108,  ..., 0.0001, 0.0001, 0.0002],
       dtype=torch.float16)),(tensor(0.0018), tensor([2.7313e-02, 1.5854e-02, 1.1726e-02,  ..., 1.1587e-04, 1.2165e-04,
        9.3877e-05], dtype=torch.float16)),(tensor(0.0009), tensor([1.7624e-02, 1.1581e-02, 1.1177e-02,  ..., 6.8367e-05, 9.2864e-05,
        1.7357e-04], dtype=torch.float16)),(tensor(0.0008), tensor([1.8936e-02, 1.0399e-02, 1.0033e-02,  ..., 3.6418e-05, 5.7578e-05,
        4.4882e-05], dtype=torch.float16)),(tensor(0.0015), tensor([2.3804e-02, 1.3443e-02, 1.0323e-02,  ..., 3.4571e-05, 4.4703e-05,
        1.3959e-04], dtype=torch.float16)),(tensor(0.0008), tensor([3.4149e-02, 1.6403e-02, 1.2215e-02,  ..., 2.6405e-05, 4.3452e-05,
        5.7697e-05], dtype=torch.float16)),(tensor(0.0015), tensor([2.0523e-02, 1.3504e-02, 1.1162e-02,  ..., 3.9041e-05, 4.8935e-05,
        7.4387e-05], dtype=torch.float16)),(tensor(0.0007), tensor([1.9135e-02, 1.0620e-02, 1.3245e-02,  ..., 2.6762e-05, 5.5313e-05,
        3.9697e-05], dtype=torch.float16)),(tensor(0.0004), tensor([3.3081e-02, 1.1414e-02, 3.4821e-02,  ..., 2.5630e-05, 5.0366e-05,
        7.6592e-05], dtype=torch.float16)),(tensor(0.0005), tensor([2.1408e-02, 1.3779e-02, 2.8259e-02,  ..., 1.3947e-05, 3.1412e-05,
        3.7611e-05], dtype=torch.float16)),(tensor(0.0007), tensor([2.5253e-02, 1.3268e-02, 1.7975e-02,  ..., 8.7857e-05, 1.4389e-04,
        2.6059e-04], dtype=torch.float16)),(tensor(0.0004), tensor([1.5945e-02, 7.9346e-03, 1.0849e-02,  ..., 1.1802e-05, 4.8161e-05,
        3.2246e-05], dtype=torch.float16))...]</code></pre>
</div>
</div>
<div id="d95626ef" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="a0da6e3a-c7ce-483a-979c-8ca3e0611ad0" data-execution_count="106">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb171" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb171-1"><a href="#cb171-1" aria-hidden="true" tabindex="-1"></a>y[<span class="dv">0</span>], y[<span class="dv">0</span>][<span class="dv">1</span>].shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="106">
<pre data-code-line-numbers=""><code>((tensor(0.0114),
  tensor([7.2815e-02, 2.7817e-02, 2.7298e-02,  ..., 3.5703e-05, 9.5844e-05,
          1.3089e-04], dtype=torch.float16)),
 torch.Size([1564]))</code></pre>
</div>
</div>
<p>So what I have here, <code>y</code>, is the LLM attention weights over the image tokens, and the ViT attention weights of the image patches.</p>
<div id="00d4ec8d" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="76eb0843-04fc-499e-da55-8b992f67bba6" data-execution_count="107">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb173" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb173-1"><a href="#cb173-1" aria-hidden="true" tabindex="-1"></a>grid_sz <span class="op">=</span> img_inp[<span class="dv">0</span>].size[<span class="dv">0</span>]<span class="op">//</span><span class="dv">14</span>, img_inp[<span class="dv">0</span>].size[<span class="dv">1</span>]<span class="op">//</span><span class="dv">14</span><span class="op">;</span> grid_sz</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="107">
<pre data-code-line-numbers=""><code>(46, 34)</code></pre>
</div>
</div>
<p>I need to resize the ViT/vision encoder tensors so it matches the number of image tokens. In addition, I’ll also take the product between the resulting reshaped attention tensor and the LLM attention weights to get the final attention over the image.</p>
<div id="ba2715c6" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="fc2d892a-9399-43b2-c428-db55a3240738" data-execution_count="108">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb175" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb175-1"><a href="#cb175-1" aria-hidden="true" tabindex="-1"></a>y[<span class="dv">0</span>][<span class="dv">1</span>].reshape(grid_sz[<span class="dv">0</span>],grid_sz[<span class="dv">1</span>]).shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="108">
<pre data-code-line-numbers=""><code>torch.Size([46, 34])</code></pre>
</div>
</div>
<div id="ZA_MxF6F6msh" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="042afca7-9dbd-4a27-d61c-f50dd58dfd5f" data-execution_count="109">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb177" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb177-1"><a href="#cb177-1" aria-hidden="true" tabindex="-1"></a>attn_over_img <span class="op">=</span> []</span>
<span id="cb177-2"><a href="#cb177-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> w, vis_attn <span class="kw">in</span> y:</span>
<span id="cb177-3"><a href="#cb177-3" aria-hidden="true" tabindex="-1"></a>  vis_attn <span class="op">=</span> vis_attn.reshape(grid_sz[<span class="dv">0</span>],grid_sz[<span class="dv">1</span>])</span>
<span id="cb177-4"><a href="#cb177-4" aria-hidden="true" tabindex="-1"></a>  attn_over_img.append(vis_attn<span class="op">*</span>w)</span>
<span id="cb177-5"><a href="#cb177-5" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(attn_over_img), attn_over_img[<span class="dv">0</span>].shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="109">
<pre data-code-line-numbers=""><code>(392, torch.Size([46, 34]))</code></pre>
</div>
</div>
<div id="2ggUofaP62nP" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="44c6a382-c0bc-43cd-e3e0-c98688571e2a" data-execution_count="110">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb179" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb179-1"><a href="#cb179-1" aria-hidden="true" tabindex="-1"></a>attn_over_img <span class="op">=</span> t.stack(attn_over_img).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span> attn_over_img.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="110">
<pre data-code-line-numbers=""><code>torch.Size([46, 34])</code></pre>
</div>
</div>
<div id="-Z6JwpGQ7KPU" class="cell" data-execution_count="111">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb181" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb181-1"><a href="#cb181-1" aria-hidden="true" tabindex="-1"></a>attn_over_img <span class="op">=</span> attn_over_img <span class="op">/</span> attn_over_img.<span class="bu">max</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="jtNOmnlY7TQK" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb182" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb182-1"><a href="#cb182-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb182-2"><a href="#cb182-2" aria-hidden="true" tabindex="-1"></a>??F.interpolate</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The reason why I’m interpolating here is to align the resolution of the current attention heatmap to the original input image. In essence, I’m upscaling the attention heatmap.</p>
<div id="OYo1OglaB5un" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="018feab2-27c6-4bcd-dc74-7ebfbd03ddc2" data-execution_count="113">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb183" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb183-1"><a href="#cb183-1" aria-hidden="true" tabindex="-1"></a>attn_over_img[<span class="va">None</span>,<span class="va">None</span>,:,:].shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="113">
<pre data-code-line-numbers=""><code>torch.Size([1, 1, 46, 34])</code></pre>
</div>
</div>
<div id="SaSCZ65NBChO" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="ba11e16e-da71-4bd4-af0b-8d69453cedd5" data-execution_count="114">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb185" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb185-1"><a href="#cb185-1" aria-hidden="true" tabindex="-1"></a>attn_over_img <span class="op">=</span> F.interpolate(attn_over_img[<span class="va">None</span>,<span class="va">None</span>,:,:], size<span class="op">=</span>img_inp[<span class="dv">0</span>].size, mode<span class="op">=</span><span class="st">'nearest'</span>)<span class="op">;</span> attn_over_img.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="114">
<pre data-code-line-numbers=""><code>torch.Size([1, 1, 644, 476])</code></pre>
</div>
</div>
<p><code>mode='nearest'</code> is an operation that could be thought of as a constrast slider when editing photos. <code>more='nearest'</code> prevents the resulting image from looking too flat or smooth.</p>
<div id="FUei4Zm3DB_T" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="490f7824-c09d-4e2d-cd86-77c5b656d613" data-execution_count="115">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb187" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb187-1"><a href="#cb187-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb187-2"><a href="#cb187-2" aria-hidden="true" tabindex="-1"></a>np_img <span class="op">=</span> np.array(img_inp[<span class="dv">0</span>]).transpose(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">2</span>)<span class="op">;</span> np_img.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="115">
<pre data-code-line-numbers=""><code>(644, 476, 3)</code></pre>
</div>
</div>
<div id="904aPGIODPvj" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="514fa672-7c17-4000-f4ee-1222da32c2a6" data-execution_count="116">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb189" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb189-1"><a href="#cb189-1" aria-hidden="true" tabindex="-1"></a>np_img <span class="op">=</span> np_img[:,:,::<span class="op">-</span><span class="dv">1</span>]<span class="op">;</span> np_img.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="116">
<pre data-code-line-numbers=""><code>(644, 476, 3)</code></pre>
</div>
</div>
<p>I’ll now overlay the attention mask on the image.</p>
<div id="IeFDnLe5DqwY" class="cell" data-execution_count="117">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb191" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb191-1"><a href="#cb191-1" aria-hidden="true" tabindex="-1"></a>np_img <span class="op">=</span> np.float32(np_img)<span class="op">/</span><span class="dv">255</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="Y0RxSVqxEEv7" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="fd645f7a-21c8-474b-e7cc-51b0230c1fa3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb192" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb192-1"><a href="#cb192-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb192-2"><a href="#cb192-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> suplog(): heatmap <span class="op">=</span> cv2.applyColorMap(np.uint8(<span class="dv">255</span><span class="op">*</span>attn_over_img.numpy()), cv2.COLOMAP_HSV)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="1xnef7E7EuMv" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:684}}" data-outputid="df9a169e-15e9-46a0-eed5-9c06900b30df">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb193" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb193-1"><a href="#cb193-1" aria-hidden="true" tabindex="-1"></a>heatmap <span class="op">=</span> cv2.applyColorMap(np.uint8(<span class="dv">255</span><span class="op">*</span>attn_over_img[<span class="dv">0</span>,<span class="dv">0</span>,...].to(t.float16).numpy()), cv2.COLORMAP_HSV)<span class="op">;</span> heatmap</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="f9zdIGSXFYEf" class="cell" data-execution_count="120">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb194" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb194-1"><a href="#cb194-1" aria-hidden="true" tabindex="-1"></a>heatmap <span class="op">=</span> np.float32(heatmap)<span class="op">/</span><span class="dv">255</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="sCIln56BFmj5" class="cell" data-execution_count="121">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb195" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb195-1"><a href="#cb195-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> suplog(): cam <span class="op">=</span> heatmap <span class="op">+</span> np.float32(np_img)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="Pw_LmZUGF0Vu" class="cell" data-execution_count="122">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb196" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb196-1"><a href="#cb196-1" aria-hidden="true" tabindex="-1"></a>cam <span class="op">=</span> heatmap <span class="op">+</span> np.float32(np_img)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="VCOac0ekGYBT" class="cell" data-execution_count="123">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb197" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb197-1"><a href="#cb197-1" aria-hidden="true" tabindex="-1"></a>cam <span class="op">/=</span> np.<span class="bu">max</span>(cam)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="ukCvuaYaGaj9" class="cell" data-execution_count="124">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb198" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb198-1"><a href="#cb198-1" aria-hidden="true" tabindex="-1"></a>img_with_attn <span class="op">=</span> np.uint(<span class="dv">255</span><span class="op">*</span>cam)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="vtG5qjsoLhMy" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="65416180-144b-411e-f338-540a829e8111" data-execution_count="125">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb199" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb199-1"><a href="#cb199-1" aria-hidden="true" tabindex="-1"></a>img_with_attn.shape, <span class="bu">type</span>(img_with_attn), img_with_attn.dtype</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="125">
<pre data-code-line-numbers=""><code>((644, 476, 3), numpy.ndarray, dtype('uint64'))</code></pre>
</div>
</div>
<div id="SsKVpgsyLazg" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="6ed05d16-b752-4bee-b087-131e61b045fd">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb201" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb201-1"><a href="#cb201-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image <span class="im">as</span> PILImage</span>
<span id="cb201-2"><a href="#cb201-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> suplog(): PILImage.fromarray(img_with_attn)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="MeNUTs9JMKko" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:661}}" data-outputid="2f0a0f7e-0c9e-4a91-daeb-c1a138eb6bc8" data-execution_count="127">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb202" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb202-1"><a href="#cb202-1" aria-hidden="true" tabindex="-1"></a>min_val <span class="op">=</span> img_with_attn.<span class="bu">min</span>()</span>
<span id="cb202-2"><a href="#cb202-2" aria-hidden="true" tabindex="-1"></a>max_val <span class="op">=</span> img_with_attn.<span class="bu">max</span>()</span>
<span id="cb202-3"><a href="#cb202-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-4"><a href="#cb202-4" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> max_val <span class="op">==</span> min_val: img_scaled <span class="op">=</span> np.zeros_like(img_with_attn, dtype<span class="op">=</span>np.uint8)</span>
<span id="cb202-5"><a href="#cb202-5" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>: img_scaled <span class="op">=</span> ((img_with_attn <span class="op">-</span> min_val) <span class="op">/</span> (max_val <span class="op">-</span> min_val) <span class="op">*</span> <span class="dv">255</span>).astype(np.uint8)</span>
<span id="cb202-6"><a href="#cb202-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-7"><a href="#cb202-7" aria-hidden="true" tabindex="-1"></a>PILImage.fromarray(img_scaled)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="127">
<div>
<figure class="figure">
<p><img src="28_visualising_vlm_attention_files/figure-html/cell-127-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Now I’ll visualize the map for all tokens.</p>
<div id="-gQhkMmTKhrc" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="a18b5666-2ad3-4e3b-ffa4-56b18e048581" data-execution_count="128">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb203" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb203-1"><a href="#cb203-1" aria-hidden="true" tabindex="-1"></a>num_imgs_per_row <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb203-2"><a href="#cb203-2" aria-hidden="true" tabindex="-1"></a>img_ratio <span class="op">=</span> img_inp[<span class="dv">0</span>].size[<span class="dv">0</span>] <span class="op">/</span> img_inp[<span class="dv">0</span>].size[<span class="dv">1</span>]<span class="op">;</span> img_ratio</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="128">
<pre data-code-line-numbers=""><code>1.3529411764705883</code></pre>
</div>
</div>
<div id="KQueKC89Sm37" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="ca9fe731-4769-4506-af72-f45ed53f465f" data-execution_count="129">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb205" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb205-1"><a href="#cb205-1" aria-hidden="true" tabindex="-1"></a>out_tok_len</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="129">
<pre data-code-line-numbers=""><code>27</code></pre>
</div>
</div>
<div id="4plBEGJGLlJC" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="62758f45-0816-425c-b816-61413521914b" data-execution_count="130">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb207" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb207-1"><a href="#cb207-1" aria-hidden="true" tabindex="-1"></a>num_rows <span class="op">=</span> out_tok_len<span class="op">//</span>num_imgs_per_row<span class="op">+</span>(<span class="dv">1</span> <span class="cf">if</span> out_tok_len<span class="op">%</span>num_imgs_per_row<span class="op">!=</span><span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span>)<span class="op">;</span> num_rows</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="130">
<pre data-code-line-numbers=""><code>4</code></pre>
</div>
</div>
<div id="T7mJMAqiLwb-" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:339}}" data-outputid="e7f9ea70-09c4-4e0f-f086-bd319391f734" data-execution_count="131">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb209" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb209-1"><a href="#cb209-1" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(num_rows, num_imgs_per_row, figsize<span class="op">=</span>(<span class="dv">10</span>,(<span class="dv">10</span><span class="op">/</span>num_imgs_per_row)<span class="op">*</span>img_ratio<span class="op">*</span>num_rows), dpi<span class="op">=</span><span class="dv">150</span>)</span>
<span id="cb209-2"><a href="#cb209-2" aria-hidden="true" tabindex="-1"></a>plt.subplots_adjust(wspace<span class="op">=</span><span class="fl">.05</span>, hspace<span class="op">=</span><span class="fl">.2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="28_visualising_vlm_attention_files/figure-html/cell-131-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="9a3b72ac" class="cell" data-execution_count="132">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb210" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb210-1"><a href="#cb210-1" aria-hidden="true" tabindex="-1"></a><span class="bu">type</span>(img_inp[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="132">
<pre data-code-line-numbers=""><code>PIL.Image.Image</code></pre>
</div>
</div>
<div id="b2kN5uewL9-h" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb212" data-source-line-numbers="nil" data-code-line-numbers="nil"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb212-1"><a href="#cb212-1" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(num_rows, num_imgs_per_row, figsize<span class="op">=</span>(<span class="dv">10</span>,(<span class="dv">10</span><span class="op">/</span>num_imgs_per_row)<span class="op">*</span>img_ratio<span class="op">*</span>num_rows), dpi<span class="op">=</span><span class="dv">150</span>)</span>
<span id="cb212-2"><a href="#cb212-2" aria-hidden="true" tabindex="-1"></a>plt.subplots_adjust(wspace<span class="op">=</span><span class="fl">.05</span>, hspace<span class="op">=</span><span class="fl">.2</span>)</span>
<span id="cb212-3"><a href="#cb212-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb212-4"><a href="#cb212-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i,ax <span class="kw">in</span> <span class="bu">enumerate</span>(axes.flatten()):</span>
<span id="cb212-5"><a href="#cb212-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> i<span class="op">&gt;=</span>out_tok_len:</span>
<span id="cb212-6"><a href="#cb212-6" aria-hidden="true" tabindex="-1"></a>    ax.axis(<span class="st">'off'</span>)</span>
<span id="cb212-7"><a href="#cb212-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">continue</span></span>
<span id="cb212-8"><a href="#cb212-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb212-9"><a href="#cb212-9" aria-hidden="true" tabindex="-1"></a>  target_tok_idx <span class="op">=</span> out_tok_idxs[i]</span>
<span id="cb212-10"><a href="#cb212-10" aria-hidden="true" tabindex="-1"></a>  attn_ws_over_vis_toks <span class="op">=</span> attn_mtx[target_tok_idx][vis_tok_idxs[<span class="dv">0</span>]:vis_tok_idxs[<span class="dv">1</span>]]</span>
<span id="cb212-11"><a href="#cb212-11" aria-hidden="true" tabindex="-1"></a>  attn_ws_over_vis_toks <span class="op">=</span> attn_ws_over_vis_toks <span class="op">/</span> attn_ws_over_vis_toks.<span class="bu">sum</span>()</span>
<span id="cb212-12"><a href="#cb212-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb212-13"><a href="#cb212-13" aria-hidden="true" tabindex="-1"></a>  attn_over_img <span class="op">=</span> []</span>
<span id="cb212-14"><a href="#cb212-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> w,vis_attn <span class="kw">in</span> <span class="bu">zip</span>(attn_ws_over_vis_toks, vis_attn_mtx):</span>
<span id="cb212-15"><a href="#cb212-15" aria-hidden="true" tabindex="-1"></a>    vis_attn <span class="op">=</span> vis_attn.reshape(grid_sz[<span class="dv">0</span>],grid_sz[<span class="dv">1</span>])</span>
<span id="cb212-16"><a href="#cb212-16" aria-hidden="true" tabindex="-1"></a>    vis_attn <span class="op">=</span> vis_attn <span class="op">/</span> vis_attn.<span class="bu">max</span>()</span>
<span id="cb212-17"><a href="#cb212-17" aria-hidden="true" tabindex="-1"></a>    attn_over_img.append(vis_attn<span class="op">*</span>w)</span>
<span id="cb212-18"><a href="#cb212-18" aria-hidden="true" tabindex="-1"></a>  attn_over_img <span class="op">=</span> t.stack(attn_over_img).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb212-19"><a href="#cb212-19" aria-hidden="true" tabindex="-1"></a>  attn_over_img <span class="op">=</span> attn_over_img <span class="op">/</span> attn_over_img.<span class="bu">max</span>()</span>
<span id="cb212-20"><a href="#cb212-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb212-21"><a href="#cb212-21" aria-hidden="true" tabindex="-1"></a>  attn_over_img <span class="op">=</span> F.interpolate(attn_over_img[<span class="va">None</span>,<span class="va">None</span>,...], size<span class="op">=</span>(img_inp[<span class="dv">0</span>].size[<span class="dv">1</span>],img_inp[<span class="dv">0</span>].size[<span class="dv">0</span>]), mode<span class="op">=</span><span class="st">'bicubic'</span>, align_corners<span class="op">=</span><span class="va">False</span>).squeeze()</span>
<span id="cb212-22"><a href="#cb212-22" aria-hidden="true" tabindex="-1"></a>  attn_over_img <span class="op">=</span> attn_over_img.<span class="bu">pow</span>(<span class="dv">1</span><span class="op">/</span><span class="fl">1.5</span>)</span>
<span id="cb212-23"><a href="#cb212-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb212-24"><a href="#cb212-24" aria-hidden="true" tabindex="-1"></a>  np_img <span class="op">=</span> np.array(img_inp[<span class="dv">0</span>])[:,:,::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb212-25"><a href="#cb212-25" aria-hidden="true" tabindex="-1"></a>  np_img <span class="op">=</span> np.float32(np_img)<span class="op">/</span><span class="dv">255</span></span>
<span id="cb212-26"><a href="#cb212-26" aria-hidden="true" tabindex="-1"></a>  hm <span class="op">=</span> cv2.applyColorMap(np.uint8(<span class="dv">255</span><span class="op">*</span>attn_over_img.cpu().numpy()), cv2.COLORMAP_HSV)</span>
<span id="cb212-27"><a href="#cb212-27" aria-hidden="true" tabindex="-1"></a>  hm <span class="op">=</span> np.float32(hm)<span class="op">/</span><span class="dv">255</span></span>
<span id="cb212-28"><a href="#cb212-28" aria-hidden="true" tabindex="-1"></a>  cam <span class="op">=</span> hm<span class="op">+</span>np.float32(np_img)</span>
<span id="cb212-29"><a href="#cb212-29" aria-hidden="true" tabindex="-1"></a>  cam <span class="op">/=</span> np.<span class="bu">max</span>(cam)</span>
<span id="cb212-30"><a href="#cb212-30" aria-hidden="true" tabindex="-1"></a>  img_with_attn <span class="op">=</span> np.uint8(<span class="dv">255</span><span class="op">*</span>cam)</span>
<span id="cb212-31"><a href="#cb212-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb212-32"><a href="#cb212-32" aria-hidden="true" tabindex="-1"></a>  ax.imshow(img_with_attn)</span>
<span id="cb212-33"><a href="#cb212-33" aria-hidden="true" tabindex="-1"></a>  ax.set_title(tokz.decode(trimmed_o[<span class="dv">0</span>][i], add_special_tokens<span class="op">=</span><span class="va">False</span>).strip(), fontsize<span class="op">=</span><span class="dv">7</span>, pad<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb212-34"><a href="#cb212-34" aria-hidden="true" tabindex="-1"></a>  ax.axis(<span class="st">'off'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>


</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script type="application/vnd.jupyter.widget-state+json">
{"001dde98014b4daca7312ea569bdfc31":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"139cb1a39be24fe3b662e0d573337a4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28d6305cdf724553a83b1a48aefc7363","placeholder":"​","style":"IPY_MODEL_001dde98014b4daca7312ea569bdfc31","value":"Loading checkpoint shards: 100%"}},"1b411723702c4cf19df72320e4717d9a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea69eb487d424d8c9e8f03701df998f1","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c45c84229c694ddca013ee6aa5ab3569","value":2}},"28d6305cdf724553a83b1a48aefc7363":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36478b0e63714b739e7dff2eee85ec26":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37aa80f78a444452867c0982c5a37a43":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7286e200dc4b4f9bbde05d746f7f7269":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e7a1225575b4349b4f1ffe7922ee9d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_139cb1a39be24fe3b662e0d573337a4a","IPY_MODEL_1b411723702c4cf19df72320e4717d9a","IPY_MODEL_ed263a410d3846019c68cc883d9b1c56"],"layout":"IPY_MODEL_36478b0e63714b739e7dff2eee85ec26"}},"c45c84229c694ddca013ee6aa5ab3569":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea69eb487d424d8c9e8f03701df998f1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed263a410d3846019c68cc883d9b1c56":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7286e200dc4b4f9bbde05d746f7f7269","placeholder":"​","style":"IPY_MODEL_37aa80f78a444452867c0982c5a37a43","value":" 2/2 [00:34&lt;00:00, 16.23s/it]"}}}
</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/forbo7\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="light">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "ForBo7/forbo7.github.io";
    script.dataset.repoId = "R_kgDOICXnAA";
    script.dataset.category = "Giscus Comments";
    script.dataset.categoryId = "DIC_kwDOICXnAM4CR8YX";
    script.dataset.mapping = "title";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><a href="https://forbo7.github.io/">ForBo7 // Salman Naqvi</a> © 2022–2025 to ∞ and ForBlog™ by <a href="https://forbo7.github.io/about.html">Salman Naqvi</a></p>
</div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://forums.fast.ai/u/forbo7/summary">
      <i class="bi bi-file-post-fill" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/ForBo7_">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.artstation.com/forbo7">
      <i class="bi bi-brush" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:salmananaqvii+forblog@gmail.com">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="../../forblog/index.xml">
      <i class="bi bi-rss" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
<p><a href="https://forbo7.github.io/patch_notes.html">Version 2.2.2.0</a> | <a href="https://forbo7.github.io/feedback.html">Feedback</a> | Website made with <a href="https://quarto.org/">Quarto</a>, by me!</p>
</div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
<script src="../../site_libs/quarto-contrib/line-highlight-1.0.0/line-highlight.js" defer="true"></script>
</body></html>